\documentclass[man,natbib]{apa6}

\title{Context-Specific Syllable-Co-Occurence Statistics in Infant-Directed Speech}
\shorttitle{Contexts}

\author{Rose Maier}
\affiliation{University of Oregon}

\abstract{Put your abstract here.}

\begin{document}
<< document_prep, echo=FALSE, message=FALSE, error=FALSE, results="hide" >>=
  
  # pull out all of the r code for this paper and save it as its own file:
  # Stangle('paper.Rnw')
  
  # update the bibliography file
  file.copy(from='/Users/TARDIS/Dropbox/library.bib', to='/Users/TARDIS/Documents/STUDIES/context_word_seg/', overwrite=TRUE, recursive=FALSE, copy.mode=FALSE, copy.date=TRUE)

# add citations I don't have in Mendeley
add.cites <- cat('
@article{Korman1984,
  title={Adaptive aspects of maternal vocalizations in differing contexts at ten weeks},
  author={Korman, Myron},
  journal={First language},
  volume={5},
  pages={44--45},
  year={1984}
}
@book{Box1987,
  title={Empirical model-building and response surfaces},
  author={Box, George EP and Draper, Norman Richard},
  volume={424},
  year={1987},
  publisher={Wiley New York}
}', file='add.cites.bib')

file.append('library.bib', 'add.cites.bib')
@
\maketitle    

\section{Introduction}

\section{Method}

\subsection{The corpus}
The \citet{Korman1984} corpus is comprised of dense longitudinal recordings from 6 infants and their middle class UK mothers at home. Mothers were instructed to keep the recording apparatus near the child and on for as much of the day as possible. Experimenters dropped off the recording equipment around noon and picked it up around noon the next day. There are six recordings for each infant, spanning the age range from 6 to 16 weeks (for more details on the participants and recording methods, see \citealt{Korman1984}).

The original corpus (available via CHILDES, cite MacWhinney) is transcribed in CHAT format, but because we were interested in the co-occurence of syllables we needed the phonetic transcription. 
This is one of the corpora used by \citet{Swingley2005}, and we used the same phonetic approximations from that study as well (for details about the phonetic approximation, see \citealt{Swingley2005}). 
This phonetic approximation is a simplified and idealized version of what the actual phonetic material would have been - a given word is represented with the exact same pronunciation each time it occurs, ignoring probable irregularities due to coarticulation, prosody, etc. 
This has the potential to exagerate performance of the segmentation model, since there is no variability from one exemplar of a word to another as there would be in real speech. 
Syllable boundaries are also already given in the phonetic approximation of this corpus, again potentially simplifying the word boundary problem for the model since word boundaries are only possible at syllable boundaries. 
\citet{Swingley2005} examines the effect of probabilistic resyllabification of the corpus on word segmentation accuracy and indeed finds that it results in a modest drop in accuracy (when syllable onsets and offsets are restricted to forms observed at utterance onsets and offsets respectively, so phonotactically illegal sequences like vmi from "give me" do not occur). 
Using unambiguous syllable boundaries in this project likely exagerates the strength of statistical cues to word boundaries. 
It is also important to consider that many features of this simplified phonetic corpus would have the opposite effect. 
For example, prosodic information, which has been demonstrated to be a useful cue to word boundaries for infants both on its own (cite) and in combination with statistical phonological cues (cite), is not available to the model, potentially handicapping performance. 
Utterance boundaries have also been removed, another important cue to word boundaries (cite). 
Information from other modalities, such as touch \citep{Seidl2014}, may also bolster the signal to word boundaries and is not available to the model. 
As \citet*{Box1987} famously note, "All models are wrong, but some are useful." 
The goal of this analyses is to explore how context shapes patterns in the statistical co-occurence of syllables, clarifying the role of statistical learning for word segmentation in natural speech. 
While our model is most definitely wrong, it is our hope that it is still useful. 

\subsection{Defining context} 
There are multiple potential ways to define context within a corpus. 
\subsubsection{Key word analysis}
For this analysis, our approach was to use a list of key words that we expected would be associated with particular contexts â€“ for example, milk was on the list for mealtime words, and wipe was on the list for diaper change words. 
The context lists were based on MCDI words, previous work describing infants' everyday activities \citep{Place2011, Fausey2015} and other models that have attempted to define contexts in infants' experience \citep{Soderstrom2013, Roy2012}, supplemented with some especially frequent content words from the corpus. 
The key word lists are available in Appendix~\ref{app:KeyWords}. 
Utterances in the corpus were then tagged for each context (not mutually exclusively) when those key words occurred, yielding 7 context-specific subcorpora. 
Word segmentation decisions based on syllable co-occurrence statistics were made within each context independently of the others, modeling zero generalization from one context to another on the part of infants. 

\subsubsection{Coder judgments}

\subsubsection{Topic modeling}

To include a list in your text \begin{seriate} \item this, \item that, and \item the other thing \end{seriate} .
Example citations \citep<e.g.,>[p.~11]{Roy2012}, 

\begin{table}[h]
\caption{Mean and standard deviation of points earned in different experimental groups}
\begin{tabular}{llll} % add one "l" for each column in the table
\hline\\
& Difficult Stimuli & Easy Stimuli & Group 3 \\
\hline\\
Mean & 34 & -20 & 3.0\\
Standard Deviation & 1.0 & 2.0 & 3.0\\
\hline
\end{tabular}
\end{table}

% To put a table in your document, you need to use this (somewhat ugly looking) latex table code. Play around with it by replacing the dummy values with the values you want. To learn more about how to make complex tables in Latex, see this tutorial https://www.sharelatex.com/learn/Tables. 

\begin{table}[!htb]
\caption{Mean and standard deviation of points earned in different experimental groups}
\begin{tabular}{llll} % add one "l" for each column in the table
\hline\\
& Difficult Stimuli & Easy Stimuli & Group 3 \\
\hline\\
Mean & 34 & -20 & 3.0\\
Standard Deviation & 1.0 & 2.0 & 3.0\\
\hline
\end{tabular}
\end{table}
% ------ End of Table Code

\bibliography{library}

\appendix
\section{Context Key Words Lists}
\label{app:KeyWords}
% \input{words by contexts.csv}

\end{document}