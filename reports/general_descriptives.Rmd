---
title: "General descriptives"
author: "Rose Hartman"
date: "July 31, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
knitr::opts_knit$set(root.dir = "/Users/TARDIS/Documents/STUDIES/context_word_seg")
```

```{r}
getwd()
library(ProjectTemplate)
load.project()
```

# How many utterances per recording?

```{r transcript_length}
transcript_lengths <- df %>% 
  as.tbl() %>% 
  extract(col=utt, into=c("child", "age", "utt"), regex="([[:alpha:]]{2})([[:digit:]]+).*_([[:digit:]]+)$", convert = TRUE) %>% 
  group_by(child, age) %>% 
  summarize(N.utts=max(utt))

ggplot(transcript_lengths, aes(y=N.utts, x=age, fill = child)) + 
  geom_bar(stat = "identity") + 
  scale_x_continuous(breaks=6:16, minor_breaks = NULL) + 
  labs(x="Age (weeks)", y="Number of utterances", title = "Transcript lengths by age")
ggsave(filename = "transcript_lengths_1.png", path = "graphs/descriptives", width = 5, height = 4, units="in")

ggplot(transcript_lengths, aes(y=N.utts, x=child)) + 
  geom_boxplot() +
  geom_point(aes(color=child)) + 
  labs(x="Child", y="Number of utterances", title = "Transcript lengths by child")
ggsave(filename = "transcript_lengths_2.png", path = "graphs/descriptives", width = 5, height = 4, units="in")
```

# What is the rhythm of activities like over each transcript?    

Sequence plots: Show the sequence of coded contexts over time for each transcript.

```{r seq_plots, eval=FALSE}
source("src/sequence_plots.R")
```

In each of the following plots, the activity contexts are shown on the y-axis, and the course of the transcript runs along the x-axis, from the first utterance to the last utterance.
Darkening in the colored bar for each context indicates that that context is coded as happening at that utterance.
Utterance number is a rough proxy for time of day since time stamps are not available in the corpus; while there are obvious shortcomings with this noisy measure, it does preserve the sequential ordering of events even if it loses information about precise timing.    

There are several general methodological points of interest visible in these plots. 
When defining 'activity context' using the word list approach, there can be (and often are) sections of the corpus that receive no context tag at all (vertical sections in the plot where there are no darkened context bars).
This is less common with the other approaches to defining context, giving the word list plots a relatively sparse appearance.
In the coder judgment plots, there are some vertical bars with no data at all --- those represent the few parts of the corpus that are not fully coded (there are not 5 independent coders for each utterance).
In order to determine context using topic modeling, I binned the transcripts into documents of 30 utterances each (necessary to allow large enough samples of speech to estimate the word co-occurrence rates on which topic modeling algorithms are based). 
This means the topic modeling approach tags the corpus in bins of 30 utterances, resulting in chunks of each context that are at least 30 utterances long.
The word list method identifies chunks of at least 5 utterances due to the smoothing procedure that picks up 2 utterances before and after each tagged utterance. 
The coder judgments are based on sections of the corpus that were 30 utterances long, but because each utterance is often coded by more than the required 5 coders a random sample of 5 coders for each utterance results in a more natural gradient at the edges of context chunks. 

Note that contexts with fewer than 100 total utterances across the entire corpus (less than 1% of the corpus) are not shown. This omits 1 context using the word list definition of context and 13 contexts using the coder judgments. 

## Example 1: la16
The following are four sequence plots from one transcript (child la at 16 weeks, the longest transcript in the corpus). 

Defining context using key words from the word lists:
![](/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/WL/seqplot-WLla16.cha.png "Sequence plot for child la at 16 weeks, word list definition of context")
Defining context using coder judgments:
![](/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/HJ/seqplot-HJla16.cha.png "Sequence plot for child la at 16 weeks, coder judgment definition of context")
Defining context using topics from LDA topic modeling:
![](/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/LDA/seqplot-LDAla16.cha.png "Sequence plot for child la at 16 weeks, LDA topic modeling definition of context")
Defining context using topics from STM topic modeling (allows for variability family to family when discovering topics):
![](/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/STM/seqplot-STMla16.cha.png "Sequence plot for child la at 16 weeks, STM topic modeling definition of context")

It is also possible to see hints of agreement across methods by examining the plots. 
For example, there is a section around utterance 1300 that appears to be identified as "bath time" by human coders, includes words from the "bath" word list, and is mostly topic 8 by the STM topic modeling (that episode is not as clearly picked out by the LDA topic modeling, although perhaps topics 10 and/or 1 correspond). 
Note that the LDA topic modeling identifies mostly one context for the duration of the transcript (topic 11), possibly getting caught on family-specific words like the child's name.

## Example 2: gl06
The following are four sequence plots from one transcript (child gl at 6 weeks). 
There appear to be a couple naps during this transcript: one around utterance 150 and another beginning around utterance 400 (and possibly another right at the beginning of the transcript, as identified by the word list and coder judgment methods only). 
There also appears to be a bath around utterance 550, according to the word list and coder judgment plots, although it is not marked with topic 8 in the STM plot, unlike with the previous example. 
Again, the LDA topic modeling uses one topic heavily throughout the transcript (topic 7).

Defining context using key words from the word lists:
![](/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/WL/seqplot-WLgl06.cha.png "Sequence plot for child gl at 6 weeks, word list definition of context")
Defining context using coder judgments:
![](/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/HJ/seqplot-HJgl06.cha.png "Sequence plot for child gl at6 weeks, coder judgment definition of context")
Defining context using topics from LDA topic modeling:
![](/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/LDA/seqplot-LDAgl06.cha.png "Sequence plot for child gl at 6 weeks, LDA topic modeling definition of context")
Defining context using topics from STM topic modeling (allows for variability family to family when discovering topics):
![](/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/STM/seqplot-STMgl06.cha.png "Sequence plot for child gl at 6 weeks, STM topic modeling definition of context")

#How are the contexts distributed across families and transcripts?

```{r contexts_by_transcript, eval=FALSE}
df_all_long <- df_all %>% 
  gather(key = "key", value = "value", -utt, -orth, -phon) %>% 
  extract(col = key, into = c("method", "context"), regex = "(^[[:upper:]]{2,3})_(.*)$", remove = FALSE) %>% 
  extract(col = utt, into = c("child", "age", "utt"), regex = "(^[[:alpha:]]{2})([[:digit:]]+)[.]cha_([[:digit:]]+)$", convert = TRUE)

min.utts <- 100

contexts_keep <- df_all_long %>% 
  group_by(key) %>% 
  summarize(N.utts = sum(value, na.rm = TRUE)) %>% 
  filter(N.utts > min.utts)

df_keep_long <- df_all_long %>% 
  dplyr::filter(key %in% contexts_keep$key) %>% 
  dplyr::select(-key) 

contexts_by_transcript <- df_keep_long %>% 
  group_by(child, age, method, context) %>% 
  summarize(N.utts = sum(value, na.rm = TRUE))

for(m in unique(contexts_by_transcript$method)){
  plot.data <- contexts_by_transcript %>% 
    dplyr::filter(method == m) %>% 
    dplyr::filter(N.utts > 0)
  
  n.levels <- length(unique(plot.data$context))
  colors <- c("#FFFF33", "#FD8D3C", "#FC4E2A","#238B45",
              brewer.pal(9,"YlGnBu")[c(4,6,8)],
              brewer.pal(9,"PuRd")[c(5,7)],
              brewer.pal(9,"BuPu")[c(5,7,9)],
              brewer.pal(9,"YlOrRd")[c(9)], 
              brewer.pal(9,"Greens")[c(4,9)])

  p1 <- ggplot(plot.data, aes(y = N.utts, x = as.factor(age), fill = context)) + 
    geom_bar(stat = "identity") +  
    facet_wrap(~ child , scales = "free") + 
    scale_fill_manual(values=colors) +
    labs(y = "Number of utterances", x="Age (weeks)", title= paste0("Context distribution across transcripts\n", m))  
  ggsave(plot = p1, filename = paste0("contexts_by_transcripts_", m, ".png"), path = "graphs/descriptives", width = 8, height = 5, units="in")
  
  p2 <- ggplot(plot.data, aes(y = N.utts, x = as.factor(age), fill = context)) + 
    geom_bar(stat = "identity", position = "fill") +  
    facet_wrap(~ child , scales = "free") + 
    scale_fill_manual(values=colors) + 
  labs(y = "Proportion of utterance codes", x="Age (weeks)", title= paste0("Context distribution across transcripts\n", m))  
  ggsave(plot = p2, filename = paste0("contexts_by_transcripts_", m, "_prop.png"), path = "graphs/descriptives", width = 8, height = 5, units="in")
}
```

These plots show how many utterances fall in each context for each transcript (each child at each age), for each of the approaches to defining context.
For each approach, there is one plot showing raw number of utterances in each transcript and a second plot presenting the same information in terms of proportion of total context codes for each transcript.    

One of the most important things to note in these plots is which contexts tend to be distributed more or less evenly across families and ages, and which contexts appear to be family- or age-specific. 
There is a particularly striking difference between the two topic modeling methods, LDA and STM, with the latter showing rather even distribution of contexts whereas the former appears to have a strong tendency to pick one or two dominant topics for each family. 
When contexts are defined according to LDA topic modeling, most of the transcripts are dominated by one context (and that same context often repeats across recordings for that family); since these are day-long, natural recordings of infants at home with their caregivers, it is surprising to see a single context characterizing an entire transcript.
The fact that this homogeneity appears in the LDA method only suggests it may be an artifact of that analysis procedure. 
This difference may be due to the fact that STM (unlike LDA) allows for variability family to family in the prevalence and characteristics of each topic during estimation.
Because LDA lacks this flexibility, it may get caught up on common words that are family-specific and miss patterns of words that vary *within* families as a consequence.
The most obvious example of family-specific words are the children's names, but there are several other words that, for one reason or another, appear often only in one family and not the others.

### Defining context using key words from the word lists:

<img src="/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/descriptives/contexts_by_transcripts_WL.png" width="600" height="400" />
<img src="/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/descriptives/contexts_by_transcripts_WL_prop.png" width="600" height="400" />

### Defining context using coder judgments:

<img src="/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/descriptives/contexts_by_transcripts_HJ.png" width="600" height="400" />
<img src="/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/descriptives/contexts_by_transcripts_HJ_prop.png" width="600" height="400" />

### Defining context using topics from LDA topic modeling:

<img src="/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/descriptives/contexts_by_transcripts_LDA.png" width="600" height="400" />
<img src="/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/descriptives/contexts_by_transcripts_LDA_prop.png" width="600" height="400" />

### Defining context using topics from STM topic modeling:

<img src="/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/descriptives/contexts_by_transcripts_STM.png" width="600" height="400" />
<img src="/Users/TARDIS/Documents/STUDIES/context_word_seg/graphs/descriptives/contexts_by_transcripts_STM_prop.png" width="600" height="400" />

Note that both STM and LDA topic models we run after removing [stop words](http://www.lextek.com/manuals/onix/stopwords2.html).
Many stop words are very high frequency in samples of natural speech, so they may show up as very probably under a given topic not because of their specificity to that topic but because of their general high probability of occurring regardless of topic.

```{r top_words_lda}
top_words_lda <- top.topic.words(lda$topics, num.words=20) %>% 
  as.data.frame(stringsAsFactors = FALSE)
colnames(top_words_lda) <- paste0("topic_", 1:ncol(top_words_lda)) 
top_words_lda %>% 
  kable(caption="Top words for each topic, in order")
```

