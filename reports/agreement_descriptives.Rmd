---
output: html_document
---


```{r setup, message=FALSE}
knitr::opts_knit$set(root.dir = "/Users/TARDIS/Documents/STUDIES/context_word_seg")
```

```{r load_project, message=FALSE}
library(ProjectTemplate)
load.project()
```

# General descriptives

## How many utterances in each context?

```{r utt_per_context}
######
# WL #
######
WL <- df_WL_bin
WL_sum <- data.frame(context = names(colSums(dplyr::select(WL, -utt, -orth, -phon))), n=colSums(dplyr::select(WL, -utt, -orth, -phon))) %>% 
  mutate(method="word lists")
WL_sum %>% 
  select(-method) %>% 
  arrange(desc(n)) %>% 
  kable()

######
# HJ #
######
HJ <- df_HJ_bin
HJ_sum <- data.frame(context=names(colSums(dplyr::select(HJ, -utt, -orth, -phon), na.rm=TRUE)), n=colSums(dplyr::select(df_HJ_bin, -utt, -orth, -phon), na.rm=TRUE)) %>% 
  mutate(method="coder judgments")
HJ_sum %>% 
  select(-method) %>% 
  arrange(desc(n)) %>% 
  kable()

######
# TM #
######
LDA <- df_LDA_bin
LDA_sum <- data.frame(context=names(colSums(dplyr::select(LDA, -utt, -orth, -phon), na.rm=TRUE)), n=colSums(dplyr::select(LDA, -utt, -orth, -phon), na.rm=TRUE)) %>% 
  mutate(method="LDA")
LDA_sum %>% 
  select(-method) %>% 
  arrange(desc(n)) %>% 
  kable()

STM <- df_STM_bin
STM_sum <- data.frame(context=names(colSums(dplyr::select(STM, -utt, -orth, -phon), na.rm=TRUE)), n=colSums(dplyr::select(STM, -utt, -orth, -phon), na.rm=TRUE)) %>% 
  mutate(method="STM")
STM_sum %>% 
  select(-method) %>% 
  arrange(desc(n)) %>% 
  kable()
```

```{r, fig.height=12}
all_sum <- rbind(WL_sum, HJ_sum, LDA_sum, STM_sum)

ggplot(all_sum, aes(y=n, x=reorder(as.factor(context), n))) + 
  geom_bar(stat = 'identity', show.legend = FALSE) + 
  geom_hline(aes(yintercept=nrow(df)), lty=2) + 
  facet_wrap(~method, scales = "free_x", ncol=1) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ggtitle("Number of utterances in each context\ndashed line shows total corpus size")
ggsave(filename ="utts_per_context.png" , path = "graphs/descriptives", width = 5, height = 14, units="in")

```

## How many contexts per utterance?

```{r context_per_utt}
######
# WL #
######
WL$N.contexts <- rowSums(dplyr::select(WL, -utt, -orth, -phon))
table(WL$N.contexts)

WL <- extract_contexts(WL)
summary(WL$context) # the number of utterances in each context

length(which(WL$N.contexts > 2)) / length(WL$N.contexts) # percent of corpus with > 2 contexts
length(which(WL$N.contexts == 0)) / length(WL$N.contexts) # percent of corpus with 0 contexts
length(which(WL$N.contexts > 2)) / length(which(WL$N.contexts > 0)) 
length(which(WL$N.contexts > 1)) / length(which(WL$N.contexts > 0)) 


######
# HJ #
######
HJ$N.contexts <- rowSums(dplyr::select(HJ, -utt, -orth, -phon), na.rm = TRUE)
table(HJ$N.contexts)

HJ <- extract_contexts(HJ)
summary(HJ$context) # the number of utterances in each context

length(which(HJ$N.contexts > 2)) / length(HJ$N.contexts) # percent of corpus with > 2 contexts
length(which(HJ$N.contexts == 0)) / length(HJ$N.contexts) # percent of corpus with 0 contexts

######
# TM #
######
LDA$N.contexts <- rowSums(dplyr::select(LDA, -utt, -orth, -phon), na.rm = TRUE)
table(LDA$N.contexts)

LDA <- extract_contexts(LDA)
summary(LDA$context) # the number of utterances in each context

length(which(LDA$N.contexts > 2)) / length(LDA$N.contexts) # percent of corpus with > 2 contexts
length(which(LDA$N.contexts == 0)) / length(LDA$N.contexts) # percent of corpus with 0 contexts

STM$N.contexts <- rowSums(dplyr::select(STM, -utt, -orth, -phon), na.rm = TRUE)
table(STM$N.contexts)

STM <- extract_contexts(STM)
summary(STM$context) # the number of utterances in each context

length(which(STM$N.contexts > 2)) / length(STM$N.contexts) # percent of corpus with > 2 contexts
length(which(STM$N.contexts == 0)) / length(STM$N.contexts) # percent of corpus with 0 contexts

###########
# OVERALL #
###########
wl_contexts <- dplyr::select(WL, utt, N.contexts, context) %>% 
  mutate(method = "word list")
hj_contexts <- dplyr::select(HJ, utt, N.contexts, context) %>% 
  mutate(method = "coder judgments")
lda_contexts <- dplyr::select(LDA, utt, N.contexts, context) %>% 
  mutate(method = "LDA")
stm_contexts <- dplyr::select(STM, utt, N.contexts, context) %>% 
  mutate(method = "STM")

all_methods_counts <- rbind(wl_contexts, hj_contexts, lda_contexts, stm_contexts) %>% 
  count(method, N.contexts) 

ggplot(all_methods_counts, aes(y=n, x=method, fill=as.factor(N.contexts))) + 
  geom_bar(stat = "identity") + 
  labs(title = "Number of contexts tagged per utterance", y="Number of utterances", x = "Context defined by") + 
  scale_fill_discrete(name="Number of contexts")
ggsave(filename ="contexts_per_utt_1.png" , path = "graphs/descriptives", width = 5, height = 4, units="in")

HJ_no_none <- dplyr::select(HJ, -none, -N.contexts, -context) 
HJ_no_none$N.contexts <- rowSums(dplyr::select(HJ_no_none, -utt, -orth, -phon), na.rm = TRUE)
HJ_no_none <- extract_contexts(HJ_no_none)
  
hj_no_none <- dplyr::select(HJ_no_none, utt, N.contexts, context) %>%
  mutate(method = "coder judgments")

all_methods_counts_no_none <- rbind(wl_contexts, hj_no_none, lda_contexts, stm_contexts) %>% 
  count(method, N.contexts) 

ggplot(all_methods_counts_no_none, aes(y=n, x=method, fill=as.factor(N.contexts))) + 
  geom_bar(stat = "identity") + 
  labs(title = "Number of contexts tagged per utterance,\nnot including 'none' for coder judgments", y="Number of utterances", x = "Context defined by") + 
  scale_fill_discrete(name="Number of contexts")
ggsave(filename ="contexts_per_utt_2.png" , path = "graphs/descriptives", width = 5, height = 4, units="in")

table(hj_contexts$N.contexts)
table(hj_no_none$N.contexts)

```


```{r}
all_methods_context_count <- rbind(wl_contexts, hj_contexts, lda_contexts, stm_contexts) %>% 
  count(method, context)
all_methods_context_count$context <- relevel(all_methods_context_count$context, "none" )
all_methods_context_count$context <- relevel(all_methods_context_count$context, "no context tag" )
all_methods_context_count$context <- relevel(all_methods_context_count$context, "ambiguous" )

ggplot(all_methods_context_count, aes(y=n, x=method, fill=context)) + 
  geom_bar(stat= "identity") + 
  scale_fill_manual(values = c("#0072B2", "#D55E00", "#E69F00", rep("#999999", length(levels(all_methods_context_count$context)) - 3)))

```

Printing context files for sharing with CF:

```{r print_context_files}
all <- rbind(wl_contexts, hj_no_none, lda_contexts, stm_contexts) %>% 
  dplyr::select(utt, context, method) %>% 
  tidyr::extract(utt, into = c("child", "age", "utt.num"), regex = "^([[:alpha:]]{2})([[:digit:]]{2})[.]cha_([[:digit:]]+)$")

all$utt.num <- as.numeric(all$utt.num)
  
all %>% 
  dplyr::filter(method=="word list") %>% 
  tidyr::spread(key=utt.num, value=context) %>% 
  dplyr::select(-method) %>% 
  write.csv("/Users/TARDIS/Dropbox/2_RoseM_TP/context_files/contexts_file_WL.csv", row.names=FALSE)
all %>% 
  dplyr::filter(method=="coder judgments") %>% 
  tidyr::spread(key=utt.num, value=context) %>% 
  dplyr::select(-method) %>% 
  write.csv("/Users/TARDIS/Dropbox/2_RoseM_TP/context_files/contexts_file_HJ.csv", row.names=FALSE)  
all %>% 
  dplyr::filter(method=="LDA") %>% 
  tidyr::spread(key=utt.num, value=context) %>% 
  dplyr::select(-method) %>% 
  write.csv("/Users/TARDIS/Dropbox/2_RoseM_TP/context_files/contexts_file_LDA.csv", row.names=FALSE)
all %>% 
  dplyr::filter(method=="STM") %>% 
  tidyr::spread(key=utt.num, value=context) %>% 
  dplyr::select(-method) %>% 
  write.csv("/Users/TARDIS/Dropbox/2_RoseM_TP/context_files/contexts_file_STM.csv", row.names=FALSE)  
```

## Select context files to use

Dropping "none" codes from HJ.
```{r rop_none_codes}
WL <- WL
HJ <- HJ_no_none
LDA <- LDA 
STM <- STM 
```

## General thoughts
The word list method can (and does) leave large portions of the corpus uncoded, unlike the coder judgment or topic modeling definitions of context. 
Coders were instructed to respond with "none" if they encountered a section of the corpus where they could not identify any activity context; 
there are `r sum(df_HJ_bin$none, na.rm=TRUE)` utterances in the corpus that are above threshold for the "none" code, indicating agreement across coders that those utterances do not have an identifiable activity context. 
This is a much smaller portion of the corpus (`r 100*round(sum(df_HJ_bin$none, na.rm=TRUE)/nrow(df), 3)`%) than the uncoded section left by the word list method (`r 100*round(length(which(WL$N.contexts == 0)) / nrow(df), 3)`%), however. 

The word list and coder definitions of context naturally produce a skewed distribution of activities, whereas the topic modeling approaches discover a more uniform distribution of activities.
To the extent that the distritbution of activities is naturally skewed (e.g. that a few activities happen very often, and many others happen rarely), topic modeling approaches to identifying activity context may distort reality.

# Word list descriptives

## Frequency of WL key words?

```{r}
contexts <- names(WL_contexts)
WL_context_data <- data.frame(NULL)
for(k in contexts){
  WL_context_data <- rbind(WL_context_data, data.frame(word=WL_contexts[[k]], context=k, stringsAsFactors = FALSE))
}

orth_stream <- paste(df$orth, collapse = " ")
# flag bigrams from WL keywords in orth stream, so they don't get separated
for(w_bar in grep(x = WL_context_data$word, pattern = "_", value=TRUE)){
  w_space <- gsub(x=w_bar, pattern="_", replacement=" ")
  orth_stream <- gsub(x=orth_stream, pattern = w_space, replacement = w_bar)
}

orth_stream <- strsplit(orth_stream, split=" ")[[1]]
orth_stream <- orth_stream[orth_stream != ""]

orth_data <- data.frame(word=orth_stream, stringsAsFactors = FALSE) %>% 
  count(word)

WL_context_data <- left_join(WL_context_data, orth_data, by="word") %>% 
  arrange(context, n)

WL_context_data %>% 
  group_by(context) %>% 
  summarise(total=sum(n, na.rm=TRUE), 
            mean.freq=mean(n, na.rm=TRUE), 
            highest=max(n, na.rm=TRUE),
            which.highest=word[which.max(n)]) %>% 
  kable()
```


# HJ

## What raw coder tags make up each category?

```{r hj_wordle_codes, cache=TRUE}
HJ_contexts_list <- colnames(dplyr::select(df_HJ_bin, - utt, -orth, -phon))

HJ_codes_df <- HJ_processedcodes %>% 
  dplyr::mutate(value=1) %>% 
  tidyr::spread(key=category, value=value, fill=0) %>% 
  dplyr::rename(orth=context)

for(k in HJ_contexts_list){
  message(k)
  cloud_from_df(HJ_codes_df, which.context = k, min.freq = 0)
}
```

## What words are associated with each category?

```{r hj_wordle_orth, cache=TRUE}

for(k in HJ_contexts_list){
  message(k)
  cloud_from_df(df_HJ_bin, k)
}
```

## Do "none" contexts from HJ match utterances with 0 context codes from WL?

```{r none_and_0context, out.width='50%', out.height='50%'}
df_HJ_none <- df_HJ_bin
df_HJ_none$HJ_none <- ifelse(df_HJ_none$none==1, 1,
                             ifelse(df_HJ_none$none==0, 0, NA))
df_HJ_none <- dplyr::select(df_HJ_none, utt, HJ_none) 

df_WL_0 <- WL
df_WL_0$WL_0 <- ifelse(df_WL_0$N.contexts == 0, 1,
                             ifelse(df_WL_0$N.contexts > 0, 0, NA))
df_WL_0 <- dplyr::select(df_WL_0, utt, WL_0) 

match <- full_join(df_HJ_none, df_WL_0, by="utt") %>% 
  mutate(match = HJ_none + WL_0)
match$match <- ifelse(match$match == 2, 1, 
                      ifelse(match$match < 2, 0, NA))
nrow(df_WL_0); nrow(df_HJ_none); nrow(match)
tab <- xtabs( ~ WL_0 + HJ_none, data = match)

addmargins(tab)
summary(tab)
mosaic(tab)
assocplot(tab)
assocstats(tab)
```

What percent of the "none" context utterances in HJ method are 0 context in WL?
```{r}
sum(match$match, na.rm=TRUE) / sum(match$HJ_none, na.rm=TRUE)
```

What percent of the 0 context utterances in WL method are "none" context in HJ?
```{r}
sum(match$match, na.rm=TRUE) / sum(match$WL_0, na.rm=TRUE)
```

# Topic modeling: LDA
What words are associated with each LDA topic? 
```{r lda_topics}
top_words_lda <- top.topic.words(lda$topics, num.words=10) %>% 
  as.data.frame(stringsAsFactors = FALSE)
colnames(top_words_lda) <- paste0("topic_", 1:ncol(top_words_lda)) 
top_words_lda %>% 
  kable(caption="Top words for each topic, in order")
```


LDAvis: [http://bl.ocks.org/rosemm/raw/a7b1ac43ffe3b49229ed5e866762613f/](http://bl.ocks.org/rosemm/raw/a7b1ac43ffe3b49229ed5e866762613f/)
```{r lda_LDAvis, eval=FALSE}
# http://cpsievert.github.io/LDAvis/reviews/reviews.html
alpha <-  0.1 # from lda package demo
eta <-  0.1 # from lda package demo
theta <- t(apply(lda$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(lda$topics) + eta, 2, function(x) x/sum(x)))

D <- length(TM_doc_prep_out$documents)  # number of documents 
W <- length(TM_doc_prep_out$vocab)  # number of terms in the vocab
doc.length <- document.lengths(TM_doc_prep_out$documents)  # number of tokens per document
N <- sum(doc.length)  # total number of tokens in the data
term.frequency <- word.counts(TM_doc_prep_out$documents, vocab = TM_doc_prep_out$vocab)


lda_data <- list(phi = phi,
                 theta = theta,
                 doc.length = doc.length,
                 vocab = TM_doc_prep_out$vocab,
                 term.frequency = as.integer(term.frequency))

json <- createJSON(phi = phi, 
                   theta = theta, 
                   doc.length = doc.length, 
                   vocab = TM_doc_prep_out$vocab, 
                   term.frequency = as.integer(term.frequency))

serVis(json, as.gist = TRUE) 
```

Wordles, showing the frequencies of words in the utterances assigned to each topic.

```{r lda_wordles_count, cache=TRUE}
cloud_from_df(df_LDA_bin, "topic_1")
cloud_from_df(df_LDA_bin, "topic_2")
cloud_from_df(df_LDA_bin, "topic_3")
cloud_from_df(df_LDA_bin, "topic_4")
cloud_from_df(df_LDA_bin, "topic_5")
cloud_from_df(df_LDA_bin, "topic_6")
cloud_from_df(df_LDA_bin, "topic_7")
cloud_from_df(df_LDA_bin, "topic_8")
cloud_from_df(df_LDA_bin, "topic_9")
cloud_from_df(df_LDA_bin, "topic_10")
cloud_from_df(df_LDA_bin, "topic_11")
cloud_from_df(df_LDA_bin, "topic_12")
```

# Topic modeling: STM
What words are associated with each STM topic?
```{r stm_topics}
summary(stm)
```

```{r stm_LDAvis, eval=FALSE}
# http://cpsievert.github.io/LDAvis/reviews/reviews.html
toLDAvis(stm, TM_doc_prep_out$documents, as.gist = TRUE) # This function does not yet allow content covariates.
```

Wordles, showing the probability of each word given the topic.
```{r stm_wordles_prob, cache=TRUE}
cloud(stm, topic=1)
cloud(stm, topic=2)
cloud(stm, topic=3)
cloud(stm, topic=4)
cloud(stm, topic=5)
cloud(stm, topic=6)
cloud(stm, topic=7)
cloud(stm, topic=8)
cloud(stm, topic=9)
cloud(stm, topic=10)
cloud(stm, topic=11)
cloud(stm, topic=12)
```

Wordles, showing the frequencies of words in the utterances assigned to each topic.

```{r stm_wordles_count, cache=TRUE}
cloud_from_df(df_STM_bin, "topic_1")
cloud_from_df(df_STM_bin, "topic_2")
cloud_from_df(df_STM_bin, "topic_3")
cloud_from_df(df_STM_bin, "topic_4")
cloud_from_df(df_STM_bin, "topic_5")
cloud_from_df(df_STM_bin, "topic_6")
cloud_from_df(df_STM_bin, "topic_7")
cloud_from_df(df_STM_bin, "topic_8")
cloud_from_df(df_STM_bin, "topic_9")
cloud_from_df(df_STM_bin, "topic_10")
cloud_from_df(df_STM_bin, "topic_11")
cloud_from_df(df_STM_bin, "topic_12")
```
