---
output: pdf_document
---


```{r setup, message=FALSE}
knitr::opts_knit$set(root.dir = "/Users/TARDIS/Documents/STUDIES/context_word_seg")
```

```{r load_project, message=FALSE}
library(ProjectTemplate)
load.project()
```

```{r}
WL <- df_WL_bin
HJ <- df_HJ_bin
LDA <- df_LDA_bin
STM <- df_STM_bin
```


# Word list descriptives

## Frequency of WL key words?

```{r}
contexts <- names(WL_contexts)
WL_context_data <- data.frame(NULL)
for(k in contexts){
  WL_context_data <- rbind(WL_context_data, data.frame(word=WL_contexts[[k]], context=k, stringsAsFactors = FALSE))
}

orth_stream <- paste(df$orth, collapse = " ")
# flag bigrams from WL keywords in orth stream, so they don't get separated
for(w_bar in grep(x = WL_context_data$word, pattern = "_", value=TRUE)){
  w_space <- gsub(x=w_bar, pattern="_", replacement=" ")
  orth_stream <- gsub(x=orth_stream, pattern = w_space, replacement = w_bar)
}

orth_stream <- strsplit(orth_stream, split=" ")[[1]]
orth_stream <- orth_stream[orth_stream != ""]

orth_data <- data.frame(word=orth_stream, stringsAsFactors = FALSE) %>% 
  count(word)

WL_context_data <- left_join(WL_context_data, orth_data, by="word") %>% 
  arrange(context, n)

WL_context_data %>% 
  group_by(context) %>% 
  summarise(total=sum(n, na.rm=TRUE), 
            mean.freq=mean(n, na.rm=TRUE), 
            highest=max(n, na.rm=TRUE),
            which.highest=word[which.max(n)]) %>% 
  kable()
```


## What words are associated with each context?

```{r wl_wordles}

wordle_WL <- df_WL_bin %>% 
  as.tbl() %>% 
  gather(key="context", value="value", -utt, -orth, -phon) %>% 
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_WL <- wordle_WL %>% 
  do({
    word.stream <- base::strsplit(paste(.$orth, collapse = " "), split = " ")[[1]]
    word.stream <- word.stream[word.stream!=""]
    freq <- table(word.stream)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_WL %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  sort() %>% 
  unique()

if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}

temp <- wordle_WL %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "WL"), note="utts", title=TRUE)
    data.frame(NULL)
  })
```


# HJ

## What raw coder tags make up each category?

```{r hj_wordle_codes, cache=TRUE}
HJ_codes_df <- HJ_processedcodes %>% 
  dplyr::mutate(value=1) %>% 
  tidyr::spread(key=category, value=value, fill=0) %>% 
  dplyr::rename(orth=context)

wordle_HJ_codes <- HJ_codes_df %>% 
  as.tbl() %>% 
  dplyr::select(one_of(colnames(df_HJ_bin)[colnames(df_HJ_bin) %in% colnames(HJ_codes_df)])) %>% 
  gather(key="context", value="value", -utt, -orth) %>% 
  dplyr::filter(context != "none") %>% # excluding "none" codes
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_HJ_codes <- wordle_HJ_codes %>% 
  do({
    freq <- table(.$orth)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_HJ_codes %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  unique()
size.breaks <- sort(unique(c(min(freqs_wordle_HJ_codes$min), median(freqs_wordle_HJ_codes$min), size.breaks)))
if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}


temp <- wordle_HJ_codes %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "HJ"), note="codes", standardize.font=FALSE)
    data.frame(NULL)
  })
  
```

## What words are associated with each category?

```{r hj_wordle_orth}

wordle_HJ <- df_HJ_bin %>% 
  as.tbl() %>% 
  gather(key="context", value="value", -utt, -orth, -phon) %>% 
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_HJ <- wordle_HJ %>% 
  do({
    word.stream <- base::strsplit(paste(.$orth, collapse = " "), split = " ")[[1]]
    word.stream <- word.stream[word.stream!=""]
    freq <- table(word.stream)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_HJ %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  sort() %>% 
  unique()

if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}

temp <- wordle_HJ %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "HJ"), note="utts", title=TRUE)
    data.frame(NULL)
  })
  
```

## Do "none" contexts from HJ match utterances with 0 context codes from WL?

```{r none_and_0context, out.width='50%', out.height='50%', eval=FALSE}
df_HJ_none <- df_HJ_bin
df_HJ_none$HJ_none <- ifelse(df_HJ_none$none==1, 1,
                             ifelse(df_HJ_none$none==0, 0, NA))
df_HJ_none <- dplyr::select(df_HJ_none, utt, HJ_none) 

df_WL_0 <- WL
df_WL_0$WL_0 <- ifelse(df_WL_0$N.contexts == 0, 1,
                             ifelse(df_WL_0$N.contexts > 0, 0, NA))
df_WL_0 <- dplyr::select(df_WL_0, utt, WL_0) 

match <- full_join(df_HJ_none, df_WL_0, by="utt") %>% 
  mutate(match = HJ_none + WL_0)
match$match <- ifelse(match$match == 2, 1, 
                      ifelse(match$match < 2, 0, NA))
nrow(df_WL_0); nrow(df_HJ_none); nrow(match)
tab <- xtabs( ~ WL_0 + HJ_none, data = match)

addmargins(tab)
summary(tab)
mosaic(tab)
assocplot(tab)
assocstats(tab)
```

What percent of the "none" context utterances in HJ method are 0 context in WL?
```{r, eval=F}
sum(match$match, na.rm=TRUE) / sum(match$HJ_none, na.rm=TRUE)
```

What percent of the 0 context utterances in WL method are "none" context in HJ?
```{r, eval=F}
sum(match$match, na.rm=TRUE) / sum(match$WL_0, na.rm=TRUE)
```

# Topic modeling: LDA
What words are associated with each LDA topic? 
```{r lda_topics}
top_words_lda <- top.topic.words(lda$topics, num.words=10) %>% 
  as.data.frame(stringsAsFactors = FALSE)
colnames(top_words_lda) <- paste0("topic_", 1:ncol(top_words_lda)) 
top_words_lda %>% 
  kable(caption="Top words for each LDA topic, in order")
```


LDAvis: [http://bl.ocks.org/rosemm/raw/a7b1ac43ffe3b49229ed5e866762613f/](http://bl.ocks.org/rosemm/raw/a7b1ac43ffe3b49229ed5e866762613f/)
```{r lda_LDAvis, eval=FALSE}
# http://cpsievert.github.io/LDAvis/reviews/reviews.html
alpha <-  0.1 # from lda package demo
eta <-  0.1 # from lda package demo
theta <- t(apply(lda$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(lda$topics) + eta, 2, function(x) x/sum(x)))

D <- length(TM_doc_prep_out$documents)  # number of documents 
W <- length(TM_doc_prep_out$vocab)  # number of terms in the vocab
doc.length <- document.lengths(TM_doc_prep_out$documents)  # number of tokens per document
N <- sum(doc.length)  # total number of tokens in the data
term.frequency <- word.counts(TM_doc_prep_out$documents, vocab = TM_doc_prep_out$vocab)


lda_data <- list(phi = phi,
                 theta = theta,
                 doc.length = doc.length,
                 vocab = TM_doc_prep_out$vocab,
                 term.frequency = as.integer(term.frequency))

json <- createJSON(phi = phi, 
                   theta = theta, 
                   doc.length = doc.length, 
                   vocab = TM_doc_prep_out$vocab, 
                   term.frequency = as.integer(term.frequency))

serVis(json, as.gist = TRUE) 
```

Wordles, showing the frequencies of words in the utterances assigned to each topic.


NOTE: set font size to proportion of total codes, so it's comparable across wordles (not just biggest font for plurality)

```{r lda_wordles_count, cache=TRUE}

wordle_LDA <- df_LDA_bin %>% 
  as.tbl() %>% 
  gather(key="context", value="value", -utt, -orth, -phon) %>% 
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_LDA <- wordle_LDA %>% 
  do({
    word.stream <- base::strsplit(paste(.$orth, collapse = " "), split = " ")[[1]]
    word.stream <- word.stream[word.stream!=""]
    freq <- table(word.stream)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_LDA %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  sort() %>% 
  unique()

if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}
temp <- wordle_LDA %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "LDA"), note="utts", standardize.font=TRUE, title=TRUE)
    data.frame(NULL)
  })
```

# Topic modeling: STM
What words are associated with each STM topic?
```{r stm_topics}
summary(stm)
```

```{r stm_LDAvis, eval=FALSE}
# http://cpsievert.github.io/LDAvis/reviews/reviews.html
toLDAvis(stm, TM_doc_prep_out$documents, as.gist = TRUE) # This function does not yet allow content covariates.
```

Wordles, showing the probability of each word given the topic.
```{r stm_wordles_prob, eval=FALSE}
for(k in 1:stm$settings$dim$K){
  cloud(stm, topic=k)
  dev.copy2pdf(file=file.path("graphs","STM", paste0("wordcloud_probs_topic_", k, ".pdf")), width=5, height=4)
  dev.off()
}
```

Wordles, showing the frequencies of words in the utterances assigned to each topic.

```{r stm_wordles_count, cache=TRUE}
wordle_STM <- df_STM_bin %>% 
  as.tbl() %>% 
  gather(key="context", value="value", -utt, -orth, -phon) %>% 
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_STM <- wordle_STM %>% 
  do({
    word.stream <- base::strsplit(paste(.$orth, collapse = " "), split = " ")[[1]]
    word.stream <- word.stream[word.stream!=""]
    freq <- table(word.stream)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_STM %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  sort() %>% 
  unique()

if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}
temp <- wordle_STM %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "STM"), note="utts", standardize.font=TRUE, title=TRUE)
    data.frame(NULL)
  })

```
