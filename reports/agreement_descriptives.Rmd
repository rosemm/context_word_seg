---
output: pdf_document
---


```{r setup, message=FALSE}
knitr::opts_knit$set(root.dir = "/Users/TARDIS/Documents/STUDIES/context_word_seg")
```

```{r load_project, message=FALSE}
library(ProjectTemplate)
load.project()
```

```{r}
WL <- df_WL_bin
HJ <- df_HJ_bin
HJ_no_none <- select(df_HJ_bin, -none)
LDA <- df_LDA_bin
STM <- df_STM_bin
```


Printing context files for sharing with CF:

```{r print_context_files, eval = FALSE}
all <- rbind(wl_contexts, hj_no_none, lda_contexts, stm_contexts) %>% 
  dplyr::select(utt, context, method) %>% 
  tidyr::extract(utt, into = c("child", "age", "utt.num"), regex = "^([[:alpha:]]{2})([[:digit:]]{2})[.]cha_([[:digit:]]+)$")

all$utt.num <- as.numeric(all$utt.num)
  
all %>% 
  dplyr::filter(method=="word list") %>% 
  tidyr::spread(key=utt.num, value=context) %>% 
  dplyr::select(-method) %>% 
  write.csv("/Users/TARDIS/Dropbox/2_RoseM_TP/context_files/contexts_file_WL.csv", row.names=FALSE)
all %>% 
  dplyr::filter(method=="coder judgments") %>% 
  tidyr::spread(key=utt.num, value=context) %>% 
  dplyr::select(-method) %>% 
  write.csv("/Users/TARDIS/Dropbox/2_RoseM_TP/context_files/contexts_file_HJ.csv", row.names=FALSE)  
all %>% 
  dplyr::filter(method=="LDA") %>% 
  tidyr::spread(key=utt.num, value=context) %>% 
  dplyr::select(-method) %>% 
  write.csv("/Users/TARDIS/Dropbox/2_RoseM_TP/context_files/contexts_file_LDA.csv", row.names=FALSE)
all %>% 
  dplyr::filter(method=="STM") %>% 
  tidyr::spread(key=utt.num, value=context) %>% 
  dplyr::select(-method) %>% 
  write.csv("/Users/TARDIS/Dropbox/2_RoseM_TP/context_files/contexts_file_STM.csv", row.names=FALSE)  
```

## Select context files to use

Dropping "none" codes from HJ.
```{r rop_none_codes}
WL <- WL
HJ <- HJ_no_none
LDA <- LDA 
STM <- STM 
```

# Word list descriptives

## Frequency of WL key words?

```{r}
contexts <- names(WL_contexts)
WL_context_data <- data.frame(NULL)
for(k in contexts){
  WL_context_data <- rbind(WL_context_data, data.frame(word=WL_contexts[[k]], context=k, stringsAsFactors = FALSE))
}

orth_stream <- paste(df$orth, collapse = " ")
# flag bigrams from WL keywords in orth stream, so they don't get separated
for(w_bar in grep(x = WL_context_data$word, pattern = "_", value=TRUE)){
  w_space <- gsub(x=w_bar, pattern="_", replacement=" ")
  orth_stream <- gsub(x=orth_stream, pattern = w_space, replacement = w_bar)
}

orth_stream <- strsplit(orth_stream, split=" ")[[1]]
orth_stream <- orth_stream[orth_stream != ""]

orth_data <- data.frame(word=orth_stream, stringsAsFactors = FALSE) %>% 
  count(word)

WL_context_data <- left_join(WL_context_data, orth_data, by="word") %>% 
  arrange(context, n)

WL_context_data %>% 
  group_by(context) %>% 
  summarise(total=sum(n, na.rm=TRUE), 
            mean.freq=mean(n, na.rm=TRUE), 
            highest=max(n, na.rm=TRUE),
            which.highest=word[which.max(n)]) %>% 
  kable()
```


# HJ



## What raw coder tags make up each category?

```{r hj_wordle_codes, cache=TRUE}
HJ_contexts_list <- colnames(dplyr::select(df_HJ_bin, - utt, -orth, -phon))

HJ_codes_df <- HJ_processedcodes %>% 
  dplyr::mutate(value=1) %>% 
  tidyr::spread(key=category, value=value, fill=0) %>% 
  dplyr::rename(orth=context)

for(k in HJ_contexts_list){
  message(k)
  cloud_from_df(HJ_codes_df, which.context = k, min.freq = 0)
}
```

## What words are associated with each category?

```{r hj_wordle_orth, cache=TRUE}

for(k in HJ_contexts_list){
  message(k)
  cloud_from_df(df_HJ_bin, k)
}
```

## Do "none" contexts from HJ match utterances with 0 context codes from WL?

```{r none_and_0context, out.width='50%', out.height='50%', eval=FALSE}
df_HJ_none <- df_HJ_bin
df_HJ_none$HJ_none <- ifelse(df_HJ_none$none==1, 1,
                             ifelse(df_HJ_none$none==0, 0, NA))
df_HJ_none <- dplyr::select(df_HJ_none, utt, HJ_none) 

df_WL_0 <- WL
df_WL_0$WL_0 <- ifelse(df_WL_0$N.contexts == 0, 1,
                             ifelse(df_WL_0$N.contexts > 0, 0, NA))
df_WL_0 <- dplyr::select(df_WL_0, utt, WL_0) 

match <- full_join(df_HJ_none, df_WL_0, by="utt") %>% 
  mutate(match = HJ_none + WL_0)
match$match <- ifelse(match$match == 2, 1, 
                      ifelse(match$match < 2, 0, NA))
nrow(df_WL_0); nrow(df_HJ_none); nrow(match)
tab <- xtabs( ~ WL_0 + HJ_none, data = match)

addmargins(tab)
summary(tab)
mosaic(tab)
assocplot(tab)
assocstats(tab)
```

What percent of the "none" context utterances in HJ method are 0 context in WL?
```{r, eval=F}
sum(match$match, na.rm=TRUE) / sum(match$HJ_none, na.rm=TRUE)
```

What percent of the 0 context utterances in WL method are "none" context in HJ?
```{r, eval=F}
sum(match$match, na.rm=TRUE) / sum(match$WL_0, na.rm=TRUE)
```

# Topic modeling: LDA
What words are associated with each LDA topic? 
```{r lda_topics}
top_words_lda <- top.topic.words(lda$topics, num.words=10) %>% 
  as.data.frame(stringsAsFactors = FALSE)
colnames(top_words_lda) <- paste0("topic_", 1:ncol(top_words_lda)) 
top_words_lda %>% 
  kable(caption="Top words for each LDA topic, in order")
```


LDAvis: [http://bl.ocks.org/rosemm/raw/a7b1ac43ffe3b49229ed5e866762613f/](http://bl.ocks.org/rosemm/raw/a7b1ac43ffe3b49229ed5e866762613f/)
```{r lda_LDAvis, eval=FALSE}
# http://cpsievert.github.io/LDAvis/reviews/reviews.html
alpha <-  0.1 # from lda package demo
eta <-  0.1 # from lda package demo
theta <- t(apply(lda$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(lda$topics) + eta, 2, function(x) x/sum(x)))

D <- length(TM_doc_prep_out$documents)  # number of documents 
W <- length(TM_doc_prep_out$vocab)  # number of terms in the vocab
doc.length <- document.lengths(TM_doc_prep_out$documents)  # number of tokens per document
N <- sum(doc.length)  # total number of tokens in the data
term.frequency <- word.counts(TM_doc_prep_out$documents, vocab = TM_doc_prep_out$vocab)


lda_data <- list(phi = phi,
                 theta = theta,
                 doc.length = doc.length,
                 vocab = TM_doc_prep_out$vocab,
                 term.frequency = as.integer(term.frequency))

json <- createJSON(phi = phi, 
                   theta = theta, 
                   doc.length = doc.length, 
                   vocab = TM_doc_prep_out$vocab, 
                   term.frequency = as.integer(term.frequency))

serVis(json, as.gist = TRUE) 
```

Wordles, showing the frequencies of words in the utterances assigned to each topic.


NOTE: set font size to proportion of total codes, so it's comparable across wordles (not just biggest font for plurality)

```{r lda_wordles_count, cache=TRUE}
cloud_from_df(df_LDA_bin, "topic_1")
cloud_from_df(df_LDA_bin, "topic_2")
cloud_from_df(df_LDA_bin, "topic_3")
cloud_from_df(df_LDA_bin, "topic_4")
cloud_from_df(df_LDA_bin, "topic_5")
cloud_from_df(df_LDA_bin, "topic_6")
cloud_from_df(df_LDA_bin, "topic_7")
cloud_from_df(df_LDA_bin, "topic_8")
cloud_from_df(df_LDA_bin, "topic_9")
cloud_from_df(df_LDA_bin, "topic_10")
cloud_from_df(df_LDA_bin, "topic_11")
cloud_from_df(df_LDA_bin, "topic_12")
```

# Topic modeling: STM
What words are associated with each STM topic?
```{r stm_topics}
summary(stm)
```

```{r stm_LDAvis, eval=FALSE}
# http://cpsievert.github.io/LDAvis/reviews/reviews.html
toLDAvis(stm, TM_doc_prep_out$documents, as.gist = TRUE) # This function does not yet allow content covariates.
```

Wordles, showing the probability of each word given the topic.
```{r stm_wordles_prob, cache=TRUE}
cloud(stm, topic=1)
cloud(stm, topic=2)
cloud(stm, topic=3)
cloud(stm, topic=4)
cloud(stm, topic=5)
cloud(stm, topic=6)
cloud(stm, topic=7)
cloud(stm, topic=8)
cloud(stm, topic=9)
cloud(stm, topic=10)
cloud(stm, topic=11)
cloud(stm, topic=12)
```

Wordles, showing the frequencies of words in the utterances assigned to each topic.

```{r stm_wordles_count, cache=TRUE}
cloud_from_df(df_STM_bin, "topic_1")
cloud_from_df(df_STM_bin, "topic_2")
cloud_from_df(df_STM_bin, "topic_3")
cloud_from_df(df_STM_bin, "topic_4")
cloud_from_df(df_STM_bin, "topic_5")
cloud_from_df(df_STM_bin, "topic_6")
cloud_from_df(df_STM_bin, "topic_7")
cloud_from_df(df_STM_bin, "topic_8")
cloud_from_df(df_STM_bin, "topic_9")
cloud_from_df(df_STM_bin, "topic_10")
cloud_from_df(df_STM_bin, "topic_11")
cloud_from_df(df_STM_bin, "topic_12")
```
