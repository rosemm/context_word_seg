---
output: pdf_document
---


```{r setup, message=FALSE}
knitr::opts_knit$set(root.dir = "/Users/TARDIS/Documents/STUDIES/context_word_seg")
```

```{r load_project, message=FALSE}
library(ProjectTemplate)
load.project()
```

```{r}
WL <- df_WL_bin
HJ <- df_HJ_bin
LDA <- df_LDA_bin
STM <- df_STM_bin
```


# Word list descriptives

## Frequency of WL key words?

```{r}
contexts <- names(WL_contexts)
WL_context_data <- data.frame(NULL)
for(k in contexts){
  WL_context_data <- rbind(WL_context_data, data.frame(word=WL_contexts[[k]], context=k, stringsAsFactors = FALSE))
}

orth_stream <- paste(df$orth, collapse = " ")
# flag bigrams from WL keywords in orth stream, so they don't get separated
for(w_bar in grep(x = WL_context_data$word, pattern = "_", value=TRUE)){
  w_space <- gsub(x=w_bar, pattern="_", replacement=" ")
  orth_stream <- gsub(x=orth_stream, pattern = w_space, replacement = w_bar)
}

orth_stream <- strsplit(orth_stream, split=" ")[[1]]
orth_stream <- orth_stream[orth_stream != ""]

orth_data <- data.frame(word=orth_stream, stringsAsFactors = FALSE) %>% 
  count(word)

WL_context_data <- left_join(WL_context_data, orth_data, by="word") %>% 
  arrange(context, n)

WL_context_data %>% 
  group_by(context) %>% 
  summarise(total=sum(n, na.rm=TRUE), 
            mean.freq=mean(n, na.rm=TRUE), 
            highest=max(n, na.rm=TRUE),
            which.highest=word[which.max(n)]) %>% 
  kable()
```


## What words are associated with each context?

```{r wl_wordles}

wordle_WL <- df_WL_bin %>% 
  as.tbl() %>% 
  gather(key="context", value="value", -utt, -orth, -phon) %>% 
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_WL <- wordle_WL %>% 
  do({
    word.stream <- base::strsplit(paste(.$orth, collapse = " "), split = " ")[[1]]
    word.stream <- word.stream[word.stream!=""]
    freq <- table(word.stream)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_WL %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  sort() %>% 
  unique()

if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}

temp <- wordle_WL %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "WL"), note="utts", title=TRUE)
    data.frame(NULL)
  })
```


# HJ

## What raw coder tags make up each category?

```{r hj_wordle_codes, cache=TRUE}
HJ_codes_df <- HJ_processedcodes %>% 
  dplyr::mutate(value=1) %>% 
  tidyr::spread(key=category, value=value, fill=0) %>% 
  dplyr::rename(orth=context)

wordle_HJ_codes <- HJ_codes_df %>% 
  as.tbl() %>% 
  dplyr::select(one_of(colnames(df_HJ_bin)[colnames(df_HJ_bin) %in% colnames(HJ_codes_df)])) %>% 
  gather(key="context", value="value", -utt, -orth) %>% 
  dplyr::filter(context != "none") %>% # excluding "none" codes
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_HJ_codes <- wordle_HJ_codes %>% 
  do({
    freq <- table(.$orth)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_HJ_codes %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  unique()
size.breaks <- sort(unique(c(min(freqs_wordle_HJ_codes$min), median(freqs_wordle_HJ_codes$min), size.breaks)))
if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}


temp <- wordle_HJ_codes %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "HJ"), note="codes", standardize.font=FALSE)
    data.frame(NULL)
  })
```

## What words are associated with each category?

```{r hj_wordle_orth}
wordle_HJ <- df_HJ_bin %>% 
  as.tbl() %>% 
  gather(key="context", value="value", -utt, -orth, -phon) %>% 
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_HJ <- wordle_HJ %>% 
  do({
    word.stream <- base::strsplit(paste(.$orth, collapse = " "), split = " ")[[1]]
    word.stream <- word.stream[word.stream!=""]
    freq <- table(word.stream)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_HJ %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  sort() %>% 
  unique()

if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}

temp <- wordle_HJ %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "HJ"), note="utts", title=TRUE)
    data.frame(NULL)
  })
  
```

## Do "none" contexts from HJ match utterances with 0 context codes from WL?

```{r none_and_0context, out.width='50%', out.height='50%', eval=FALSE}
df_HJ_none <- df_HJ_bin
df_HJ_none$HJ_none <- ifelse(df_HJ_none$none==1, 1,
                             ifelse(df_HJ_none$none==0, 0, NA))
df_HJ_none <- dplyr::select(df_HJ_none, utt, HJ_none) 

df_WL_0 <- WL
df_WL_0$WL_0 <- ifelse(df_WL_0$N.contexts == 0, 1,
                             ifelse(df_WL_0$N.contexts > 0, 0, NA))
df_WL_0 <- dplyr::select(df_WL_0, utt, WL_0) 

match <- full_join(df_HJ_none, df_WL_0, by="utt") %>% 
  mutate(match = HJ_none + WL_0)
match$match <- ifelse(match$match == 2, 1, 
                      ifelse(match$match < 2, 0, NA))
nrow(df_WL_0); nrow(df_HJ_none); nrow(match)
tab <- xtabs( ~ WL_0 + HJ_none, data = match)

addmargins(tab)
summary(tab)
mosaic(tab)
assocplot(tab)
assocstats(tab)
```

What percent of the "none" context utterances in HJ method are 0 context in WL?
```{r, eval=F}
sum(match$match, na.rm=TRUE) / sum(match$HJ_none, na.rm=TRUE)
```

What percent of the 0 context utterances in WL method are "none" context in HJ?
```{r, eval=F}
sum(match$match, na.rm=TRUE) / sum(match$WL_0, na.rm=TRUE)
```

# Topic modeling: LDA
What words are associated with each LDA topic? 
```{r lda_topics}
top_words_lda <- top.topic.words(lda$topics, num.words=10) %>% 
  as.data.frame(stringsAsFactors = FALSE)
colnames(top_words_lda) <- paste0("topic_", 1:ncol(top_words_lda)) 
top_words_lda %>% 
  kable(caption="Top words for each LDA topic, in order")
```


LDAvis: [http://bl.ocks.org/rosemm/raw/a7b1ac43ffe3b49229ed5e866762613f/](http://bl.ocks.org/rosemm/raw/a7b1ac43ffe3b49229ed5e866762613f/)
```{r lda_LDAvis, eval=FALSE}
# http://cpsievert.github.io/LDAvis/reviews/reviews.html
alpha <-  0.1 # from lda package demo
eta <-  0.1 # from lda package demo
theta <- t(apply(lda$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(lda$topics) + eta, 2, function(x) x/sum(x)))

D <- length(TM_doc_prep_out$documents)  # number of documents 
W <- length(TM_doc_prep_out$vocab)  # number of terms in the vocab
doc.length <- document.lengths(TM_doc_prep_out$documents)  # number of tokens per document
N <- sum(doc.length)  # total number of tokens in the data
term.frequency <- word.counts(TM_doc_prep_out$documents, vocab = TM_doc_prep_out$vocab)


lda_data <- list(phi = phi,
                 theta = theta,
                 doc.length = doc.length,
                 vocab = TM_doc_prep_out$vocab,
                 term.frequency = as.integer(term.frequency))

json <- createJSON(phi = phi, 
                   theta = theta, 
                   doc.length = doc.length, 
                   vocab = TM_doc_prep_out$vocab, 
                   term.frequency = as.integer(term.frequency))

serVis(json, as.gist = TRUE) 
```

Wordles, showing the frequencies of words in the utterances assigned to each topic.


NOTE: set font size to proportion of total codes, so it's comparable across wordles (not just biggest font for plurality)

```{r lda_wordles_count, cache=TRUE}

wordle_LDA <- df_LDA_bin %>% 
  as.tbl() %>% 
  gather(key="context", value="value", -utt, -orth, -phon) %>% 
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_LDA <- wordle_LDA %>% 
  do({
    word.stream <- base::strsplit(paste(.$orth, collapse = " "), split = " ")[[1]]
    word.stream <- word.stream[word.stream!=""]
    freq <- table(word.stream)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_LDA %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  sort() %>% 
  unique()

if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}
temp <- wordle_LDA %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "LDA"), note="utts", standardize.font=TRUE, title=TRUE)
    data.frame(NULL)
  })
```

# Topic modeling: STM
What words are associated with each STM topic?
```{r stm_topics}
summary(stm)
```

```{r stm_LDAvis, eval=FALSE}
# http://cpsievert.github.io/LDAvis/reviews/reviews.html
toLDAvis(stm, TM_doc_prep_out$documents, as.gist = TRUE) # This function does not yet allow content covariates.
```

Wordles, showing the probability of each word given the topic.
```{r stm_wordles_prob, eval=FALSE}
for(k in 1:stm$settings$dim$K){
  cloud(stm, topic=k)
  dev.copy2pdf(file=file.path("graphs","STM", paste0("wordcloud_probs_topic_", k, ".pdf")), width=5, height=4)
  dev.off()
}
```

Wordles, showing the frequencies of words in the utterances assigned to each topic.

```{r stm_wordles_count, cache=TRUE}
wordle_STM <- df_STM_bin %>% 
  as.tbl() %>% 
  gather(key="context", value="value", -utt, -orth, -phon) %>% 
  dplyr::filter(value==1) %>% 
  group_by(context)

freqs_wordle_STM <- wordle_STM %>% 
  do({
    word.stream <- base::strsplit(paste(.$orth, collapse = " "), split = " ")[[1]]
    word.stream <- word.stream[word.stream!=""]
    freq <- table(word.stream)
    data.frame(min=min(freq), Q25=quantile(freq, .25), Q50=quantile(freq, .5), Q75=quantile(freq, .75), Q90=quantile(freq, .9), Q95=quantile(freq, .95), Q99=quantile(freq, .99), max=max(freq))
  })

min.freq <- 2

size.breaks <- freqs_wordle_STM %>% 
  ungroup() %>% 
  dplyr::select(-context) %>% 
  summarize_all(max) %>% 
  as.integer() %>% 
  sort() %>% 
  unique()

if( min(size.breaks) < min.freq ){
  i <- 1
  while( min(size.breaks) < min.freq ) {
    size.breaks[i] <- min.freq
    i <- i + 1
  }
  size.breaks <- unique(size.breaks)
}
temp <- wordle_STM %>% 
  do({
    cloud_from_df(., which.context=unique(.$context), min.freq=min.freq, size.breaks=size.breaks, save.to = file.path("graphs", "STM"), note="utts", standardize.font=TRUE, title=TRUE)
    data.frame(NULL)
  })

```

Initial plan was to undertake continuous version of these analyses as well, but that isn't possible. 
Note that one would not expect the continuous and categorical versions of the anlayses to diverge very much, for a variety of reasons [...]. 

# Assessing overlap across all three methods: LCA
```{r}
cat_WL_HJ[[3]]
cat_WL_LDA[[3]]
cat_WL_STM[[3]]
cat_HJ_STM[[3]]
cat_HJ_LDA[[3]]
cat_STM_LDA[[3]]

v_table <- data.frame(method=c("WL","HJ", "LDA", "STM"), 
                      WL=c(NA, cat_WL_HJ[[3]]$cramer, cat_WL_LDA[[3]]$cramer, cat_WL_STM[[3]]$cramer), 
                      HJ=c(NA, NA, cat_HJ_LDA[[3]]$cramer, cat_HJ_STM[[3]]$cramer), 
                      LDA=c(NA, NA, NA, cat_STM_LDA[[3]]$cramer), 
                      STM=c(NA, NA, NA, NA))
kable(v_table, caption = "Measure of agreement across methods (Cramer's V)")


# including LCA
v_table <- data.frame(method=c("WL","HJ", "STM", "LCA"), 
                      WL=c(NA, cat_WL_HJ[[3]]$cramer, cat_WL_STM[[3]]$cramer, cat_LCA_WL[[3]]$cramer), 
                      HJ=c(NA, NA, cat_HJ_STM[[3]]$cramer, cat_LCA_HJ[[3]]$cramer), 
                      STM=c(NA, NA, NA, cat_LCA_STM[[3]]$cramer), 
                      LCA=c(NA, NA, NA, NA))
kable(v_table, caption = "Measure of agreement across methods (Cramer's V)")
```


