```{r}
rm(list = ls()) # clear the environment
library(dplyr)
library(tidyr)
library(ggplot2)
library(reshape2)
library(knitr)
library(devtools)
library(stm)
library(vcd)
library(car)

# read in the functions written for this analysis
source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/data_processing_functions.r")
setwd("/Users/TARDIS/Documents/STUDIES/context_word_seg")
```


## translate orth to phon (this also reads in the dict file):
```{r}
# to translate CHILDES transcripts to phon approximations using Swingley's dictionary:
# source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/data_processing_orth_to_phon.R")

# after the first time, we can just read in the saved results from the above code
dict <- read.table("dict_all3_updated.txt", header=1, sep="\t", quote="", comment.char ="")
df <- read.table("utt_orth_phon_KEY.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") # this gets used for word-lists contexts, it will get over-written for other contexts

if( length(df$orth[grepl(x=df$orth, pattern="[[:upper:]]")]) > 0 )  df$orth <- tolower(df$orth) # make sure the orth stream is all lower case
if( length(df$phon[grepl(x=df$phon, pattern="-", fixed=T)])  > 0 )  df$phon <- gsub(x=df$phon, pattern="-", replacement=" ", fixed=T) # make sure all word-internal syllable boundaries "-" are represnted just the same as between-word syllable boundaries (space)


# add frequency for each word to dictionary
dict$freq.orth <- NA
search_in <- strsplit(paste(df$orth, collapse=" "), split=" ", fixed=T)[[1]] # look for it in the vector of words (df$orth, collapsed into one paste, and then split into a unit for each word)
for(i in 1:nrow(dict)){
  search_for <- paste("^",dict$word[i],"$", sep="") # look for the ith word in the dictionary
  dict$freq.orth[i] <- length(grep(pattern=search_for, x=search_in)) # how many times does it find the ith dict word in the orth stream?
}
```
  
## read in context word list
```{r}
contexts <- read.csv("/Users/TARDIS/Documents/STUDIES/context_word_seg/words by contexts.csv")

# mark seed word bigrams in df$orth
bigrams <- grep(pattern="_", x=unlist(as.list(contexts)), fixed=T, value=T) # all of the context seed words that are bigrams (have an _)
for(b in 1:length(bigrams)){
  search_for <- paste0(gsub(x=bigrams[[b]], pattern="_", replacement=" ")) # look for the bigram, but with a space instead of the _
  replace_with <- bigrams[[b]] # replace it with the bigram
  df$orth <- gsub(pattern=search_for, x=df$orth, replacement=replace_with)
}
```


## add word context to dictionary dataframe and plot
```{r}
# add context for each word to dictionary
c <- gather(contexts, key="context", value="word") %>%
  filter(word!="")
c <- left_join(c, dict) 

c.missed <- filter(c, grepl(x=word, pattern="_", fixed=T)) # grab the bigrams
search_in <- df$orth
for(i in 1:nrow(c.missed)){
  search_for <- c.missed[i, ]$word
  count <-  length(grep(x=search_in, pattern=search_for)) 
  c.missed[i, ]$freq.orth <- ifelse(count==0, NA, count)
}
c <- rbind(c, c.missed)
c <- filter(c, !is.na(freq.orth) & freq.orth > 0) %>% 
  select(context, word, freq=freq.orth) %>%
  arrange(desc(freq))
c$word.num <- 1:nrow(c) # adding a number identifier for each word (in desc freq order), to control order of plotting

con.freq <- c   %>%
  group_by(context) %>%
  summarize(mean=mean(freq), max=max(freq), min=min(freq), median=median(freq))

ggplot(c, aes(x=as.factor(word.num), y=freq, fill=context)) + 
  geom_bar(position=position_dodge(), stat="identity", show_guide=F) +
  facet_wrap(~context, scales="free", ncol=4) +
  theme(text = element_text(size=20), axis.ticks = element_blank(), axis.text.x = element_blank() ) +
  labs(x=NULL, y=NULL)
ggsave(filename="plots/wordlists/key_word_freq_by_context.png", width=12, height=5, units="in")
```

## only keep context words that actually occur in the corpus
(this is only relevant for presenting the list, e.g. in a talk - the code works fine with extra words in there)
```{r contexts_occuring}
contexts.list <- as.list(contexts)
for(i in 1:length(names(contexts.list))){
  this.context <- contexts.list[[i]][contexts.list[[i]] %in% global.data$streams$orth.stream] # the context words that show up in the corpus
  this.context <- c(as.character(this.context), rep("", nrow(contexts)-length(this.context))) # add empty values at the end to make all of the columns the same length
  contexts.list[[i]] <- this.context
}
contexts.occuring <- as.data.frame(contexts.list)
contexts.occuring <- contexts.occuring[-which(apply(contexts.occuring,1,function(x)all(x==""))),] # remove empty rows at the end
kable(contexts.occuring)
write.table(contexts.occuring, file="contexts.occuring.csv", sep=",", row.names=F)
############################
```

## code contexts in df by word lists
```{r}
df <- df[ , 1:3] # only keep the first three columns (there should only be three columns anyway, but just to be sure)

temp <- unique(df$orth) # to speed up processing, only code each unique utterance once (then we'll join it back to the full df)
temp.codes <- data.frame(orth=temp)
for(k in 1:length(names(contexts))){
  
  temp.codes[[names(contexts)[k]]] <- 0 # make a new column for this context
  words <- as.character(unique(contexts[,k])) # this word list
  words <- words[words !=""] # drop empty character element in the list
  
  for(w in 1:length(words)){
    # for every orth entry that contains this word, put a 1 in this context's column
    temp.codes[[names(contexts)[k]]][grep(pattern=paste0("\\<", words[w], "\\>"), x=temp.codes$orth )] <- 1 
  }
}
df <- left_join(df, temp.codes) # join temp.codes back to full df

df <- expand_windows(df, context.names=names(contexts)) # extend context codes 2 utterances before and after each hit

write.table(df, file="contexts_WL.txt", quote=F, col.names=T, row.names=F, append=F, sep="\t")
# df <- read.table("contexts_WL.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") # this overwrites the word list context df above
```

## tag contexts by human coder judgments
```{r}
# run code in context_coding_cleaning.r to retrieve and clean codes
df <- read.table("contexts_HJ_prop.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") # this overwrites the word list context df above
```

## tag contexts by topic modeling
![](stm_comparison.png)

```{r topic_modeling_data_wrangling}
library(stm)
library(lda)

df <- df[ , 1:3] # only keep the first three columns (there should only be three columns anyway, but just to be sure)

df.wn.count <- df %>%
  select(-phon) %>%
  extract(col=utt, into=c("child", "age_weeks"), regex="^([[:alpha:]]{2})([[:digit:]]{2})", remove=FALSE) %>%
  extract(col=utt, into=c("file", "utt.num"), regex="^([[:alpha:]]{2}[[:digit:]]{2})[.]cha_([[:digit:]]+)", remove=FALSE) %>%
  filter(child != "hi") # remove this child, since the transcripts are so short

wn <- 30 # number of utterances in each window
# add wn blank lines at the end of each file, so we can break it into wn-utterance windows without spanning recordings
df.wn.count <- group_by(df.wn.count, file) %>%
  do({
    max.utt.num <- as.numeric(max(as.numeric(.$utt.num)))
    new.rows <- data.frame(utt=NA, 
                           file=.$file[1], 
                           utt.num=seq(from=(max.utt.num+1), to=(max.utt.num+wn)),
                           child=NA,
                           age_weeks=NA,
                           orth=NA)
    file.data <- rbind(., new.rows)
    n <- ceiling(nrow(file.data)/wn) # how many levels will be needed in the counter?
    file.data$wn.count <- gl(n=n, k=wn, labels=paste0(paste(sample(letters, 20, replace=T), collapse=""), 1:n))[1:nrow(file.data)] 
    # add a counter that goes 1 to wn, so we can split the resulting data frame using that counter
    return(file.data)
    })

df.wn.count <- na.omit(df.wn.count) # delete the extra empty rows made as a buffer between recordings

stopifnot(wn >= max(table(df.wn.count$wn.count))) # if this is greater than wn, error!

# collapse utterances from within each wn-utterance window
doc.data <- group_by(df.wn.count, wn.count) %>%
  dplyr::select(child, age_weeks, orth, wn.count) %>%
  do({
    doc.data <- data.frame(child=.$child[1],
                      age_weeks=.$age_weeks[1],
                      documents=paste(.$orth, collapse=" "))
  }) 
doc.data$child <- as.factor(doc.data$child)

# adding the phon column back in
df.wn.count <- df.wn.count %>%
    ungroup() %>%
    left_join(df, by=c("utt", "orth")) 
```

```{r topic_modeling_prep}
# prep
processed <- textProcessor(doc.data$documents, 
                           metadata = doc.data, 
                           removestopwords=TRUE, 
                           wordLengths=c(1, Inf))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta,
                     lower.thresh = 2) 
# removes infrequent terms depending on user-set parameter lower.thresh (the minimum number of documents a word needs to appear in order for the word to be kept within the vocabulary)
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
```


```{r stm_searchK, cache=TRUE}
source("stm/R/searchK.R") # use my own version of this command
source("stm/R/plot.searchK.R") # use my own version of this command

ntopics <- searchK(out$documents, out$vocab, 
                   K = c(8:20), # how many topics?
                   iter=10,
                   data = meta,
                   prevalence =~ child,
                   content =~ child) 
ntopics.results <- ntopics$results %>%
  gather(key="measure", value="value", -K) %>%
  filter(measure!="em.its" & measure!="iter" & measure!="bound") # drop measures from the plot
ggplot(ntopics.results, aes(y=value, x=K)) +
  geom_point(alpha=.5) +
  geom_smooth(n=20) +
  facet_wrap(~ measure, scales="free") +
  labs(y=NULL, x="Number of topics") +
  ggtitle(paste0("searchK() results\n", paste(format(ntopics$call), collapse="")))
ggsave(filename=paste0("plots/topicmodeling/searchK_results_iter_winutts", wn ,".png"), width=12, height=6, units="in")

nK <- 12
```


```{r lda_model}
# Need to reindex docs to start counting vocab at 0 instead of 1 for lda package
docs.lda <- docs
for(i in 1:length(docs.lda)){
  docs.lda[[i]][1,] <- as.integer(docs[[i]][1,]-1)
  if(dim(docs.lda[[i]])[1] != 2) message(paste("wrong number of rows in document", i))
  if(dim(docs.lda[[i]])[2] < 1) message(paste("wrong number of columns in document", i))
}

lda <- lda.collapsed.gibbs.sampler(docs.lda,
                            K=nK,
                            vocab=vocab,
                            num.iterations = 500, # 25 is the number used in the lda demo
                            0.1, # from demo
                            0.1, # from demo
                            burnin = 5,
                            compute.log.likelihood=TRUE)

plot(lda$log.likelihoods[1,], 
     ylab="log likelihood",
     xlab="iteration",
     main="full log likelihood (including the prior)")
plot(lda$log.likelihoods[2,], 
     ylab="log likelihood",
     xlab="iteration",
     main="log likelihood of the observations\n conditioned on the assignments")

top.topic.words(lda$topics, 10, by.score=TRUE) ## Get the top words for each topic
```

```{r lda_doc_loadings}
# from topic_modeling_functions.r
df.lda <- doc_loadings(method="lda", 
                             model=lda, 
                             meta=out$meta, 
                             df.wn.count)   

write.table(df.lda, file=paste0("contexts_LDA_winutts", wn, ".txt"), quote=F, col.names=T, row.names=F, append=F, sep="\t")

hist(unlist(lda.loadings[,5:16]), breaks=50)
```

```{r lda_threshold}
thresholds <- seq(0,.75,.05) # what thresholds to try

# from topic_modeling_functions.r
# any additional values to be saved in the plot name can be included as extra arguments
threshold_plots(df.lda, 
                thresholds=thresholds, 
                method="lda",
                save.to="plots/topicmodeling",
                wn=wn,
                remove.stops=TRUE) 
```


```{r lda_apply_threshold}
# apply threshold
lda.threshold <- .3 # set threshold for inclusion in a context
df.lda.bin <- df.lda %>% 
  gather(key="topic", value="loading", starts_with("topic_"))
df.lda.bin$include <- ifelse(df.lda.bin$loading > lda.threshold, 1, 0)
df.lda.bin <- df.lda.bin %>% 
  select(-loading) %>% 
  spread(key=topic, value=include)

write.table(df.lda.bin, file=paste0("contexts_LDA_bin_winutts", wn, ".txt"), quote=F, col.names=T, row.names=F, append=F, sep="\t")
```


```{r stm_model, cache=TRUE}
# Metadata covariates for topical prevalence allow the observed metadata to affect the frequency with which a topic is discussed. 
# Topical prevalence captures how much each topic contributes to a document. Because different documents come from different sources, it is natural then to want to allow this prevalence to vary with metadata that we have about document sources.
# Covariates in topical content allow the observed metadata to affect the word rate use within a given topicâ€“that is, how a particular topic is discussed.
# A topical content variable allows for the vocabulary used to talk about a particular topic to vary.
# As with all mixed-membership topic models, the posterior is intractable and non-convex, which creates a multimodal estimation problem that can be sensitive to initialization. Put differently, the answers the estimation procedure comes up with may depend on starting values of the parameters (e.g., the distribution over words for a particular topic). There are two approaches to dealing with this that the STM package facilitates. The first is to use a specific intitialization based on the method of moments, which is deterministic and globally consistent under reasonable conditions (Roberts et al. Forthcoming). This is known as a spectral initialization.10 In practice we have found this intialization to be very helpful. This can be chosen by setting init.type = "Spectral" in the stm function. We use this option in the above example. This means that no matter the seed that is set, the same results will be generated. However, it currently does not scale to extremely large vocabularies (uncommon in most applications) in which case alternative initializations are available
stm.null <- stm(out$documents, # the documents
           out$vocab, # the words
           K = nK, # number of topics
           max.em.its = 75, # set to run for a maximum of 75 EM iterations
           data = out$meta, # all the variables
           init.type = "Spectral")
stm.cont <- stm(out$documents, # the documents
           out$vocab, # the words
           K = nK, # number of topics
           content =~ child,
           max.em.its = 75, 
           data = out$meta, 
           init.type = "Spectral")
stm.prev <- stm(out$documents, # the documents
           out$vocab, # the words
           K = nK, # number of topics
           prevalence =~ child,
           max.em.its = 75, 
           data = out$meta, 
           init.type = "Spectral")
stm.both <- stm(out$documents, # the documents
           out$vocab, # the words
           K = nK, # number of topics
           prevalence =~ child, 
           content =~ child,
           max.em.its = 75, 
           data = out$meta, 
           init.type = "Spectral")

stms <- list(stm.null=stm.null, stm.cont=stm.cont, stm.prev=stm.prev, stm.both=stm.both)

sink("plots/topicmodeling/modelinfo.txt", append = T, type="output")
paste0("\n",date(), "\n")
for(f in 1:length(stms)){
  print((names(stms)[[f]]))
  print(paste("\n", wn, "utterances per document\n"))
  stm <- stms[[f]]
  
  pdf(paste0("plots/topicmodeling/topics_winutts", wn, "_",names(stms)[[f]], ".pdf"))
  plot.STM(stm, type = "summary", main = paste0("Topics\n", names(stms)[[f]])) 
  dev.off()
  
  print(labelTopics(stm))
  print(findThoughts(stm, texts=out$meta$documents, topics=1:nK, n=3))
  
  pdf(paste0("plots/topicmodeling/convergence_winutts", wn, "_",names(stms)[[f]], ".pdf"))
  plot(stm$convergence$bound, type = "l",
     ylab = "Approximate Objective",
     main = paste0("Convergence\n", names(stms)[[f]]))
  dev.off()
  
  pdf(paste0("plots/topicmodeling/topic_net_winutts", wn, "_",names(stms)[[f]], ".pdf"))
  # topicCorr(stm) # lots of output :)
  plot(topicCorr(stm), main = paste0("Topic correlations\n", names(stms)[[f]]))
  dev.off()
  
  pdf(paste0("plots/topicmodeling/topic_heatmap_winutts", wn, "_",names(stms)[[f]], ".pdf"))
  c <- topicCorr(stm)$cor # just the correlations between topics
  print(round(c, 2))
  diag(c) <- NA # remove the 1's on the diagonal so we can see the variation in the correlations
  heatmap(c, keep.dendro=F, symm=TRUE, main = paste0("Topic correlations: max", round(max(c), 2), "\n", names(stms)[[f]]))
  dev.off()
}
sink(NULL)

stm <- stms$stm.both
```

```{r stm_estimateEffect}
est <- estimateEffect(1:nK ~ child, stm, meta=out$meta)

prev.ests <- plot(est, "child")
# mean point estimates
prev.ests.means <- prev.ests$means
names(prev.ests.means) <- paste0("topic", prev.ests$topics)
prev.ests.means <- as.data.frame(prev.ests.means)
prev.ests.means$child <- levels(prev.ests$uvals)
prev.ests.means <- gather(prev.ests.means, key="topic", value="mean", -child)
# ci point estimates
prev.ests.cis <- prev.ests$cis
names(prev.ests.cis) <- paste0("topic", 1:nK)
prev.ests.cis.df <- data.frame(ci.2.5=NULL, ci.97.5=NULL, child=NULL, topic=NULL)
for(k in prev.ests$topics){
  c <- as.data.frame(prev.ests.cis[[k]])
  c <- as.data.frame(t(c))
  colnames(c) <- c("ci.2.5", "ci.97.5")
  c$child <- levels(prev.ests$uvals)
  c$topic <- names(prev.ests.cis[k])
  row.names(c) <- NULL
  prev.ests.cis.df <- rbind(prev.ests.cis.df, c)
}
prev.ests.df <- left_join(prev.ests.means, prev.ests.cis.df, by=c("child", "topic"))

ggplot(prev.ests.df, aes(x=child, y=mean, fill=child)) +
  geom_bar(position=position_dodge(), stat="identity") +
  geom_errorbar(aes(ymin=ci.2.5, ymax=ci.97.5),
                width=.2,                    # Width of the error bars
                position=position_dodge(.9)) +
  facet_wrap(~topic) + 
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y="topic prevalence", x=NULL, title="Topic prevalence by child\nwith 95CI error bars")
ggsave(filename=paste0("plots/topicmodeling/topic_prevalence_STM_winutts", wn ,".png"), width=12, height=12, units="in")
```

The topic loading for a document is the percentage of words attributable to a latent topic. [https://scholar.princeton.edu/sites/default/files/bstewart/files/ajpsappendix.pdf](https://scholar.princeton.edu/sites/default/files/bstewart/files/ajpsappendix.pdf)
```{r stm_doc_loadings}
# from topic_modeling_functions.r
df.stm <- doc_loadings(method="stm", 
                             model=stm, 
                             meta=out$meta, 
                             df.wn.count) 

write.table(df.stm, file=paste0("contexts_STM_winutts", wn, ".txt"), quote=F, col.names=T, row.names=F, append=F, sep="\t")

hist(unlist(stm.loadings[,5:16]), breaks=50)
```


```{r stm_threshold}
# from topic_modeling_functions.r
# any additional values to be saved in the plot name can be included as extra arguments
threshold_plots(df.stm, 
                   thresholds=thresholds, # what thresholds to try
                   method="stm",
                   save.to="plots/topicmodeling",
                   wn=wn,
                   remove.stops=TRUE) 
```


```{r stm_apply_threshold}
# apply threshold
stm.threshold <- .2 # set threshold for inclusion in a context
df.stm.bin <- df.stm %>% 
  gather(key="topic", value="loading", starts_with("topic_"))
df.stm.bin$include <- ifelse(df.stm.bin$loading > stm.threshold, 1, 0)
df.stm.bin <- df.stm.bin %>% 
  select(-loading) %>% 
  spread(key=topic, value=include)

write.table(df.stm.bin, file=paste0("contexts_STM_bin_winutts", wn, ".txt"), quote=F, col.names=T, row.names=F, append=F, sep="\t")
```

## agreement
```{r xtabs}
WL <- read.table("contexts_WL.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") %>% 
  mutate_each(funs(y=ifelse(.>0, 1, 0)), mealtime:routines) # for word list analysis only, make all contexts either 1 or 0 (smoothing over 1.5's from expand_windows)
# remove underscores from WL orth column
WL$orth <- gsub(x=WL$orth, pattern="_", replacement=" ")
WL.long <- gather(WL, key="WL", value="include", -utt, -orth, -phon) 

STM.con <- read.table(paste0("contexts_STM_winutts", wn, ".txt"), header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 
STM.con.long <- gather(STM.con, key="STM.con", value="include", -utt, -orth, -phon)

STM.bin <- read.table(paste0("contexts_STM_bin_winutts", wn, ".txt"), header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 
STM.bin.long <- gather(STM.bin, key="STM.bin", value="include", -utt, -orth, -phon)

HJ.con <- read.table("contexts_HJ_prop.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 
HJ.con.long <- gather(HJ.con, key="HJ.con", value="include", -utt, -orth, -phon)

HJ.bin <- read.table("contexts_HJ_bin.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 
HJ.bin.long <-  gather(HJ.bin, key="HJ.bin", value="include", -utt, -orth, -phon)


## xtabs
WL.long <- filter(WL.long, include==1) %>% 
  select(-include)

STM.bin.long <- filter(STM.bin.long, include==1) %>% 
  select(-include)

freqs <- full_join(WL.long, STM.bin.long, by="utt")
length(unique(WL$utt)); length(unique(STM.bin$utt)); length(unique(freqs$utt)) # how many utterances included

freqs <- freqs %>% 
  count(WL, STM.bin)

x_WL_STM.bin <- xtabs(n ~ WL + STM.bin, data=freqs)
summary(x_WL_STM.bin) # chi-squared test of independence
kable(x_WL_STM.bin)

library(vcd)
assocstats(x_WL_STM.bin) # cramers v
```

```{r logistic_regression}
nrow(WL); nrow(STM.con); nrow(STM.bin); nrow(HJ.con); nrow(HJ.bin)

# drop orth and phon in case there are superficial issues matching them with the join() below
WL <- select(WL, -orth, -phon)
STM.con <- select(STM.con, -orth, -phon)
STM.bin <- select(STM.bin, -orth, -phon)
HJ.con <- select(HJ.con, -orth, -phon)
HJ.bin <- select(HJ.bin, -orth, -phon)
colnames(WL)[-1] <- paste0("WL_", colnames(WL)[-1])
colnames(STM.con)[-1] <- paste0("STM.con_", colnames(STM.con)[-1])
colnames(STM.bin)[-1] <- paste0("STM.bin_", colnames(STM.bin)[-1])
colnames(HJ.con)[-1] <- paste0("HJ.con_", colnames(HJ.con)[-1])
colnames(HJ.bin)[-1] <- paste0("HJ.bin_", colnames(HJ.bin)[-1])

all.methods <- full_join(WL, STM.con, by="utt") %>% 
  full_join(STM.bin, by="utt") %>% 
  full_join(HJ.con, by="utt") %>% 
  full_join(HJ.bin, by="utt")
nrow(all.methods)  

# multivariate logistic regression
# WL_multivariate <- glm(as.matrix(select(all.methods, starts_with("WL_"))) ~ as.matrix(select(all.methods, starts_with("STM.con_"))), family = binomial(link="logit")) 
# Manova(WL_multivariate, type=3) # Performs a multivariate test of each predictor 

all.methods <- na.omit(all.methods) # listwise deletion (i.e. only analyze utterances we have for every method)
WL_contexts <- as.matrix(select(all.methods, starts_with("WL_")))
STM.con_contexts <- as.matrix(select(all.methods, starts_with("STM.con_")))
WL_multivariate <- MCMCglmm(WL_contexts ~ STM.con_contexts, data=all.methods, family="categorical")

WL_mealtime <- glm(all.methods$WL_mealtime ~ as.matrix(select(all.methods, starts_with("STM.con_"))), family = binomial(link="logit")) 

summary(mreg) # Provides coefficients as well as standard errors and signficance tests #


```


## run on aciss
```{r}
library(BatchJobs)

library(dplyr); library(tidyr); library(doParallel); library(devtools)

starts <- 1:20

batch_function <- function(starts){
  library(dplyr)
  library(tidyr)
  library(devtools)
  fun.version <- "2400f7eeb4395" # refers to the current commit for data_processing_functions.r
  source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/data_processing_functions.r", 
             sha1=fun.version)
  
  # note that df should have the context columns already (from lists, human coding, or topic modeling, etc.)
  # df <- get_from_https("https://raw.githubusercontent.com/rosemm/context_word_seg/master/contexts_HJ_voting.txt"); prop=FALSE; expand=FALSE
  # df <- get_from_https("https://raw.githubusercontent.com/rosemm/context_word_seg/master/contexts_HJ_prop.txt"); prop=TRUE; expand=FALSE
  df <- get_from_https("https://raw.githubusercontent.com/rosemm/context_word_seg/master/contexts_WL.txt") ; prop=FALSE; expand=TRUE
  if(nrow(df) == 0) stop("df didn't load")
  
  dict <- read.table("dict_all3_updated.txt", sep="\t", quote="", comment.char ="", header=1, stringsAsFactors=F)
  if(nrow(dict) == 0) stop("dict didn't load")
  
  iter <- 50 # the number of times to generate random samples
  
  library(doParallel)
  registerDoParallel()
  r <- foreach(1:iter, 
               .combine = rbind, 
               .packages=c("dplyr", "tidyr", "devtools") ) %dopar% par_function(dataframe=df,
                                                                                dict=dict,
                                                                                expand=expand,
                                                                                seg.utts=TRUE,
                                                                                TP=FALSE,
                                                                                MI=TRUE, 
                                                                                verbose=FALSE, 
                                                                                prop=prop,
                                                                                cutoff=.85,
                                                                                nontext=TRUE,
                                                                                fun.version=fun.version)
}
# create a registry
id <- "bootstrapWL2"
reg.wl <- makeRegistry(id = id)

# map function and data to jobs and submit
ids  <- batchMap(reg.wl, batch_function, starts)
done <- submitJobs(reg.wl, resources = list(nodes = 20, walltime=28800)) # expected to run for 8 hours (28800 seconds)


# create a registry
id <- "bootstrapHJpropContext2"
reg.prop.con2 <- makeRegistry(id = id)

# map function and data to jobs and submit
ids  <- batchMap(reg.prop.con2, batch_function, starts)
done <- submitJobs(reg.prop.con2, resources = list(nodes = 12, walltime=28800)) # expected to run for 8 hours (28800 seconds)

showStatus(reg.prop.con2)

# create a registry
id <- "bootstrapHJpropNontext2"
reg.prop.non2 <- makeRegistry(id = id)

# map function and data to jobs and submit
ids  <- batchMap(reg.prop.non2, batch_function, starts)
done <- submitJobs(reg.prop.non2, resources = list(nodes = 12, walltime=28800)) # expected to run for 8 hours (28800 seconds)

showStatus(reg.prop.non2)
```

## read in aciss results
```{r}
HJnontext.results1 <- process_batch_results(id="bootstrapHJpropNontext", dir="nontexts_humanjudgments")
HJnontext.results2 <- process_batch_results(id="bootstrapHJpropNontext2", dir="nontexts_humanjudgments")
HJnontext.results <- HJnontext.results2 
HJnontext.results$analysis <- "nontext"

HJcontext.results1 <- process_batch_results(id="bootstrapHJpropContext", dir="contexts_humanjudgments")
HJcontext.results2 <- process_batch_results(id="bootstrapHJpropContext1", dir="contexts_humanjudgments")
HJcontext.results <- rbind(HJcontext.results1, HJcontext.results2)
HJcontext.results$analysis <- "context"

HJresults <- rbind(HJcontext.results, HJnontext.results)
HJresults.summary <- HJresults %>%
  gather(key=measure, value=value, recall:precision) %>%
  group_by(analysis, nontext, measure) %>%
  summarize(mean=mean(value), sd=sd(value)) %>%
  rename(context=nontext)
HJresults.summary <- as.data.frame(HJresults.summary)

HJresults.plot <- HJresults %>%
  gather(key=measure, value=value, recall:precision) %>%
  rename(context=nontext)

# histograms
ggplot(HJresults.plot, aes(x=value)) +
  geom_histogram(data=filter(HJresults.plot, analysis=="nontext"), alpha=.5) +
  geom_histogram(data=filter(HJresults.plot, analysis=="context"), aes(x=value, fill=context), alpha=.5) +
  facet_wrap(~ measure + context, scales="free", ncol=12) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y=NULL)

# barplots
ggplot(HJresults.summary, aes(x=context, y=mean, fill=analysis)) +
  geom_bar(position=position_dodge(), stat="identity") +
  geom_errorbar(aes(ymin=mean - sd, ymax=mean + sd),
                width=.2,                    # Width of the error bars
                position=position_dodge(.9)) +
  facet_wrap(~ measure) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL) 
```

## global results
```{r}
global.data <- list(N.utterances=nrow(df), streams=NULL, phon.pairs=NULL, unique.phon.pairs=NULL) # storage variable

global.data$streams <- make_streams(df)
global.data$unique.phon.pairs <- calc_MI(phon.pairs=global.data$streams$phon.pairs, phon.stream=global.data$streams$phon.stream)


hist(global.data$unique.phon.pairs$MI, main="Global MI")
# hist(global.data$unique.phon.pairs$MI.1, main="Global MI 1 \nred line shows chance"); abline(v=1, lty=2, col="red")
# hist(global.data$unique.phon.pairs$MI.2, main="Global MI 2\nred line shows chance"); abline(v=1, lty=2, col="red")
hist(global.data$unique.phon.pairs$TP, main="Global TP")
```

## context results
```{r}
colnames(df)[which(is.na(colnames(df)) )] <- "none" # replace NA with "none"

context.data <- context_results(df=df, seg.utts=TRUE) # calls make_streams() and calc_MI()

# present results
par(mfrow=c(1,2))
for(k in 1:length(colnames(contexts))){
  hist(context.data[[k]]$unique.phon.pairs$MI, main=paste("Mutual Information,\n", colnames(contexts)[k], "context"), xlim=c(min(global.data$unique.phon.pairs$MI), max(global.data$unique.phon.pairs$MI)))
  
  hist(context.data[[k]]$unique.phon.pairs$TP, main=paste("Transitional Probability,\n", colnames(contexts)[k], "context"), xlim=c(0,1))
}


# how many utterances are in each context corpus?
N.utt <- vector("numeric", length=length(colnames(contexts)))
for(k in 1:length(colnames(contexts))){
  N.utt[k] <- context.data[[k]]$N.utterances
}
names(N.utt) <- colnames(contexts)
```

## segment speech
```{r}
global.data$TP85$seg.phon.stream <- segment_speech(cutoff=.85, 
                                                   stat="TP", 
                                                   data=global.data)

global.data$MI85$seg.phon.stream <- segment_speech(cutoff=.85, 
                                                   stat="MI", 
                                                   data=global.data)

for(k in 1:length(names(context.data))){
  
  message(paste("processing ", names(contexts)[k], "...", sep=""))
      
  context.data[[k]]$TP85$seg.phon.stream <- segment_speech(cutoff=.85, 
                                                           stat="TP", 
                                                           data=context.data[[k]])
  
  context.data[[k]]$MI85$seg.phon.stream <- segment_speech(cutoff=.85, 
                                                           stat="MI", 
                                                           data=context.data[[k]])
  
}
```

## assess segmentation
```{r}
global.data$TP85$seg.results <- assess_seg(seg.phon.stream=global.data$TP85$seg.phon.stream, words=global.data$streams$words, dict=dict)
colMeans(global.data$TP85$seg.results[,3:4], na.rm=T)

global.data$MI85$seg.results <- assess_seg(seg.phon.stream=global.data$MI85$seg.phon.stream, words=global.data$streams$words, dict=dict)
colMeans(global.data$MI85$seg.results[,3:4], na.rm=T)
saveRDS(global.data, file="results/global_results.rds")

for(k in 1:length(names(context.data))){
  
  message(paste("processing ", names(contexts)[k], "...", sep=""))
  message("TPs...")
      
  context.data[[k]]$TP85$seg.results <- assess_seg(seg.phon.stream=context.data[[k]]$TP85$seg.phon.stream, words=context.data[[k]]$streams$words, dict=dict)
  
  print(colMeans(context.data[[k]]$TP85$seg.results[,3:4], na.rm=T))
  
  message("MIs...")
  context.data[[k]]$MI85$seg.results <- assess_seg(seg.phon.stream=context.data[[k]]$MI85$seg.phon.stream, words=context.data[[k]]$streams$words, dict=dict)
  
  print(colMeans(context.data[[k]]$MI85$seg.results[,3:4], na.rm=T))
}
# saveRDS(context.data, file="results/context_results_WL.rds") # save the word list results
# saveRDS(context.data, file="results/context_results_HJ.rds") # save the human judgment context results
# context.data <- readRDS("results/context_results_WL.rds")
# context.data <- readRDS("results/context_results_HL.rds")
```

## combine results into plotable dataframe
```{r}
#####################################################
# use batchjobs_script.r to run nontext comparison distributions on ACISS (HPC)
#####################################################
nontext.results <- process_batch_results(id="bootstrapWL1", dir="nontexts_wordlists")


library(tidyr)
nontext.results <- nontext.results %>%
  mutate(cutoff=85) %>%
  unite(criterion, stat, cutoff, sep="", remove=F) %>%
  gather(measure, value, recall:precision) %>%
  rename(context=nontext)

context.results <- data.frame(context=NULL, criterion=NULL, measure=NULL, context.est=NULL)
for (k in 1:length(names(context.data)) ){
  
  criteria <- c("TP85", "MI85")
  
  for (c in 1:length(criteria)){
    context.est <- colMeans(context.data[[k]][[criteria[c]]]$seg.results[,c(3:4)], na.rm=T)
    estimates <- as.data.frame(context.est)
    estimates$measure <- row.names(estimates)
    estimates$context <- names(context.data)[k]
    estimates$criterion <- criteria[c]
    row.names(estimates) <- NULL
    
    context.results <- rbind(context.results, estimates) # add the results from this context and criterion to the rest
  }
}  

full.results <- left_join(nontext.results, context.results)

# add global estimates
global.results <- data.frame(measure=NA, criterion=NA, global.est=NA)
global.results[1,] <- c("recall",    "TP85", colMeans(global.data$TP85$seg.results[, 3:4], na.rm=T)[1])
global.results[2,] <- c("precision", "TP85", colMeans(global.data$TP85$seg.results[, 3:4], na.rm=T)[2])
global.results[3,] <- c("recall",    "MI85", colMeans(global.data$MI85$seg.results[, 3:4], na.rm=T)[1])
global.results[4,] <- c("precision", "MI85", colMeans(global.data$MI85$seg.results[, 3:4], na.rm=T)[2])
global.results$global.est <- as.numeric(global.results$global.est)

full.results <- left_join(full.results, global.results)
```

## THE PLOT
```{r}
full.results$context <- factor(full.results$context, levels=c("mealtime", "bedtime", "diaper.change", "bathtime", "body.touch", "play", "routines"))
ggplot(filter(full.results, criterion=="MI85"), aes(x=context, y=value))+
  # geom_boxplot() +
  geom_boxplot(color=NA, fill=NA, outlier.colour =NA) +
  facet_wrap(~measure) +
  geom_point(aes(x=context, y=context.est, color=context), size=4, show_guide=F) + 
  geom_hline(aes(yintercept=global.est), linetype = 2, size=1.5) + 
  theme(text = element_text(size=30), axis.ticks = element_blank(), axis.text.x = element_blank()) +
  labs(x=NULL, y=NULL)
ggsave(filename="plots/wordlists/global_context.png", width=12, height=5, units="in")

# with histograms instead of boxplots, to examine the nontext distributions (for shape, skew, etc.)
ggplot(filter(full.results, criterion=="MI85"), aes(x=value))+
  geom_histogram() +
  geom_vline(aes(color=context, xintercept=context.est), size=2) + 
  facet_wrap(~ measure + context, scales="free", ncol=7) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y=NULL)
```

## significance test
```{r}
# bootstrap p-values
full.results$above.est <- ifelse(full.results$value > full.results$context.est, 1, 0)
tests <- full.results %>%
  group_by(context, criterion, measure) %>%
  summarize(p.val = mean(above.est))
tests$stars <- ifelse(tests$p.val < .001, "***",
                      ifelse(tests$p.val < .01, "**",
                             ifelse(tests$p.val < .05, "*", 
                                    ifelse(tests$p.val < .1, "+", ""))))

kable(filter(tests, criterion=="MI85")[,-2], digits=3)
```

## sequence plots
```{r seqplot}
#########################################################################
# sequence plots
#########################################################################

# read in the contexts as they're used in the analysis (for both contexts and nontexts)
context_seq <- read.table("contexts_WL.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 
context_seq <- read.table("contexts_HJ_voting.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") # this file is generated in context_coding_cleaning.R
context_seq <- read.table("contexts_HJ_prop.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") # this file is generated in context_coding_cleaning.R
context_seq <- read.table("contexts_STM.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="")

master_doc_seq <- gather(context_seq, key=context, value=value, -orth, -phon, -utt) %>%
  separate(col=utt, into=c("file", "UttNum"), sep="_", remove=FALSE) %>%
  # for word list analysis only, make all contexts either 1 or 0 (smoothing over 1.5's from expand_windows)
  mutate(value=ifelse(value>0, 1, 0)) 

master_doc_seq$context <- as.factor(master_doc_seq$context)
master_doc_seq$utt <- as.factor(master_doc_seq$utt)
master_doc_seq$file <- as.factor(master_doc_seq$file)
master_doc_seq$UttNum <- as.numeric(master_doc_seq$UttNum)


# sequence plots for all of the files
library(ggplot2)
ggplot(filter(na.omit(master_doc_seq), context != "misc", context !="none"), aes(x=UttNum, y=context, color=context, alpha=value) ) +
  geom_point()+
  facet_wrap(~file, scales="free_x") +
  scale_alpha(guide = 'none')

# sequence plots for each file separately
context_codes <- "WL"
context_codes <- "HJ_voting"
context_codes <- "HJ_prop"
context_codes <- "STM"

files <- unique(master_doc_seq$file)
for (f in 1:length(files)){
  data <- filter(master_doc_seq, file==files[f], context != "misc", context !="none")
  # data <- filter(master_doc_seq, file==files[f])
  
  ggplot(data ) +
    geom_point(aes(x=UttNum, y=context, color=context, alpha=value, size=4, shape="|" ), na.rm = TRUE, show_guide=F) + 
    scale_shape_identity() +
    scale_alpha(guide = 'none') + 
    scale_size(guide = 'none')
  ggsave( paste0("plots/seqplot-",context_codes, files[f], ".png"), width=9, height=3, units="in" )
}

```

## descriptives
```{r}
descriptives <- results_descriptives(data=global.data, 
                                     context="global",
                                     criterion="MI85")
for(k in 1:length(colnames(contexts))){
  descriptives <- rbind(descriptives, results_descriptives(data=context.data[[k]], 
                                                           context=colnames(contexts[k]), 
                                                           criterion="MI85") )
}

# histograms of MI for each context
for(k in 0:length(names(contexts))){
  
  if(k==0) data <- global.data else data <- context.data[[k]]
  title <- ifelse(k==0, "global", names(contexts)[k])
  
  p <- ggplot(data$unique.phon.pairs, aes(x=MI)) +
    geom_histogram(fill="grey") +
    geom_vline(aes(xintercept=quantile(data$unique.phon.pairs$MI, .85)[[1]]), size=2, lty=2) + 
    theme_bw() +
    theme(text = element_text(size=20), axis.ticks = element_blank()) +
    ggtitle(title) + 
    labs(x="mutual information (MI)", y=NULL) +
    xlim(c(-5,15))
  
  ggsave(filename=paste0("plots/descriptives/MIhist_", title, ".png"),
         plot=p, 
         units="in", 
         width=4, 
         height=4)
}

# corpus descriptives
corpus_info_global <- corpus_decriptives(corpus=df[ , 1:3], 
                                         data=global.data, 
                                         contexts, 
                                         dict)


corpus.sum <- data.frame(Mean.length.utt=corpus_info_global$syls.per.utt$Mean.length.utt,
                         SD.length.utt=corpus_info_global$syls.per.utt$SD.length.utt,
                         Perc.monosyl.utts=corpus_info_global$syls.per.utt$Perc.syls.per.utt[[1]],
                         N.utts=corpus_info_global$syls.per.utt$N.utts,
                         N.words=length(global.data$streams$orth.stream),
                         context="global")
corpus.sum <- cbind(corpus.sum, corpus_info_global$syl.summary)

# for contexts where 1 (or greater) = yes and 0 = no (not prop=TRUE)
corpus_info_contexts <- list()
for(k in 1:length(names(contexts))){
  # only keep utterances that have > 0 for this context
  corpus <- df %>%
    gather(key=context, value=value, -utt, -orth, -phon) %>%
    filter(context == names(contexts)[k] & value > 0) %>%
    spread(key=context, value=value, fill=0) %>%
    select(utt, orth, phon)
  
  corpus_info_contexts[[ names(contexts)[k] ]] <- corpus_decriptives(corpus=corpus, 
                                                                     data=context.data[[k]], 
                                                                     contexts, 
                                                                     dict) 
  message(names(contexts)[k])
  # print(corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt)
  
  syl.freqs <- corpus_info_contexts[[ names(contexts)[k] ]]$word.freq %>%
    group_by(N.syl) %>%
    summarize(freq.mean=mean(freq), freq.sd=sd(freq), N.types=n(), freq.se = freq.sd/sqrt(N.types))
  
  this.corpus.sum <- data.frame(Mean.length.utt=corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt$Mean.length.utt,
                                SD.length.utt=corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt$SD.length.utt,
                                Perc.monosyl.utts=corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt$Perc.syls.per.utt[[1]],
                                N.utts=corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt$N.utts,
                                N.words=length(context.data[[k]]$streams$orth.stream),
                                context=names(contexts)[k])
  this.corpus.sum <- cbind(this.corpus.sum, corpus_info_contexts[[ names(contexts)[k] ]]$syl.summary)
  corpus.sum <- rbind(corpus.sum, this.corpus.sum)
}
corpus.sum$length.utt.se <- corpus.sum$SD.length.utt/sqrt(corpus.sum$N.utts)

ggplot(corpus.sum, aes(x=context, y=Mean.length.utt)) +
  geom_bar(fill=NA, stat="identity", show_guide=F) +
  geom_bar(data=filter(corpus.sum, context == "global"), fill="grey", stat="identity", show_guide=F) +
  geom_bar(data=filter(corpus.sum, context != "global"), aes(fill=context), stat="identity", show_guide=F) +
  geom_errorbar(aes(ymax = Mean.length.utt + length.utt.se, ymin=Mean.length.utt - length.utt.se), position=position_dodge(width=0.9), width=.25) +
  # geom_errorbar(aes(ymax = Mean.length.utt + SD.length.utt, ymin=Mean.length.utt - SD.length.utt), position=position_dodge(width=0.9), width=0, alpha=.5, linetype=2) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y="Syllables per utterance")
ggsave(filename ="plots/syls_per_utt.png", width=12, height=6, units="in")

# percent monosyllable utterances?
ggplot(corpus.sum, aes(x=context, y=Perc.monosyl.utts)) +
  geom_bar(fill=NA, stat="identity", show_guide=F) +
  geom_bar(data=filter(corpus.sum, context == "global"), fill="grey", stat="identity", show_guide=F) +
  geom_bar(data=filter(corpus.sum, context != "global"), aes(fill=context), stat="identity", show_guide=F) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y="Proportion monosyllabic utterances")
ggsave(filename ="plots/perc_monosyl_utts.png", width=12, height=6, units="in")

# freq of 1- 2- and 3-syl words?
freq.plot <- corpus.sum %>%
  gather(key=key, value=value, freq1syl.tot:freq4syl.se ) %>%
  separate(col=key, into=c("N.syl", "stat"), sep="[.]") %>%
  separate(col=N.syl, into=c("xx", "N.syl"), sep="q") %>%
  select(-xx) %>%
  extract(col=N.syl, into=c("N.syl"), regex="([[:digit:]]+)") %>%
  spread(key=stat, value=value) %>%
  gather(key=key, value=value, freq1st.word:freq3rd.freq) %>%
  extract(col=key, into=c("rank", "stat"), regex="([[:digit:]]+)[[:alpha:]]{2}[.]([[:alpha:]]+)") %>%
  spread(key=stat, value=value) %>%
  select(context, N.words, N.syl, tot, mean, sd, se) %>%
  mutate(prop=tot/N.words) %>%
  unique()
  
ggplot(freq.plot, aes(x=context, y=prop)) +
  geom_bar(fill=NA, stat="identity", show_guide=F) +
  geom_bar(data=filter(freq.plot, context == "global"), fill="grey", stat="identity", show_guide=F) +
  geom_bar(data=filter(freq.plot, context != "global"), aes(fill=context), stat="identity", show_guide=F) +
  facet_wrap(~ N.syl) + 
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y="Proportion of total words")

ggplot(freq.plot, aes(x=N.syl, y=prop)) +
  geom_bar(data=filter(freq.plot, context == "global"), fill="grey", stat="identity", show_guide=F) +
  geom_bar(data=filter(freq.plot, context != "global"), aes(fill=context), stat="identity", show_guide=F) +
  facet_wrap(~ context, ncol=4) + 
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x="Number of syllables", y="Proportion of total words")
ggsave(filename ="plots/syls_per_word.png", width=12, height=6, units="in")

# chi-square tests
global.props <- filter(freq.plot, context=="global")$prop + (1-sum(filter(freq.plot, context=="global")$prop))/4 # need to make sure it sums to 1
context.props <- list()
for(k in 1:length(names(contexts)) ){
  context.props[[names(contexts)[k]]] <- filter(freq.plot, context==names(contexts)[k])$prop + (1-sum(filter(freq.plot, context==names(contexts)[k])$prop))/4 # need to make sure it sums to 1
  message(names(contexts)[k])
  print(chisq.test( context.props[[names(contexts)[k]]] , p=global.props ))
}

##################################################
# NOTE: Swingley 2005 only considers a "word" segmented if it has BOTH high within-unit MI and also high frequency as a unit.

plot.data.con <- data.frame(context=names(context.data), TP85recall.mean=NA, TP85precision.mean=NA, MI85recall.mean=NA, MI85precision.mean=NA, TP85recall.sd=NA, TP85precision.sd=NA, MI85recall.sd=NA, MI85precision.sd=NA, N.utt=NA, group="context")
for(k in 1:length(names(contexts))){
  plot.data.con[k, 2:3] <- colMeans(context.data[[k]]$TP85$seg.results[,4:5], na.rm=T)
  plot.data.con[k, 4:5] <- colMeans(context.data[[k]]$MI85$seg.results[,4:5], na.rm=T)
  plot.data.con$N.utt[k] <- context.data[[k]]$N.utterances
}


plot.data.non <- bootstrap.summary
plot.data.non$N.utt <- plot.data.con$N.utt
plot.data.non$group <- "nontext"

plot.data <- rbind(plot.data.con, plot.data.non)
plot.data$group <- as.factor(plot.data$group)

plot.data <- plot.data %>%
  tidyr::gather(variable, value, TP85recall.mean:MI85precision.sd) %>%
  tidyr::extract(col=variable, into=c("criterion", "variable", "stat"), regex="([A-Z]{2}[0-9]{2})([a-z]+)[.]([a-z]+)") %>%
  tidyr::spread(stat, value)
plot.data$criterion <- as.factor(plot.data$criterion)
plot.data$variable <- as.factor(plot.data$variable)

#####################################################
# Is accuracy just driven by number of utterances?

ggplot(plot.data, aes(x=N.utt, y=mean, group=group)) +
  geom_point(aes(color=context, shape=group), size=4 )+
  geom_line(stat="smooth", method="lm", aes(linetype=group)) +
  facet_wrap(~variable + criterion)

# exclude routines
ggplot(subset(plot.data, context != "routines"), aes(x=N.utt, y=mean, group=group)) +
  geom_point(aes(color=context, shape=group), size=4 )+
  geom_line(stat="smooth", method="lm", aes(linetype=group)) +
  facet_wrap(~variable + criterion) +
  ggtitle("Excluding routines context and corresponding nontext")

summary(lm(mean ~ group*N.utt*criterion, data=plot.data))

#####################################################
# Do context vs. nontexts differ in accuracy?

ggplot(plot.data, aes(x=group, y=mean, fill=context)) +
  geom_bar(aes(), stat="identity", position="dodge")+
  geom_errorbar(aes(ymax = mean + sd, ymin=mean - sd), width=.3, position=position_dodge(.9)) +
  facet_wrap(~ variable + criterion)

ggplot(plot.data, aes(x=context, y=mean, fill=group)) +
  geom_bar(aes(), stat="identity", position="dodge")+
  geom_errorbar(aes(ymax = mean + sd, ymin=mean - sd), width=.3, position=position_dodge(.9)) +
  facet_wrap(~ variable + criterion) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="Error bars are +-SD", x=NULL, y=NULL)

ggplot(plot.data, aes(x=criterion:variable, y=mean, fill=group)) +
  geom_bar(aes(), stat="identity", position="dodge")+
  geom_errorbar(aes(ymax = mean + sd, ymin=mean - sd), width=.3, position=position_dodge(.9)) +
  facet_wrap(~ context, ncol=4) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="Error bars are +-SD", x=NULL, y=NULL)


#####################################################
# Segmentation sucess by frequency
plot_seg_results(global.data$TP85$seg.results, "global TP85")
plot_seg_results(global.data$MI85$seg.results, "global MI85")

for(k in 1:length(colnames(contexts))){
  TPplot <- plot_seg_results(context.data[[k]]$TP85$seg.results, title=paste(colnames(contexts)[k], "TP85"))
  MIplot <- plot_seg_results(context.data[[k]]$MI85$seg.results, title=paste(colnames(contexts)[k], "MI85"))
  #print(TPplot)
  print(MIplot)
}



#####################################################
# is it segmenting seed words correctly?
global.data$TP85$check_seed_words <- check_seed_words(global.data$TP85$seg.results)
summary(global.data$TP85$check_seed_words$seg.result)
plot_seg_results(seg.results=global.data$TP85$check_seed_words, title="Segmentation accuracy for seed words\nTP85", boxplot=TRUE, scatterplot=TRUE, by="contexts")
plot_seg_results(seg.results=global.data$TP85$check_seed_words, title="Segmentation accuracy for seed words\nTP85", boxplot=TRUE, scatterplot=TRUE, by="syl")

global.data$MI85$check_seed_words <- check_seed_words(global.data$MI85$seg.results)
summary(global.data$MI85$check_seed_words$seg.result)
plot_seg_results(seg.results=global.data$MI85$check_seed_words, title="Segmentation accuracy for seed words\nMI85", boxplot=TRUE, scatterplot=TRUE, by="contexts")
plot_seg_results(seg.results=global.data$MI85$check_seed_words, title="Segmentation accuracy for seed words\nMI85", boxplot=TRUE, scatterplot=TRUE, by="syl")

for(k in 1:length(colnames(contexts))){
  context.data[[k]]$TP85$check_seed_words <- check_seed_words(context.data[[k]]$TP85$seg.results)
  summary(context.data[[k]]$TP85$check_seed_words$seg.result)
  TPplot<- plot_seg_results(seg.results=context.data[[k]]$TP85$check_seed_words, title=paste("Segmentation accuracy for seed words\nTP85", colnames(contexts)[k]), boxplot=TRUE, scatterplot=TRUE, by="contexts")
  
  context.data[[k]]$MI85$check_seed_words <- check_seed_words(context.data[[k]]$MI85$seg.results)
  summary(context.data[[k]]$MI85$check_seed_words$seg.result)
  MIplot <- plot_seg_results(seg.results=context.data[[k]]$MI85$check_seed_words, title=paste("Segmentation accuracy for seed words\n", colnames(contexts)[k], "context"), boxplot=TRUE, scatterplot=TRUE, by="contexts")
  #print(TPplot)
  print(MIplot)
}


gmodels::CrossTable(global.data$MI85$seg.results[,6], global.data$MI85$seg.results$syl.bins, prop.r=T, prop.c=F, prop.chisq=F)
seg.by.syl <- table(global.data$MI85$seg.results[,c(6, 10)])
syl.by.seg <- table(global.data$MI85$seg.results[,c(10, 6)])
plot(table(global.data$MI85$seg.results[,c(6, 10)]))
barplot(seg.by.syl, beside=T, legend=rownames(seg.by.syl), xlab="syllables")
barplot(seg.by.syl, beside=F, legend=rownames(seg.by.syl), xlab="syllables")
barplot(syl.by.seg, beside=T, legend=rownames(syl.by.seg), xlab=NULL)
barplot(syl.by.seg, beside=F, legend=rownames(syl.by.seg), xlab=NULL)

combined.seg.results <- context.data[[1]]$MI85$seg.results
combined.seg.results$context.corpus <- colnames(contexts)[1]
for(k in 2:length(colnames(contexts))){
  seg.results <- context.data[[k]]$MI85$seg.results
  seg.results$context.corpus <- colnames(contexts)[k]
  combined.seg.results <- rbind(combined.seg.results,seg.results)
}

combined.seg.results$context.corpus <- as.factor(combined.seg.results$context.corpus)

bottoms <- grepl("bottom", df$orth)
bottom.corpus <- df[bottoms,]
bottom.corpus[,c(1,6,7)]

bellys <- grepl("belly", df$orth)
belly.corpus <- df[bellys,]
belly.corpus[, c(1, 3:9)]
belly.seg <- grepl("'bE-lI", combined.seg.results$phon, fixed=TRUE)
belly.seg <- combined.seg.results[belly.seg,]

big.kiss <- df[grepl("'bIg 'kIs", df$phon, fixed=T),]; nrow(big.kiss)
big <- df[grepl("'bIg", df$phon, fixed=T),]; nrow(big)
kiss <- df[grepl("'kIs", df$phon, fixed=T),]; nrow(kiss)

arrange(filter(context.data$body.touch$unique.phon.pairs, syl1=="'bIg"), -MI)
arrange(filter(global.data$unique.phon.pairs, syl1=="'bIg"), -MI)
arrange(filter(context.data$body.touch$unique.phon.pairs, syl2=="'f{t"), -MI)
arrange(filter(global.data$unique.phon.pairs, syl2=="'f{t"), -MI)
arrange(filter(context.data$body.touch$unique.phon.pairs, syl1=="'tV"), -MI)
arrange(filter(context.data$body.touch$unique.phon.pairs, syl2=="mI"), -MI)
arrange(filter(global.data$unique.phon.pairs, syl1=="'tV"), -MI)
arrange(filter(global.data$unique.phon.pairs, syl2=="mI"), -MI)
head(arrange(context.data$body.touch$unique.phon.pairs, -freq), 40)

length(which(global.data$streams$phon.pairs$syl1=="'tV"))
length(which(context.data$body.touch$streams$phon.pairs$syl1=="'tV"))

filter(context.data$body.touch$unique.phon.pairs, syl2=="'kIs")
filter(global.data$unique.phon.pairs, syl2=="'kIs")

combined.seg.results[grepl("'kIs", combined.seg.results$phon),] # kiss

combined.seg.results[grepl("'tI-kP", combined.seg.results$phon),c(1,2,7,13)] # tickle
combined.seg.results[grepl("kP", combined.seg.results$phon),c(1,2,7,9,13)] # 'ckle (end of "tickle", "chuckle", "typical")
global.data$MI85$seg.results[grepl("'tI-kP", global.data$MI85$seg.results$phon, fixed=T),] # tickle

combined.seg.results[grepl("'f{t", combined.seg.results$phon, fixed=T),] # fat
combined.seg.results[grepl("'bIg", combined.seg.results$phon, fixed=T),] # big
combined.seg.results[grepl("'bIg-'f{t", combined.seg.results$phon, fixed=T),] # big fat
global.data$MI85$seg.results[grepl("'f{t", global.data$MI85$seg.results$phon, fixed=T),] # fat
global.data$MI85$seg.results[grepl("'bIg", global.data$MI85$seg.results$phon, fixed=T),] # big
global.data$MI85$seg.results[grepl("'tV", global.data$MI85$seg.results$phon, fixed=T),] # tu- (mmy)
df[grepl("'f{t", df$phon, fixed=T),c(1,7,9)] # fat
df[grepl("'bIg 'f{t 'tV mI", df$phon, fixed=T),c(1,7,9)] # big fat tummy
df[grepl("'bIg 'f{t", df$phon, fixed=T),c(1,7,9)] # big fat
df[grepl("'bIg 'kIs", df$phon, fixed=T),c(1,7,9)]# big kiss
df[grepl("'bIg", df$phon, fixed=T),c(1,7,9)] # big
df[grepl("'kIs", df$phon, fixed=T),c(1,7,9)]# kiss
global.data$MI85$seg.results[grepl("^@$", global.data$MI85$seg.results$phon),] # a
combined.seg.results[grepl("^@$", combined.seg.results$phon),] # a

# global.data$MI85$seg.results[grepl("'fV", global.data$MI85$seg.results$phon, fixed=T),] # fun (-nny)
# combined.seg.results[grepl("'h{-pI", combined.seg.results$phon, fixed=T),] # happy
# combined.seg.results[grepl("'kr2-IN", combined.seg.results$phon, fixed=T),] # crying
# combined.seg.results[grepl("'bQ-tP", combined.seg.results$phon, fixed=T),] # bottle
# combined.seg.results[grepl("'sI-lI", combined.seg.results$phon, fixed=T),]# 'sI-lI
# combined.seg.results[grepl("'bE-lI", combined.seg.results$phon, fixed=T),] # belly
# combined.seg.results[grepl("'bju-t@-fUl", combined.seg.results$phon, fixed=T),] # beautiful
# combined.seg.results[grepl("'wIn-dI", combined.seg.results$phon, fixed=T),] # windy
# combined.seg.results[grepl("'mV-mI", combined.seg.results$phon, fixed=T),] # mummy
# combined.seg.results[grepl("'s5-pI", combined.seg.results$phon, fixed=T),] # splashing
combined.seg.results[grepl("'spl{-SIN", combined.seg.results$phon, fixed=T),] # splashing
combined.seg.results[grepl("'spl{", combined.seg.results$phon, fixed=T),] # spla-
global.data$MI85$seg.results[grepl("'spl{-SIN", global.data$MI85$seg.results$phon, fixed=T),] # splashing
global.data$MI85$seg.results[grepl("'spl{", global.data$MI85$seg.results$phon, fixed=T),] # spla-
global.data$MI85$seg.results[grepl("SIN", global.data$MI85$seg.results$phon, fixed=T),] # -shing
global.data$unique.phon.pairs[grepl("'spl{", global.data$unique.phon.pairs$syl1, fixed=T) ,] # spla-
global.data$unique.phon.pairs[grepl("SIN", global.data$unique.phon.pairs$syl2, fixed=T) ,] # -shing

head(arrange(filter(global.data$MI85$seg.results, seg.result=="miss"), -freq), 100)
head(arrange(filter(global.data$MI85$seg.results, seg.result=="miss" & freq>2), freq), 100)

high.freq.fails <- arrange(filter(combined.seg.results, seg.result!="hit"), -freq) # high freq fails
df[grepl("'mV mI", df$phon, fixed=T),1:2] # mummy
df[grepl("'lI", df$phon, fixed=T),1:2] # li' (e.g., li-cking, little, li-sten)
df[grepl("'kV ", df$phon, fixed=T),1:2] # kuh (cu-ddle, co-ming, cou-ple, )
df[grepl("'I ", df$phon, fixed=T),1:2] # i (i-sn't, i-t'll, )
df[grepl("'d7 ", df$phon, fixed=T),1:2] # dear
filter(global.data$unique.phon.pairs, syl1=="'lI") # global data
filter(context.data$mealtime$unique.phon.pairs, syl1=="'lI") # global data

unique(global.data$streams$orth.stream[-which(global.data$streams$orth.stream %in% dict$word)])

library(igraph)
df.big <- df[grepl("'bIg", df$phon, fixed=T),]
streams.big <- make_streams(df.big)
plot(graph.edgelist(as.matrix(streams.big$phon.pairs), directed=F))

df.big.sample <- df.big[sample(1:nrow(df.big), 15),]
streams.big.sample <- make_streams(df.big.sample)
plot(graph.edgelist(as.matrix(streams.big.sample$phon.pairs), directed=F), main="overall\nsample with big")

df.big.body.touch <- df[grepl("'bIg", df$phon, fixed=T) & df$body.touch>0,]
df.big.body.touch.sample <- df.big.body.touch[sample(1:nrow(df.big.body.touch), 15),]
streams.big.body.touch.sample <- make_streams(df.big.body.touch.sample)
plot(graph.edgelist(as.matrix(streams.big.body.touch.sample$phon.pairs), directed=F), main="body.touch\nsample with big")

df.body.touch <- df[df$body.touch>0,]
df.body.touch.sample <- df.body.touch[sample(1:nrow(df.body.touch), 22),]
streams.df.body.touch.sample <- make_streams(df.body.touch.sample)
plot(graph.edgelist(as.matrix(streams.df.body.touch.sample$phon.pairs), directed=F), main="body.touch\nrandom sample")

df.sample <- df[sample(1:nrow(df), 22),]
streams.sample <- make_streams(df.sample)
plot(graph.edgelist(as.matrix(streams.sample$phon.pairs), directed=F), main="overall\nrandom sample")

body.touch.edge.list <- context.data$body.touch$streams$phon.pairs[grepl("'bIg", context.data$body.touch$streams$phon.pairs[,1], fixed=T),]
plot(graph.edgelist(as.matrix(body.touch.edge.list), directed=F))

overall.edge.list <- global.data$streams$phon.pairs[grepl("'bIg", global.data$streams$phon.pairs[,1], fixed=T),]
plot(graph.edgelist(as.matrix(overall.edge.list)))

routines.edge.list <- context.data$routines$streams$phon.pairs[grepl("'bIg", context.data$routines$streams$phon.pairs[,1], fixed=T),]
plot(graph.edgelist(as.matrix(routines.edge.list)))

network_plot(global.data, "global.data")
network_plot(context.data$body.touch, "body.touch")

library(GGally)
global.network <- network::network(as.matrix(global.data$streams$phon.pairs))
ggnet(global.network,  title="overall\nwhole corpus", weight.method="degree")

body.touch.network <- network::network(as.matrix(context.data$body.touch$streams$phon.pairs))
ggnet(body.touch.network, main="body.touch\nwhole corpus", weight.method="degree")


el <- graph.data.frame(global.data$unique.phon.pairs)
plot(el,layout=layout.fruchterman.reingold, edge.width=E(el)$width/2)
# line node size should be syllable freq, weights of edges should be 

ggplot(combined.seg.results, aes(x=seg.result, y=freq.segd)) +
  facet_wrap(~context.corpus, ncol=7) +
  geom_boxplot() +
  scale_y_log10() + labs(y="Log10(frequency)", x=NULL, title=NULL) + 
  scale_x_discrete(limits=c("false alarm", "hit")) +
  coord_flip() +
  theme(text = element_text(size=30))
ggplot(combined.seg.results, aes(x=seg.result, y=freq.orth)) +
  facet_wrap(~context.corpus, ncol=7) +
  geom_boxplot() +
  scale_y_log10() + labs(y="Log10(frequency)", x=NULL, title=NULL) + 
  scale_x_discrete(limits=c("miss", "hit")) +
  coord_flip() +
  theme(text = element_text(size=30))

combined.check_seed_words <- context.data[[1]]$MI85$check_seed_words
combined.check_seed_words$context.corpus <- colnames(contexts)[1]
for(k in 2:length(colnames(contexts))){
  check_seed_words <- context.data[[k]]$MI85$check_seed_words
  check_seed_words$context.corpus <- colnames(contexts)[k]
  combined.check_seed_words <- rbind(combined.check_seed_words,check_seed_words)
}

combined.check_seed_words$context.corpus <- as.factor(combined.check_seed_words$context.corpus)

ggplot(combined.check_seed_words, aes(x=seg.result, y=freq)) +
  facet_wrap(~context.corpus, ncol=4) +
  geom_boxplot() +
  geom_point(aes(color=context), alpha=.7, size=4, position = position_jitter(w = .3, h = 0)) +
  scale_y_log10() + labs(y="Log10(frequency)", x=NULL, title=title) + 
  coord_flip()

library(knitr)
N.utt
tests <- vector("list", length(names(contexts))) # storage variable
names(tests) <- colnames(contexts)

for(k in 1:length(names(contexts))){
  message(paste("processing ", names(contexts)[k], "...", sep=""))
  
  tests[[k]]$context.pairs.N <- nrow(context.data[[k]]$unique.phon.pairs)
  tests[[k]]$nontext.pairs.N <- nrow(nontext.data[[k]]$unique.phon.pairs)
  
  tests[[k]]$test.MI <- ks.test(context.data[[k]]$unique.phon.pairs$MI, nontext.data[[k]]$unique.phon.pairs$MI)
  tests[[k]]$test.TP <-ks.test(context.data[[k]]$unique.phon.pairs$TP, nontext.data[[k]]$unique.phon.pairs$TP)
}

tests

# # reformat for printing as table
tests.table <- data.frame(N.utt=N.utt , TP.KSstat = NA, TP.KSpval=NA, MI.KSstat=NA, MI.KSpval=NA)
  for(k in 1:length(names(context.data))){
    tests.table[k,2:5] <- round(c(tests[[k]]$test.TP$statistic, tests[[k]]$test.TP$p.value, tests[[k]]$test.MI$statistic, tests[[k]]$test.MI$p.value), 3)
  }
tests.table$TP.KSpval <- ifelse(tests.table$TP.KSpval==0, ">.001", tests.table$TP.KSpval)
tests.table$MI.KSpval <- ifelse(tests.table$MI.KSpval==0, ">.001", tests.table$MI.KSpval)
knitr::kable(tests.table)
x


# reformat data for plotting
plot.data.wide <- data.frame(context=NULL, sample=NULL, MI=NULL, TP=NULL)
for(k in 1:length(colnames(contexts))){
  con <- data.frame(context=colnames(contexts)[k], sample="context", MI=context.data[[k]]$unique.phon.pairs$MI, TP=context.data[[k]]$unique.phon.pairs$TP)
  non <- data.frame(context=colnames(contexts)[k], sample="nontext", MI=nontext.data[[k]]$unique.phon.pairs$MI, TP=nontext.data[[k]]$unique.phon.pairs$TP)
  
 plot.data.wide <- rbind(plot.data.wide, con, non)
 
}
plot.data <- melt(plot.data.wide)

ggplot(subset(plot.data, variable=="MI"), aes(x=value, fill=sample)) + 
  geom_histogram(aes(y = ..density..), position="identity", alpha=.5) +
  facet_wrap( ~ context , scales="free") +
  theme_bw() +
  labs(title="Mutual Information")

ggplot(subset(plot.data, variable=="TP"), aes(x=value, fill=sample)) + 
  geom_histogram(aes(y = ..density..), position="identity", alpha=.5) +
  facet_wrap( ~ context , scales="free") +
  theme_bw() +
  labs(title="Transitional Probabilities")

# dist of syllables
par(mfrow=c(1,1))
for(k in 1:length(colnames(contexts))){
  syl.freq <- table(context.data[[k]]$streams$phon.stream)
  hist(syl.freq, main=paste(colnames(contexts)[k], "syllable freq"))
}

for(k in 1:length(names(nontext.data))){
  syl.freq <- table(nontext.data[[k]]$streams$phon.stream)
  hist(syl.freq, main=paste(names(nontext.data)[k], " syllable freq (", colnames(contexts)[k], ")", sep=""))
}



freq.bigrams <- summarise(group_by(global.data$streams$phon.pairs, syl1, syl2), count=n()) # frequency of bigrams
freq.words <- table(global.data$streams$orth.stream) # frequency of words
freq.syl <- table(global.data$streams$phon.stream) # frequency of syllables

# freq plots
barplot(sort(freq.words, decreasing=TRUE)) 
barplot(sort(freq.words, decreasing=TRUE)[1:100]) # top 100 words only 
barplot(sort(freq.syl, decreasing=TRUE))
barplot(sort(freq.syl, decreasing=TRUE)[1:100]) # top 100 syllables only 

freq.words.df <- as.data.frame(freq.words)

# merge frequency with dictionary information (number of syllables, etc.)
colnames(freq.words.df)[1] <- "word"
freq.words.df <- merge(x=freq.words.df, y=dict, all.x=TRUE)

# present results

plot(Freq ~ jitter(N.syl), data = freq.words.df, main="Frequency of words \nby number of syllables")
plot(Freq ~ jitter(N.syl), data = filter(freq.words.df, Freq<2000), main="Frequency of words \nby number of syllables \n(outlier removed)") # remove outlier

ggplot(freq.words.df, aes(y=Freq, x=jitter(N.syl))) +
  geom_point(aes(alpha=.5)) +
  facet_wrap(~context)

freq.summary <- summarise(group_by(freq.words.df, Syllables=as.factor(N.syl)), Mean.Freq=mean(Freq), sd=sd(Freq), n=n(), se=sd/sqrt(n))
ggplot(freq.summary, aes(x=Syllables, y=Mean.Freq))+
  geom_bar(stat="identity", position=position_dodge(.9)) +
  geom_errorbar(aes(ymax = Mean.Freq + se, ymin=Mean.Freq - se), width=.3, position=position_dodge(.9)) +
  labs(title="Frequency by number of syllables\n(Error bars +-SE)")


# contexts and ambiguity?
context.syl.tokens <- list(NULL)
# number of syllables per utterance in contexts vs. nontexts?
for(k in 1:length(names(context.data))){
  context.syl.tokens[k] <- length(context.data[[k]]$streams$phon.stream)
}
nontext.syl.tokens <- list(NULL)
# number of syllables per utterance in contexts vs. nontexts?
for(k in 1:length(names(nontext.data))){
  nontext.syl.tokens[k] <- length(nontext.data[[k]]$streams$phon.stream)
}

names(context.syl.tokens) <- names(context.data)
names(nontext.syl.tokens) <- names(context.data)

N.utt

df$utterance <- row.names(df)
df$y <- 1

par(mfrow=c(1,1))

fifth <- floor(nrow(df)/5)
plot.data <- df[1:100, ]
ggplot(plot.data, aes(x=utterance, y=y, color=context)) + 
  theme_bw() +
  geom_point()


filter(context.data$mealtime$unique.phon.pairs, syl1=="'mIlk") 
filter(nontext.data[[1]]$unique.phon.pairs, syl1=="'mIlk") 
