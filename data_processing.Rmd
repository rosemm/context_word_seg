```{r set-up}
rm(list = ls()) # clear the environment

list.of.packages <- c("dplyr", 
                      "tidyr", 
                      "ggplot2", 
                      "reshape2", 
                      "stringr",
                      "knitr", 
                      "stm", 
                      "lda", 
                      "tm", 
                      "SnowballC", 
                      "LDAvis",
                      "vcd", 
                      "car")
require(devtools)
source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/R/utils.R")

check_packages(list.of.packages) # checks to make sure packages are installed (and installs them if they aren't), then calls library() on each

setwd("/Users/TARDIS/Documents/STUDIES/context_word_seg")
```


## translate orth to phon (this also reads in the dict file):
```{r read_df}
# to translate CHILDES transcripts to phon approximations using Swingley's dictionary:
# source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/data_processing_orth_to_phon.R")

# after the first time, we can just read in the saved results from the above code
df <- read.table("utt_orth_phon_KEY.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") # this gets used for word-lists contexts, it will get over-written for other contexts

df$orth <- tolower(df$orth) # make sure the orth stream is all lower case
df$phon <- gsub(x=df$phon, pattern="-", replacement=" ", fixed=T) # make sure all word-internal syllable boundaries "-" are represnted just the same as between-word syllable boundaries (space)
```

```{r read_dict}
dict <- read.table("dict_all3_updated.txt", header=1, sep="\t", quote="", comment.char ="", stringsAsFactors = FALSE)

# add frequency for each word to dictionary
orth_words <- strsplit(paste(df$orth, collapse=" "), split=" ", fixed=T)[[1]] # the vector of words (df$orth, collapsed into one paste, and then split into a unit for each word)

# drop empty elements
freq_orth <- data.frame(word=orth_words[orth_words != ""], stringsAsFactors = FALSE) %>% 
  count(word) # get count of each word

dict <- left_join(dict, freq_orth, by="word") %>% 
  rename(freq.orth = n)
```

  
## read in context word list
```{r}
contexts <- read.csv("/Users/TARDIS/Documents/STUDIES/context_word_seg/words by contexts.csv")

# mark seed word bigrams in df$orth
bigrams <- grep(pattern="_", x=unlist(as.list(contexts)), fixed=T, value=T) # all of the context seed words that are bigrams (have an _)
for(b in 1:length(bigrams)){
  search_for <- paste0(gsub(x=bigrams[[b]], pattern="_", replacement=" ")) # look for the bigram, but with a space instead of the _
  replace_with <- bigrams[[b]] # replace it with the bigram
  df$orth <- gsub(pattern=search_for, x=df$orth, replacement=replace_with)
}
rm(b, search_for, replace_with) # cleaning up the workspace
```


## add word context to dictionary dataframe and plot
```{r}
# add context for each word to dictionary
c <- gather(contexts, key="context", value="word") %>%
  filter(word!="")
c <- left_join(c, dict, by="word") 

c.missed <- filter(c, grepl(x=word, pattern="_", fixed=T)) # grab the bigrams
search_in <- df$orth
for(i in 1:nrow(c.missed)){
  search_for <- c.missed[i, ]$word
  count <-  length(grep(x=search_in, pattern=search_for)) 
  c.missed[i, ]$freq.orth <- ifelse(count==0, NA, count)
}
rm(i, search_for, search_in) # cleaning up the workspace
c <- rbind(c, c.missed)
c <- filter(c, !is.na(freq.orth) & freq.orth > 0) %>% 
  select(context, word, freq=freq.orth) %>%
  arrange(desc(freq))
c$word.num <- 1:nrow(c) # adding a number identifier for each word (in desc freq order), to control order of plotting

con.freq <- c   %>%
  group_by(context) %>%
  summarize(mean=mean(freq), max=max(freq), min=min(freq), median=median(freq))

ggplot(c, aes(x=as.factor(word.num), y=freq, fill=context)) + 
  geom_bar(position=position_dodge(), stat="identity", show.legend=F) +
  facet_wrap(~context, scales="free", ncol=4) +
  theme(text = element_text(size=20), axis.ticks = element_blank(), axis.text.x = element_blank() ) +
  labs(x=NULL, y=NULL)
ggsave(filename="plots/wordlists/key_word_freq_by_context.png", width=12, height=5, units="in")
```

## only keep context words that actually occur in the corpus
(this is only relevant for presenting the list, e.g. in a talk - the code works fine with extra words in there)
```{r contexts_occuring, eval=FALSE}
contexts.list <- as.list(contexts)
for(i in 1:length(names(contexts.list))){
  # words.in.utts <- sapply(df$orth, strsplit, split=" ")
  this.context <- contexts.list[[i]][contexts.list[[i]] %in% global.data$streams$orth.stream] # the context words that show up in the corpus
  this.context <- c(as.character(this.context), rep("", nrow(contexts)-length(this.context))) # add empty values at the end to make all of the columns the same length
  contexts.list[[i]] <- this.context
}
contexts.occuring <- as.data.frame(contexts.list)
contexts.occuring <- contexts.occuring[-which(apply(contexts.occuring,1,function(x)all(x==""))),] # remove empty rows at the end
kable(contexts.occuring)
write.table(contexts.occuring, file="contexts.occuring.csv", sep=",", row.names=F)
############################
```

## tag contexts in df by word lists
```{r WL_contexts}
source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/R/data_processing_functions.r")
df <- df[ , 1:3] # only keep the first three columns (there should only be three columns anyway, but just to be sure)

temp <- unique(df$orth) # to speed up processing, only code each unique utterance once (then we'll join it back to the full df)
temp.codes <- data.frame(orth=temp)
for(k in 1:length(names(contexts))){
  
  temp.codes[[names(contexts)[k]]] <- 0 # make a new column for this context
  words <- as.character(unique(contexts[,k])) # this word list
  words <- words[words !=""] # drop empty character element in the list
  
  for(w in 1:length(words)){
    # for every orth entry that contains this word, put a 1 in this context's column
    temp.codes[[names(contexts)[k]]][grep(pattern=paste0("\\<", words[w], "\\>"), x=temp.codes$orth )] <- 1 
  }
}
df_WL <- left_join(df, temp.codes, by="orth") # join temp.codes back to full df

df_WL <- expand_windows(df_WL, context.names=names(contexts)) # extend context codes 2 utterances before and after each hit

write.table(df_WL, file="contexts_files/contexts_WL.txt", quote=F, col.names=T, row.names=F, append=F, sep="\t")
# df <- read.table("contexts_files/contexts_WL.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") # this overwrites the word list context df above
```

## tag contexts by human coder judgments
```{r HJ_data_wrangling}
source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/R/coding_functions.R")
source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/R/analysis_functions.R")
# set working directory to transcript coding folder on server:
# setwd("/Volumes/cas-fs2/baldwinlab/Maier/Transcript Coding")
# setwd("/Volumes/cas-fs2/Learninglab/6_ContextCoding_Maier")
doc_doctor()

master_doc <- collect_codes()

setwd("/Users/TARDIS/Documents/STUDIES/context_word_seg")
write.table(master_doc, file="master_doc.txt", quote=F, col.names=T, row.names=F, append=F, sep="\t")
# master_doc <- read.table("master_doc.txt", header=1, sep="\t", stringsAsFactors=F)

# # to update coding doc leaving only utterances that are below criterion on number of codes, to fill in the thin places
# update_doc <- UpdateDoc(master_doc, criterion = 5, unique.coders=TRUE)
# write.table(update_doc, file="coding_doc.txt", quote=F, col.names=T, row.names=F, append=F, sep="\t")

contexts_clean <- master_doc %>% 
  process_codes(min.codes=5, max.codes=5, unique.coders=TRUE) %>% 
  clean_contexts(key_file="context_cleaning_keys.txt")
context_cats <- contexts_clean %>% 
  clean_categories(key_file="categories_cleaning_keys.txt") 
# note that running clean_categories also updates the .md file, which pushes to here https://github.com/rosemm/context_word_seg/blob/master/categories_keys.md


master_doc_count <- context_cats %>%
  select(utt, category) %>%
  count(utt, category ) %>%
  spread(key=category, value=n, fill = 0) %>%
  separate(col=utt, into=c("file", "UttNum"), sep="_" , remove=F) %>%
  arrange(file, as.numeric(UttNum) ) 

df.hj.raw <- left_join(df[,1:3], select(master_doc_count, -file, -UttNum), by="utt" ) 
write.table(df.hj.raw, file="contexts_files/contexts_HJ_raw.txt", quote=F, col.names=T, row.names=F, append=F, sep="\t")

master_doc_prop <- master_doc_count %>% 
  ungroup() %>%
  dplyr::select(-utt, -file, -UttNum) %>%
  mutate(total=rowSums(., na.rm=FALSE)) %>% 
  mutate_each(funs(./total)) 
master_doc_prop$utt <- master_doc_count$utt  # to add back in the non-numeric utt column
master_doc_prop$total <- NULL  # delete column
  
df.hj.con <- left_join(df[,1:3], master_doc_prop, by="utt" ) 
write.table(df.hj.con, file="contexts_files/contexts_HJ.txt", quote=F, col.names=T, row.names=F, append=F, sep="\t")

votes <- master_doc_count[, 4:ncol( master_doc_count )]

##########################################
# number of utterances per context
##########################################
counts <- sort(colSums(master_doc_count[,4:ncol(master_doc_count)]),decreasing = TRUE)
count.data <- data.frame(n.utts=counts, 
                        context=factor(names(counts), levels=names(counts)))
# over the median number of utterances?
count.data$over.med <- ifelse(count.data$n.utts > median(counts), "yes", "no") 

ggplot(count.data, aes(x=context, y=n.utts, fill=over.med)) +
  geom_bar(stat = "identity", show.legend = F) +
  geom_text(aes(label=n.utts), position=position_dodge(width=0.9), vjust=-0.25)+
  scale_fill_manual(values = c("yes" = "darkgreen","no" = "grey")) +
  labs(x=NULL, y="Number of utterances tagged") +
  theme(axis.text.x = element_text(angle=330, vjust=1, hjust=0)) 
ggsave("plots/HJ/uttsper_context.png", width=16, height=5, units="in")
ggplot(count.data, aes(x=context, y=n.utts, fill=over.med)) +
  geom_bar(stat = "identity", show.legend = F) +
  geom_text(aes(label=n.utts), position=position_dodge(width=0.9), vjust=-0.25)+
  scale_fill_manual(values = c("yes" = "darkgreen","no" = "grey")) +
  labs(x=NULL, y="log(Number of utterances tagged)") +
  scale_y_log10() +
  theme(axis.text.x = element_text(angle=330, vjust=1, hjust=0)) 
ggsave("plots/HJ/uttsper_context_logtrans.png", width=16, height=5, units="in")

```

```{r HJ_threshold}
df.hj.con <- read.table("contexts_files/contexts_HJ.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 

thresholds <- seq(0,.75,.05) # what thresholds to try

# any additional values to be saved in the plot name can be included as extra arguments
threshold_plots(df.hj.con, 
                thresholds=thresholds, 
                method="HJ",
                save.to="plots/HJ",
                misc.cutoff="none") 
```

```{r HJ_apply_threshold}
# apply threshold
hj.threshold <- .25 # set threshold for inclusion in a context

df.hj.bin <- apply_threshold(df.hj.con, hj.threshold, 
                             plot=T, method="HJ", save.to="plots/HJ")

write.table(df.hj.bin, file=paste0("contexts_files/contexts_HJ_bin.txt"), quote=F, col.names=T, row.names=F, append=F, sep="\t")

# How many utterances per context sub-corpus?
corpora.sizes <- colSums(select(df.hj.bin, -utt, -orth, -phon), na.rm=TRUE)
size.data <- data.frame(n.utts=corpora.sizes, context=names(corpora.sizes)) 

ggplot(size.data, aes(x=reorder(context, desc(n.utts)), y=n.utts)) +
  geom_bar(stat = "identity", show.legend = F) +
  geom_text(aes(label=n.utts), position=position_dodge(width=0.9), vjust=-0.25) +
  labs(x=NULL, y="Number of utterances in sub-corpus") +
  theme(axis.text.x = element_text(angle=30, vjust=1, hjust=1, size=20))
ggsave("plots/HJ/uttsper_context_corpora.png", width=16, height=5, units="in")

```


## tag contexts by topic modeling
![](stm_comparison.png)

[http://cpsievert.github.io/LDAvis/reviews/reviews.html](http://cpsievert.github.io/LDAvis/reviews/reviews.html)

```{r topic_modeling_data_wrangling}
library(stm)
library(lda)

df <- df[ , 1:3] # only keep the first three columns (there should only be three columns anyway, but just to be sure)

df.wn.count <- df %>%
  select(-phon) %>%
  extract(col=utt, into=c("child", "age_weeks"), regex="^([[:alpha:]]{2})([[:digit:]]{2})", remove=FALSE) %>%
  extract(col=utt, into=c("file", "utt.num"), regex="^([[:alpha:]]{2}[[:digit:]]{2})[.]cha_([[:digit:]]+)", remove=FALSE) %>%
  filter(child != "hi") # remove this child, since the transcripts are so short

wn <- 30 # number of utterances in each window
# add wn blank lines at the end of each file, so we can break it into wn-utterance windows without spanning recordings
df.wn.count <- group_by(df.wn.count, file) %>%
  do({
    max.utt.num <- as.numeric(max(as.numeric(.$utt.num)))
    new.rows <- data.frame(utt=NA, 
                           file=.$file[1], 
                           utt.num=seq(from=(max.utt.num+1), to=(max.utt.num+wn)),
                           child=NA,
                           age_weeks=NA,
                           orth=NA,
                           stringsAsFactors=FALSE)
    file.data <- rbind(., new.rows)
    n <- ceiling(nrow(file.data)/wn) # how many levels will be needed in the counter?
    file.data$wn.count <- gl(n=n, k=wn, labels=paste0(paste(sample(letters, 20, replace=T), collapse=""), 1:n))[1:nrow(file.data)] 
    # add a counter that goes 1 to wn, so we can split the resulting data frame using that counter
    return(file.data)
    })

df.wn.count <- na.omit(df.wn.count) # delete the extra empty rows made as a buffer between recordings

stopifnot(wn >= max(table(df.wn.count$wn.count))) # if this is greater than wn, error!

# collapse utterances from within each wn-utterance window
doc.data <- group_by(df.wn.count, wn.count) %>%
  dplyr::select(child, age_weeks, orth, wn.count) %>%
  do({
    doc.data <- data.frame(child=.$child[1],
                      age_weeks=.$age_weeks[1],
                      documents=paste(.$orth, collapse=" "),
                      stringsAsFactors=FALSE)
  }) 
doc.data$child <- as.factor(doc.data$child)

# adding the phon column back in
df.wn.count <- df.wn.count %>%
    ungroup() %>%
    left_join(df, by=c("utt", "orth")) 
```

```{r topic_modeling_prep}
# prep
processed <- textProcessor(doc.data$documents, 
                           metadata = doc.data, 
                           removestopwords=TRUE, 
                           wordLengths=c(1, Inf))
out <- prepDocuments(processed$documents, processed$vocab, processed$meta,
                     lower.thresh = 2) 
# removes infrequent terms depending on user-set parameter lower.thresh (the minimum number of documents a word needs to appear in order for the word to be kept within the vocabulary)
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
```


```{r stm_searchK, cache=TRUE}
source("stm/R/searchK.R") # use my own version of this command
source("stm/R/plot.searchK.R") # use my own version of this command

ntopics <- searchK(out$documents, out$vocab, 
                   K = c(8:20), # how many topics?
                   iter=10,
                   data = meta,
                   prevalence =~ child,
                   content =~ child) 
ntopics.results <- ntopics$results %>%
  gather(key="measure", value="value", -K) %>%
  filter(measure!="em.its" & measure!="iter" & measure!="bound") # drop measures from the plot
ggplot(ntopics.results, aes(y=value, x=K)) +
  geom_point(alpha=.5) +
  geom_smooth(n=20) +
  facet_wrap(~ measure, scales="free") +
  labs(y=NULL, x="Number of topics") +
  ggtitle(paste0("searchK() results\n", paste(format(ntopics$call), collapse="")))
ggsave(filename=paste0("plots/topicmodeling/searchK_results_iter_winutts", wn ,".png"), width=12, height=6, units="in")

nK <- 12
```


```{r lda_model}
# Need to reindex docs to start counting vocab at 0 instead of 1 for lda package
docs.lda <- docs
for(i in 1:length(docs.lda)){
  docs.lda[[i]][1,] <- as.integer(docs[[i]][1,]-1)
  if(dim(docs.lda[[i]])[1] != 2) message(paste("wrong number of rows in document", i))
  if(dim(docs.lda[[i]])[2] < 1) message(paste("wrong number of columns in document", i))
}

lda <- lda.collapsed.gibbs.sampler(docs.lda,
                            K=nK,
                            vocab=vocab,
                            num.iterations = 500, # 25 is the number used in the lda demo
                            0.1, # from demo
                            0.1, # from demo
                            burnin = 5,
                            compute.log.likelihood=TRUE)

plot(lda$log.likelihoods[1,], 
     ylab="log likelihood",
     xlab="iteration",
     main="full log likelihood (including the prior)")
plot(lda$log.likelihoods[2,], 
     ylab="log likelihood",
     xlab="iteration",
     main="log likelihood of the observations\n conditioned on the assignments")

top.topic.words(lda$topics, 10, by.score=TRUE) ## Get the top words for each topic
```

```{r lda_doc_loadings}
source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/R/topic_modeling_functions.R")
# from topic_modeling_functions.r
df.lda <- doc_loadings(method="lda", 
                             model=lda, 
                             meta=out$meta, 
                             df.wn.count)   

write.table(df.lda, file=paste0("contexts_files/contexts_LDA_winutts", wn, ".txt"), quote=F, col.names=T, row.names=F, append=F, sep="\t")

hist(unlist(df.lda[,4:ncol(df.lda)]), breaks=50)
```

```{r lda_threshold}
thresholds <- seq(0,.75,.05) # what thresholds to try


# any additional values to be saved in the plot name can be included as extra arguments
threshold_plots(df.lda, 
                thresholds=thresholds, 
                method="lda",
                save.to="plots/topicmodeling",
                wn=wn,
                remove.stops=TRUE) 
```


```{r lda_apply_threshold}
# apply threshold
lda.threshold <- .3 # set threshold for inclusion in a context

df.lda.bin <- apply_threshold(df.lda, lda.threshold)

write.table(df.lda.bin, file=paste0("contexts_files/contexts_LDA_bin_winutts", wn, ".txt"), quote=F, col.names=T, row.names=F, append=F, sep="\t")
```


```{r stm_model, cache=TRUE}
# Metadata covariates for topical prevalence allow the observed metadata to affect the frequency with which a topic is discussed. 
# Topical prevalence captures how much each topic contributes to a document. Because different documents come from different sources, it is natural then to want to allow this prevalence to vary with metadata that we have about document sources.
# Covariates in topical content allow the observed metadata to affect the word rate use within a given topic–that is, how a particular topic is discussed.
# A topical content variable allows for the vocabulary used to talk about a particular topic to vary.
# As with all mixed-membership topic models, the posterior is intractable and non-convex, which creates a multimodal estimation problem that can be sensitive to initialization. Put differently, the answers the estimation procedure comes up with may depend on starting values of the parameters (e.g., the distribution over words for a particular topic). There are two approaches to dealing with this that the STM package facilitates. The first is to use a specific intitialization based on the method of moments, which is deterministic and globally consistent under reasonable conditions (Roberts et al. Forthcoming). This is known as a spectral initialization.10 In practice we have found this intialization to be very helpful. This can be chosen by setting init.type = "Spectral" in the stm function. We use this option in the above example. This means that no matter the seed that is set, the same results will be generated. However, it currently does not scale to extremely large vocabularies (uncommon in most applications) in which case alternative initializations are available
stm.null <- stm(out$documents, # the documents
           out$vocab, # the words
           K = nK, # number of topics
           max.em.its = 75, # set to run for a maximum of 75 EM iterations
           data = out$meta, # all the variables
           init.type = "Spectral")
stm.cont <- stm(out$documents, # the documents
           out$vocab, # the words
           K = nK, # number of topics
           content =~ child,
           max.em.its = 75, 
           data = out$meta, 
           init.type = "Spectral")
stm.prev <- stm(out$documents, # the documents
           out$vocab, # the words
           K = nK, # number of topics
           prevalence =~ child,
           max.em.its = 75, 
           data = out$meta, 
           init.type = "Spectral")
stm.both <- stm(out$documents, # the documents
           out$vocab, # the words
           K = nK, # number of topics
           prevalence =~ child, 
           content =~ child,
           max.em.its = 75, 
           data = out$meta, 
           init.type = "Spectral")

stms <- list(stm.null=stm.null, stm.cont=stm.cont, stm.prev=stm.prev, stm.both=stm.both)

sink("plots/topicmodeling/modelinfo.txt", append = T, type="output")
paste0("\n",date(), "\n")
for(f in 1:length(stms)){
  print((names(stms)[[f]]))
  print(paste("\n", wn, "utterances per document\n"))
  stm <- stms[[f]]
  
  pdf(paste0("plots/topicmodeling/topics_winutts", wn, "_",names(stms)[[f]], ".pdf"))
  plot.STM(stm, type = "summary", main = paste0("Topics\n", names(stms)[[f]])) 
  dev.off()
  
  print(labelTopics(stm))
  print(findThoughts(stm, texts=out$meta$documents, topics=1:nK, n=3))
  
  pdf(paste0("plots/topicmodeling/convergence_winutts", wn, "_",names(stms)[[f]], ".pdf"))
  plot(stm$convergence$bound, type = "l",
     ylab = "Approximate Objective",
     main = paste0("Convergence\n", names(stms)[[f]]))
  dev.off()
  
  pdf(paste0("plots/topicmodeling/topic_net_winutts", wn, "_",names(stms)[[f]], ".pdf"))
  # topicCorr(stm) # lots of output :)
  plot(topicCorr(stm), main = paste0("Topic correlations\n", names(stms)[[f]]))
  dev.off()
  
  pdf(paste0("plots/topicmodeling/topic_heatmap_winutts", wn, "_",names(stms)[[f]], ".pdf"))
  c <- topicCorr(stm)$cor # just the correlations between topics
  print(round(c, 2))
  diag(c) <- NA # remove the 1's on the diagonal so we can see the variation in the correlations
  heatmap(c, keep.dendro=F, symm=TRUE, main = paste0("Topic correlations: max", round(max(c), 2), "\n", names(stms)[[f]]))
  dev.off()
}
sink(NULL)

stm <- stms$stm.both
```

```{r stm_estimateEffect}
est <- estimateEffect(1:nK ~ child, stm, meta=out$meta)

prev.ests <- plot(est, "child")
# mean point estimates
prev.ests.means <- prev.ests$means
names(prev.ests.means) <- paste0("topic", prev.ests$topics)
prev.ests.means <- as.data.frame(prev.ests.means)
prev.ests.means$child <- levels(prev.ests$uvals)
prev.ests.means <- gather(prev.ests.means, key="topic", value="mean", -child)
# ci point estimates
prev.ests.cis <- prev.ests$cis
names(prev.ests.cis) <- paste0("topic", 1:nK)
prev.ests.cis.df <- data.frame(ci.2.5=NULL, ci.97.5=NULL, child=NULL, topic=NULL)
for(k in prev.ests$topics){
  c <- as.data.frame(prev.ests.cis[[k]])
  c <- as.data.frame(t(c))
  colnames(c) <- c("ci.2.5", "ci.97.5")
  c$child <- levels(prev.ests$uvals)
  c$topic <- names(prev.ests.cis[k])
  row.names(c) <- NULL
  prev.ests.cis.df <- rbind(prev.ests.cis.df, c)
}
prev.ests.df <- left_join(prev.ests.means, prev.ests.cis.df, by=c("child", "topic"))

ggplot(prev.ests.df, aes(x=child, y=mean, fill=child)) +
  geom_bar(position=position_dodge(), stat="identity") +
  geom_errorbar(aes(ymin=ci.2.5, ymax=ci.97.5),
                width=.2,                    # Width of the error bars
                position=position_dodge(.9)) +
  facet_wrap(~topic) + 
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  labs(y="topic prevalence", x=NULL, title="Topic prevalence by child\nwith 95CI error bars")
ggsave(filename=paste0("plots/topicmodeling/topic_prevalence_STM_winutts", wn ,".png"), width=12, height=12, units="in")
```

The topic loading for a document is the percentage of words attributable to a latent topic. [https://scholar.princeton.edu/sites/default/files/bstewart/files/ajpsappendix.pdf](https://scholar.princeton.edu/sites/default/files/bstewart/files/ajpsappendix.pdf)
```{r stm_doc_loadings}
# from topic_modeling_functions.r
df.stm <- doc_loadings(method="stm", 
                             model=stm, 
                             meta=out$meta, 
                             df.wn.count) 

write.table(df.stm, file=paste0("contexts_files/contexts_STM_winutts", wn, ".txt"), quote=F, col.names=T, row.names=F, append=F, sep="\t")

hist(unlist(df.stm[,5:16]), breaks=50)
```


```{r stm_threshold}

# any additional values to be saved in the plot name can be included as extra arguments
threshold_plots(df.stm, 
                   thresholds=thresholds, # what thresholds to try
                   method="stm",
                   save.to="plots/topicmodeling",
                   wn=wn,
                   remove.stops=TRUE) 
```


```{r stm_apply_threshold}
# apply threshold
stm.threshold <- .2 # set threshold for inclusion in a context

df.stm.bin <- apply_threshold(df.stm, stm.threshold)

write.table(df.stm.bin, file=paste0("contexts_files/contexts_STM_bin_winutts", wn, ".txt"), quote=F, col.names=T, row.names=F, append=F, sep="\t")
```

## agreement
```{r agreement_data_wrangling}
WL <- read.table("contexts_files/contexts_WL.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") %>% 
  mutate_each(funs(y=ifelse(.>0, 1, 0)), mealtime:routines) # for word list analysis only, make all contexts either 1 or 0 (smoothing over 1.5's from expand_windows)
# remove underscores from WL orth column
WL$orth <- gsub(x=WL$orth, pattern="_", replacement=" ")

STM.con <- read.table(paste0("contexts_files/contexts_STM_winutts", wn, ".txt"), header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 
STM.bin <- read.table(paste0("contexts_files/contexts_STM_bin_winutts", wn, ".txt"), header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 

LDA.con <- read.table(paste0("contexts_files/contexts_LDA_winutts", wn, ".txt"), header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="")
LDA.bin <- read.table(paste0("contexts_files/contexts_LDA_bin_winutts", wn, ".txt"), header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="")

HJ.con <- read.table("contexts_files/contexts_HJ.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 
HJ.bin <- read.table("contexts_files/contexts_HJ_bin.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 
```

```{r xtabs}
cat_agreement(cat.codes=list(WL=WL, 
                             STM.bin=STM.bin))
cat_agreement(cat.codes=list(WL=WL, 
                             HJ.bin=HJ.bin))
cat_agreement(cat.codes=list(HJ.bin=HJ.bin, 
                             STM.bin=STM.bin))
```


```{r logistic_regression}
nrow(WL); nrow(STM.con); nrow(STM.bin); nrow(HJ.con); nrow(HJ.bin)

# drop orth and phon in case there are superficial issues matching them with the join() below
WL <- select(WL, -orth, -phon)
STM.con <- select(STM.con, -orth, -phon)
STM.bin <- select(STM.bin, -orth, -phon)
LDA.con <- select(LDA.con, -orth, -phon)
LDA.bin <- select(LDA.bin, -orth, -phon)
HJ.con <- select(HJ.con, -orth, -phon)
HJ.bin <- select(HJ.bin, -orth, -phon)
colnames(WL)[-1] <- paste0("WL_", colnames(WL)[-1])
colnames(STM.con)[-1] <- paste0("STM.con_", colnames(STM.con)[-1])
colnames(STM.bin)[-1] <- paste0("STM.bin_", colnames(STM.bin)[-1])
colnames(LDA.con)[-1] <- paste0("LDA.con_", colnames(LDA.con)[-1])
colnames(LDA.bin)[-1] <- paste0("LDA.bin_", colnames(LDA.bin)[-1])
colnames(HJ.con)[-1] <- paste0("HJ.con_", colnames(HJ.con)[-1])
colnames(HJ.bin)[-1] <- paste0("HJ.bin_", colnames(HJ.bin)[-1])

all.methods <- full_join(WL, STM.con, by="utt") %>% 
  full_join(STM.bin, by="utt") %>% 
  full_join(LDA.con, by="utt") %>% 
  full_join(LDA.bin, by="utt") %>%
  full_join(HJ.con, by="utt") %>% 
  full_join(HJ.bin, by="utt")
nrow(all.methods)  

# univariate logistic regressions
WL_STM.con <- logistic_regressions(all.methods, outcome_method="WL", predictor_method="STM.con", save.to="plots/agreement")
WL_HJ.con  <- logistic_regressions(all.methods, outcome_method="WL", predictor_method="HJ.con", save.to="plots/agreement")
HJ.bin_STM.con  <- logistic_regressions(all.methods, outcome_method="HJ.bin", predictor_method="STM.con", save.to="plots/agreement")
STM.bin_HJ.con  <- logistic_regressions(all.methods, outcome_method="STM.bin", predictor_method="HJ.con", save.to="plots/agreement")

# all.methods <- na.omit(all.methods) ; nrow(all.methods) # listwise deletion (i.e. only analyze utterances we have for every method)

# multivariate logistic regression
WL_contexts <- as.matrix(select(all.methods, starts_with("WL_")))
STM.con_contexts <- as.matrix(select(all.methods, starts_with("STM.con_")))
WL_multivariate <- MCMCglmm(WL_contexts ~ STM.con_contexts, data=all.methods, family="categorical")

WL_mealtime <- glm(all.methods$WL_mealtime ~ as.matrix(select(all.methods, starts_with("STM.con_"))), family = binomial(link="logit")) 
```


## run on aciss
This code is all run on ACISS (the UO high performance cluster), not on my local machine. 
```{r on_aciss_first_time_only, eval=FALSE}
# system('ssh rosem@login.aciss.uoregon.edu') # SSH into ACISS
# system('module add R/3.1.3') # start R

# http://blogs.uoregon.edu/rclub/2015/06/02/easy-massively-parallel-r-on-uos-aciss-cluster/
# https://nsaunders.wordpress.com/2015/04/01/configuring-the-r-batchjobs-package-for-torque-batch-queues/

# http://aciss-computing.uoregon.edu/2013/09/04/how-to-submission-queues/

###################################
# Write simple.tmpl file for BatchJobs configuration
###################################
# from https://raw.githubusercontent.com/tudo-r/BatchJobs/master/examples/cfTorque/simple.tmpl
simple <- "#PBS -N <%= job.name %>
## merge standard error and output
#PBS -j oe
## direct streams to our logfile
#PBS -o <%= log.file %>
#PBS -l walltime=<%= resources$walltime %>,nodes=<%= resources$nodes %>,vmem=<%= resources$memory %>
## remove this line if your cluster does not support arrayjobs
#PBS -t 1-<%= arrayjobs %>

## Run R:
## we merge R output with stdout from PBS, which gets then logged via -o option
module load R
R CMD BATCH --no-save --no-restore '<%= rscript %>' /dev/stdout
"

writeLines(simple, "simple.tmpl", sep="\n")

###################################
# install BatchJobs
###################################
library(BatchJobs)

###########################################################
# configure BatchJobs (do this on your local machine, and then use sftp to send .Batchjobs.R and simple.tmpl to your working directory on ACISS)
###########################################################
# set up batchjobs configuration for this project to override the global settings
batch.conf <- readLines("/Library/Frameworks/R.framework/Versions/3.1/Resources/library/BatchJobs/etc/BatchJobs_global_config.R")
batch.conf[1] <- "cluster.functions = makeClusterFunctionsTorque('simple.tmpl')"

writeLines(batch.conf, ".BatchJobs.R", sep="\n") # write this file to the current working directory

loadConfig()
# then SFTP the .BatchJobs.R file to ACISS
# system('sftp rosem@ftn.aciss.uoregon.edu')
```


```{r on_aciss_setup, eval=FALSE}
rm(list = ls())
system('rm -r *files') # delete any old BatchJobs folders

library(dplyr); library(tidyr); library(doParallel); library(devtools); library(BatchJobs)
sessionInfo() # to check
getConfig() # to check

# read in the functions written for this analysis
source_url("https://raw.githubusercontent.com/rosemm/context_word_seg/master/R/data_processing_functions.r")

df_LDA <- get_from_https("https://raw.githubusercontent.com/rosemm/context_word_seg/master/contexts_files/contexts_LDA_bin_winutts30.txt")

df_STM <- get_from_https("https://raw.githubusercontent.com/rosemm/context_word_seg/master/contexts_files/contexts_STM_bin_winutts30.txt")

df_HJ <- get_from_https("https://raw.githubusercontent.com/rosemm/context_word_seg/master/contexts_files/contexts_HJ_bin.txt")
df_HJ <- df_HJ[ ,c(TRUE, TRUE, TRUE, colSums(df_HJ[,4:ncol(df_HJ)], na.rm=T) > 100)] # only keep contexts with at least 100 utterances, otherwise it's hard to estimate

df_WL <- get_from_https("https://raw.githubusercontent.com/rosemm/context_word_seg/master/contexts_files/contexts_WL.txt") 
df_WL[df_WL==1.5] <- 1 # replace all 1.5's with 1's, so it doesn't distinguish between utterances originally tagged with key words and ones picked up by the +-2 uterance smoothing

dict <- read.table("dict_all3_updated.txt", sep="\t", quote="", comment.char ="", header=1, stringsAsFactors=F)

fun.version <- "d866eaee5e1850" # fun.version refers to the current commit for data_processing_functions.r
```

```{r on_aciss_params, eval=FALSE}
run_analysis_args <- list(dataframe=NULL, 
                          dict=dict, 
                          consider.freq=TRUE,
                          embedding.rule=TRUE,
                          trisyl.limit=TRUE,
                          N.types=NULL, 
                          N.utts=NULL, 
                          by.size=FALSE, 
                          expand=FALSE, 
                          seg.utts=TRUE, 
                          TP=FALSE, 
                          MI=TRUE, 
                          verbose=FALSE, 
                          prop=FALSE, 
                          cutoff=.85, 
                          nontext=TRUE, 
                          quiet=TRUE,
                          fun.version=fun.version)
```


```{r on_aciss_nontexts, eval=FALSE}

# 100 starts and 20 iter gives 2000 samples for each nontext (takes about 60min per 1000 samples)
run_analysis_args$dataframe <- df_WL
aciss_function(fun.version, id="nontexts_WL", starts=1:50, iter=200, run_analysis_args) 

run_analysis_args$dataframe <- df_STM
aciss_function(fun.version, id="nontexts_STM", starts=1:50, iter=200, run_analysis_args) 

run_analysis_args$dataframe <- df_HJ
aciss_function(fun.version, id="nontexts_HJ", starts=1:50, iter=200, run_analysis_args)

run_analysis_args$dataframe <- df_LDA
aciss_function(fun.version, id="nontexts_LDA", starts=1:50, iter=200, run_analysis_args)

showStatus(makeRegistry(id = "nontexts_WL")); showStatus(makeRegistry(id = "nontexts_STM")); showStatus(makeRegistry(id = "nontexts_HJ")); showStatus(makeRegistry(id = "nontexts_LDA"))

###################################################
# Include freq cutoff as well (a la Swingley 2005)
###################################################
run_analysis_args$consider.freq <- TRUE
run_analysis_args$embedding.rule <- TRUE
run_analysis_args$trisyl.limit <- TRUE
run_analysis_args$cutoff <- .75

run_analysis_args$dataframe <- df_WL
aciss_function(fun.version, id="nontexts_WL_freq", starts=1:50, iter=200, run_analysis_args) 

run_analysis_args$dataframe <- df_STM
aciss_function(fun.version, id="nontexts_STM_freq", starts=1:50, iter=100, run_analysis_args) 

run_analysis_args$dataframe <- df_HJ
aciss_function(fun.version, id="nontexts_HJ_freq", starts=1:50, iter=100, run_analysis_args)

run_analysis_args$dataframe <- df_LDA
aciss_function(fun.version, id="nontexts_LDA_freq", starts=1:50, iter=200, run_analysis_args)

showStatus(makeRegistry(id = "nontexts_WL_freq")); showStatus(makeRegistry(id = "nontexts_STM_freq")); showStatus(makeRegistry(id = "nontexts_HJ_freq")); showStatus(makeRegistry(id = "nontexts_LDA_freq"))

###################################################
# Old expand version of nontexts
###################################################
df_WL_old <- get_from_https("https://raw.githubusercontent.com/rosemm/context_word_seg/master/contexts_files/contexts_WL.txt") # keep the 1.5's and 1's

run_analysis_args <- list(dataframe=df_WL_old, 
                          dict=dict, 
                          consider.freq=FALSE,
                          embedding.rule=FALSE,
                          trisyl.limit=FALSE,
                          N.types=NULL, 
                          N.utts=NULL, 
                          by.size=FALSE, 
                          expand=TRUE, 
                          seg.utts=TRUE, 
                          TP=FALSE, 
                          MI=TRUE, 
                          verbose=FALSE, 
                          prop=FALSE, 
                          cutoff=.85, 
                          nontext=TRUE, 
                          quiet=TRUE,
                          fun.version=fun.version)
aciss_function(fun.version, id="nontexts_WL_expand", starts=1:50, iter=100, run_analysis_args)

showStatus(makeRegistry(id = "nontexts_WL_expand")); showStatus(makeRegistry(id = "nontexts_WL"))

system('qstat -antu rosem') # to show all of my current jobs

# loadResults(makeRegistry(id = "nontexts_WL"))

# killJobs(makeRegistry(id = "nontexts_WL"),findRunning(makeRegistry(id = "nontexts_WL")))
# killJobs(makeRegistry(id = "nontexts_HJ"),findRunning(makeRegistry(id = "nontexts_HJ")))
# killJobs(makeRegistry(id = "nontexts_LDA"),findRunning(makeRegistry(id = "nontexts_LDA")))
# killJobs(makeRegistry(id = "nontexts_STM"),findRunning(makeRegistry(id = "nontexts_STM")))

# system('sftp rosem@ftn.aciss.uoregon.edu')
```

```{r on_aciss_simulations, eval=FALSE}
###########################################
# does cutoff affect results?
###########################################
run_analysis_args$consider.freq <- FALSE
run_analysis_args$embedding.rule <- FALSE
run_analysis_args$trisyl.limit <- FALSE

starts <- seq(.4, .9, by=.05)
run_analysis_args$cutoff <- as.name("start") # doesn't work

run_analysis_args$dataframe <- df_WL
aciss_function(fun.version, id="Sim_cutoff_WL", starts=starts, iter=12, run_analysis_args) 

run_analysis_args$dataframe <- df_STM
aciss_function(fun.version, id="Sim_cutoff_STM", starts=starts, iter=12, run_analysis_args) 

run_analysis_args$dataframe <- df_HJ
aciss_function(fun.version, id="Sim_cutoff_HJ", starts=starts, iter=12, run_analysis_args)

run_analysis_args$dataframe <- df_LDA
aciss_function(fun.version, id="Sim_cutoff_LDA", starts=starts, iter=12, run_analysis_args)

showStatus(makeRegistry(id = "Sim_cutoff_WL")); showStatus(makeRegistry(id = "Sim_cutoff_STM")); showStatus(makeRegistry(id = "Sim_cutoff_HJ")); showStatus(makeRegistry(id = "Sim_cutoff_LDA"))

###########################################
# does corpus size affect results?
###########################################
run_analysis_args$cutoff <- .85 # reset from previous
run_analysis_args$by.size <- TRUE
run_analysis_args$N.types <- NULL
run_analysis_args$N.utts <- NULL
  
run_analysis_args$dataframe <- df_WL[ ,1:3]
aciss_function(fun.version, id="Sim_size_Korman", starts=1:100, iter=12, run_analysis_args) # this takes much longer to run, so need to use fewer iterations so ACISS doesn't kill the job

run_analysis_args$N.types <- 1000
run_analysis_args$N.utts <- 1000

run_analysis_args$dataframe <- "skewed"
aciss_function(fun.version, id="Sim_size_skew", starts=1:50, iter=100, run_analysis_args)

run_analysis_args$dataframe <- "unif"
aciss_function(fun.version, id="Sim_size_unif", starts=1:50, iter=100, run_analysis_args)

showStatus(makeRegistry(id = "Sim_size_Korman")); showStatus(makeRegistry(id = "Sim_size_skew")); showStatus(makeRegistry(id = "Sim_size_unif"))

# to correct mistakes:
# killJobs(makeRegistry(id = "Sim_size_Korman"),findRunning(makeRegistry(id = "Sim_size_Korman")))
# removeRegistry(makeRegistry(id = "Sim_size_Korman"), ask="no")
```

Use SFTP to get the folders BatchJobs makes and put them on my local machine. 

## read in aciss results
```{r sim_results_data_wrangling}
size_skew <- read_batch(dir="Sim_size_skew-files")
size_unif <- read_batch(dir="Sim_size_unif-files")
size_korm <- read_batch(dir="Sim_size_Korman-files")

# combine all the sim results into one data frame
size_skew$sim <- "size"
size_unif$sim <- "size"
size_korm$sim <- "size"
size_skew$method <- "skew"
size_unif$method <- "unif"
size_korm$method <- "korman"

sim_results <- rbind(size_skew, size_unif, size_korm)

sim_results$method <- factor(sim_results$method, 
                             levels=c("skew", "unif", "korman"),
                             labels=c("art lang (skew)", "art lang (unif)", "Korman"))
sim_results$sim <- factor(sim_results$sim)

# gather for plotting
sim_results <- sim_results %>% 
  gather( key="measure", value="value", recall, precision) %>% 
  unite(col="criterion", stat, cutoff, sep="_", remove=FALSE) 

```

```{r sim_result_plotting}
# all simulations: precision and recall
ggplot(filter(sim_results, sim=="size"), aes(x=N.utts, y=value)) +
  geom_jitter(alpha=.2, width = 0.4) +
  facet_wrap( ~ method + measure, scales="free", ncol=2) +
  theme(text = element_text(size=30), axis.ticks = element_blank()) +
  labs(x=NULL, y=NULL) 
ggsave(filename="plots/sim_results/sim_results_size.png", width=12, height=20, units="in")

# just Korman corpus: precision and recall
ggplot(filter(sim_results, sim=="size" & method=="Korman"), aes(x=N.utts, y=value)) +
  geom_jitter(alpha=.2, width = 0.4) +
  facet_wrap( ~ measure, scales="free", ncol=2) +
  theme(text = element_text(size=30), axis.ticks = element_blank()) +
  labs(x="Number of utterances", y=NULL) 
ggsave(filename="plots/sim_results/sim_results_size_Korman.png", width=12, height=6, units="in")

# Korman corpus with context estimates: precision and recall
ggplot(filter(sim_results, sim=="size" & method=="Korman"), aes(x=N.utts, y=value)) +
  geom_jitter(alpha=.2, width = 0.4) +
  geom_point(data=context_all, aes(x=N.utts, y=value, color=method), size=4, alpha=.5) +
  facet_wrap( ~ measure, scales="free", ncol=2) +
  theme(text = element_text(size=30), axis.ticks = element_blank()) +
  labs(x="Number of utterances", y=NULL) 
ggsave(filename="plots/sim_results/sim_results_size_Korman_withcontexts.png", width=12, height=6, units="in")
# Korman corpus with context estimates: TTR
ggplot(filter(sim_results, sim=="size" & method=="Korman"), aes(x=N.utts, y=TTR)) +
  geom_jitter(alpha=.2, width = 0.4) +
  geom_point(data=context_all, aes(color=method, shape=method), size=4, show.legend=TRUE) +
  facet_wrap( ~ measure, scales="free", ncol=2) +
  theme(text = element_text(size=30), axis.ticks = element_blank()) +
  labs(x="Number of utterances", y=NULL) 
# Korman corpus with context estimates: prop most freq syl
ggplot(filter(sim_results, sim=="size" & method=="Korman"), aes(x=N.utts, y=TTR)) +
  geom_jitter(alpha=.2, width = 0.4) +
  geom_point(data=context_all, aes(color=method, shape=method), size=4, show.legend=TRUE) +
  facet_wrap( ~ measure, scales="free", ncol=2) +
  theme(text = element_text(size=30), axis.ticks = element_blank()) +
  labs(x="Number of utterances", y=NULL) 

```


```{r}
HJresults.summary <- HJresults %>%
  gather(key=measure, value=value, recall:precision) %>%
  group_by(analysis, nontext, measure) %>%
  summarize(mean=mean(value), sd=sd(value)) %>%
  rename(context=nontext)
HJresults.summary <- as.data.frame(HJresults.summary)

HJresults.plot <- HJresults %>%
  gather(key=measure, value=value, recall:precision) %>%
  rename(context=nontext)

# histograms
ggplot(HJresults.plot, aes(x=value)) +
  geom_histogram(data=filter(HJresults.plot, analysis=="nontext"), alpha=.5) +
  geom_histogram(data=filter(HJresults.plot, analysis=="context"), aes(x=value, fill=context), alpha=.5) +
  facet_wrap(~ measure + context, scales="free", ncol=12) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y=NULL)

# barplots
ggplot(HJresults.summary, aes(x=context, y=mean, fill=analysis)) +
  geom_bar(position=position_dodge(), stat="identity") +
  geom_errorbar(aes(ymin=mean - sd, ymax=mean + sd),
                width=.2,                    # Width of the error bars
                position=position_dodge(.9)) +
  facet_wrap(~ measure) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL) 
```

## context results
```{r context_results}

dict <- read.table("dict_all3_updated.txt", sep="\t", quote="", comment.char ="", header=1, stringsAsFactors=F)

df_STM <- read.table("contexts_files/contexts_STM_bin_winutts30.txt", sep="\t", quote="", comment.char ="", header=1, stringsAsFactors=F)
df_LDA <- read.table("contexts_files/contexts_LDA_bin_winutts30.txt", sep="\t", quote="", comment.char ="", header=1, stringsAsFactors=F)
df_HJ <- read.table("contexts_files/contexts_HJ_bin.txt", sep="\t", quote="", comment.char ="", header=1, stringsAsFactors=F)
df_HJ <- df_HJ[ ,c(TRUE, TRUE, TRUE, colSums(df_HJ[,4:ncol(df_HJ)], na.rm=T) > 100)] # only keep contexts with at least 100 utterances, otherwise it's hard to estimate
df_WL <- read.table("contexts_files/contexts_WL.txt", sep="\t", quote="", comment.char ="", header=1, stringsAsFactors=F)
df_WL[df_WL==1.5] <- 1 # replace all 1.5's with 1's, so it doesn't distinguish between utterances originally tagged with key words and ones picked up by the +-2 uterance smoothing

run_analysis_args <- list(dataframe=NULL, 
                          dict=dict, 
                          consider.freq=FALSE,
                          embedding.rule=FALSE,
                          trisyl.limit=FALSE,
                          N.types=NULL, 
                          N.utts=NULL, 
                          by.size=FALSE, 
                          expand=FALSE, 
                          seg.utts=TRUE, 
                          TP=FALSE, 
                          MI=TRUE, 
                          verbose=TRUE, 
                          prop=FALSE, 
                          cutoff=.85, 
                          nontext=FALSE, 
                          quiet=TRUE,
                          fun.version=fun.version)

run_analysis_args$dataframe <- df_WL
context_WL_results <- run_analysis(run_analysis_args)
# context_WL <- context_WL_results$stat.results
# context_WL_detail <- context_WL_results$addl.info

run_analysis_args$dataframe <- df_STM
context_STM_results <- run_analysis(run_analysis_args)
# context_STM <- context_STM_results$stat.results
# context_STM_detail <- context_STM_results$addl.info

run_analysis_args$dataframe <- df_LDA
context_LDA_results <- run_analysis(run_analysis_args)
# context_LDA <- context_LDA_results$stat.results
# context_LDA_detail <- context_LDA_results$addl.info

run_analysis_args$dataframe <- df_HJ
context_HJ_results <- run_analysis(run_analysis_args)
# context_HJ <- context_HJ_results$stat.results
# context_HJ_detail <- context_HJ_results$addl.info

####################
# with freq
####################
run_analysis_args$consider.freq <- TRUE
run_analysis_args$embedding.rule <- TRUE
run_analysis_args$trisyl.limit <- TRUE
run_analysis_args$cutoff <- .75

run_analysis_args$dataframe <- df_WL
context_WL_freq_results <- run_analysis(run_analysis_args)
context_WL_freq <- context_WL_freq_results$stat.results
context_WL_freq_seg <- context_WL_freq_results$MI85.seg.results
# context_WL_freq_detail <- context_WL_freq_results$addl.info

run_analysis_args$dataframe <- df_STM
context_STM_freq_results <- run_analysis(run_analysis_args)
context_STM_freq <- context_STM_freq_results$stat.results
context_STM_freq_seg <- context_STM_freq_results$MI85.seg.results
# context_STM_freq_detail <- context_STM_freq_results$addl.info

run_analysis_args$dataframe <- df_LDA
context_LDA_freq_results <- run_analysis(run_analysis_args)
context_LDA_freq <- context_LDA_freq_results$stat.results
context_LDA_freq_seg <- context_LDA_freq_results$MI85.seg.results
# context_LDA_freq_detail <- context_LDA_freq_results$addl.info

run_analysis_args$dataframe <- df_HJ
context_HJ_freq_results <- run_analysis(run_analysis_args)
context_HJ_freq <- context_HJ_freq_results$stat.results
context_HJ_freq_seg <- context_HJ_freq_results$MI85.seg.results
# context_HJ_freq_detail <- context_HJ_freq_results$addl.info

```

```{r context_results_data_wrangling}
# combine all the context results into one data frame
context_WL$method <- "WL"
context_STM$method <- "STM"
context_LDA$method <- "LDA"
context_HJ$method <- "HJ"

context_all <- rbind(context_WL, context_STM, context_LDA, context_HJ)

context_all$method.short <- factor(context_all$method)
context_all$method <- factor(context_all$method, 
                                 levels=c("WL", "WL_bare", "Fam", "STM", "LDA", "HJ"), 
                                 labels = c("word lists", "word lists_no smoothing", "family", "topic modeling (STM)", "topic modeling (LDA)", "subjective judgments"))


# gather for plotting
context_all <- context_all %>% 
  gather( key="measure", value="value", recall, precision) %>% 
  unite(col="criterion", stat, cutoff, sep="_", remove=FALSE) %>% 
  rename(context=nontext) 

context_all$measure <- as.factor(context_all$measure)

############
# with freq
#############
# combine all the context results into one data frame
context_WL_freq$method <- "WL"
context_STM_freq$method <- "STM"
context_LDA_freq$method <- "LDA"
context_HJ_freq$method <- "HJ"

context_all_freq <- rbind(context_WL_freq, context_STM_freq, context_LDA_freq, context_HJ_freq)

context_all_freq$method.short <- factor(context_all_freq$method)
context_all_freq$method <- factor(context_all_freq$method, 
                                 levels=c("WL", "WL_bare", "Fam", "STM", "LDA", "HJ"), 
                                 labels = c("word lists", "word lists_no smoothing", "family", "topic modeling (STM)", "topic modeling (LDA)", "subjective judgments"))


# gather for plotting
context_all_freq <- context_all_freq %>% 
  unite(col="criterion", stat, cutoff, sep="_", remove=FALSE) %>% 
  rename(context=nontext) 
```

```{r context_detail_data_wrangling}
# combine all the context additional info into one data frame
context_WL_detail$method <- "WL"
context_STM_detail$method <- "STM"
context_LDA_detail$method <- "LDA"
context_HJ_detail$method <- "HJ"

context_detail <- rbind(context_WL_detail, context_STM_detail, context_LDA_detail, context_HJ_detail)

context_detail$method.short <- factor(context_detail$method)
context_detail$method <- factor(context_detail$method, 
                                 levels=c("WL", "Fam", "STM", "LDA", "HJ"), 
                                 labels = c("word lists", "family", "topic modeling (STM)", "topic modeling (LDA)", "subjective judgments")) 
context_detail_summary <- context_detail %>% 
  group_by(method.short, context) %>% 
  summarise(which.A=which.max(A.freq.tot), 
            which.B=which.max(B.freq.tot),
            freq.syl.A=syl1[which.A], 
            freq.syl.B=syl2[which.B],
            most.freq.syl=unique(freq.syl.A,freq.syl.B))
    
context_all <- left_join(context_all, select(context_detail_summary, method.short, context, most.freq.syl), by=c("context", "method.short"))

##############################
# segmentation details
##############################
context_WL_freq_seg$method <- "WL"
context_STM_freq_seg$method <- "STM"
context_LDA_freq_seg$method <- "LDA"
context_HJ_freq_seg$method <- "HJ"

context_seg <- rbind(context_WL_freq_seg, context_STM_freq_seg, context_LDA_freq_seg, context_HJ_freq_seg)

context_seg$method.short <- factor(context_seg$method)
context_seg$method <- factor(context_seg$method, 
                                 levels=c("WL", "Fam", "STM", "LDA", "HJ"), 
                                 labels = c("word lists", "family", "topic modeling (STM)", "topic modeling (LDA)", "subjective judgments")) 

context_seg_summary <- context_seg %>% 
  group_by(method, context, seg.result, N.syl) %>% 
  summarise(count=n(), freq=sum(freq)) %>% 
  gather(key="measure", value="value", count, freq) %>% 
  unite(col="key", measure, N.syl) %>% 
  spread(key=key, value=value) %>% 
  mutate(freq.tot=sum(freq_1, freq_2, freq_3, freq_4, freq_5, na.rm=TRUE))

```

## nontext results
```{r nontext_results_data_wrangling}
nontext_WL <- read_batch(dir="nontexts_WL-files")
nontext_STM <- read_batch(dir="nontexts_STM-files")
nontext_LDA <- read_batch(dir="nontexts_LDA-files")
nontext_HJ <- read_batch(dir="nontexts_HJ-files")

# combine all the context results into one data frame
nontext_WL$method <- "WL"
nontext_STM$method <- "STM"
nontext_LDA$method <- "LDA"
nontext_HJ$method <- "HJ"

nontext_results <- rbind(nontext_WL, nontext_STM, nontext_LDA, nontext_HJ)

nontext_results$method.short <- factor(nontext_results$method)
nontext_results$method <- factor(nontext_results_freq$method, 
                                 levels=c("WL", "WL_bare", "Fam", "STM", "LDA", "HJ"), 
                                 labels = c("word lists", "word lists_no smoothing", "family", "topic modeling (STM)", "topic modeling (LDA)", "subjective judgments"))

# gather for plotting
nontext_results <- nontext_results %>% 
  gather( key="measure", value="value", recall, precision) %>% 
  unite(col="criterion", stat, cutoff, sep="_", remove=FALSE) %>% 
  rename(context=nontext) 

full_results <- left_join(nontext_results, context_ests, by=c("criterion", "method", "context", "measure")) # add context estimates


################  
# with freq
################# 
nontext_WL <- read_batch(dir="nontexts_WL_freq-files")
nontext_STM <- read_batch(dir="nontexts_STM_freq-files")
nontext_LDA <- read_batch(dir="nontexts_LDA_freq-files")
nontext_HJ <- read_batch(dir="nontexts_HJ_freq-files")

# combine all the context results into one data frame
nontext_WL$method <- "WL"
nontext_STM$method <- "STM"
nontext_LDA$method <- "LDA"
nontext_HJ$method <- "HJ"

nontext_results_freq <- rbind(nontext_WL, nontext_STM, nontext_LDA, nontext_HJ)

nontext_results_freq$method.short <- factor(nontext_results_freq$method)
nontext_results_freq$method <- factor(nontext_results_freq$method, 
                                 levels=c("WL", "WL_bare", "Fam", "STM", "LDA", "HJ"), 
                                 labels = c("word lists", "word lists_no smoothing", "family", "topic modeling (STM)", "topic modeling (LDA)", "subjective judgments")) 

nontext_results_freq <- nontext_results_freq %>% 
  unite(col="criterion", stat, cutoff, sep="_", remove=FALSE) %>% 
  rename(context=nontext) 
```

```{r nontext_WL_expand}

nontext_WL_expand <- read_batch(dir="nontexts_WL_expand-files")
check.N.utts <- nontext_WL_expand %>% 
  group_by(nontext) %>% 
  summarize(mean=mean(N.utts), sd=sd(N.utts))
  
context.N.utts <- context_WL %>%
  rename(context=nontext) %>% 
  group_by(context) %>% 
  summarize(mean=mean(N.utts), sd=sd(N.utts))

plot.data <- nontext_WL_expand %>% 
  select(context=nontext, N.utts) 
ggplot(plot.data, aes(y=N.utts, x=context)) +
  geom_boxplot() +
  geom_point(data=context.N.utts, aes(y=mean, x=context, color=context), size=2,show.legend = F) + 
  facet_wrap(~context, scales = "free")
ggsave("plots/expand_windows_effect_facet.png")
```


## global results
```{r global_results}
run_analysis_args <- list(dataframe=NULL,  
                          dict=dict, 
                          consider.freq=FALSE,
                          embedding.rule=FALSE,
                          trisyl.limit=FALSE,
                          N.types=NULL, 
                          N.utts=NULL, 
                          by.size=FALSE,
                          expand=FALSE, 
                          seg.utts=TRUE, 
                          TP=FALSE, 
                          MI=TRUE, 
                          verbose=TRUE, 
                          prop=FALSE, 
                          cutoff=.85, 
                          nontext=FALSE, 
                          quiet=TRUE,
                          fun.version=fun.version)

run_analysis_args$dataframe <- df[1:100,] # only taking the first three columns, and 100 rows, for speed
quick_check <- run_analysis(run_analysis_args)

run_analysis_args$dataframe <- df[ , 1:3] # only taking the first three columns
global_data_results <- run_analysis(run_analysis_args)
global_results  <- global_data_results$stat.results
global_detail  <- global_data_results$addl.info

##################################################
# consider freq (also applies embedding constraint)
##################################################
run_analysis_args$dataframe <- df[ , 1:3] # only taking the first three columns
run_analysis_args$consider.freq <- TRUE
run_analysis_args$embedding.rule <- TRUE
run_analysis_args$trisyl.limit <- TRUE
run_analysis_args$verbose <- TRUE
run_analysis_args$cutoff <- .75

cutoffs <- seq(from=.1, to=.99, by=.01)
global_cutoff_results <- data.frame(NULL)
global_cutoff_pairs <- data.frame(NULL)
global_cutoff_TPseg <- data.frame(NULL)
global_cutoff_MIseg <- data.frame(NULL)
for(i in 1:length(cutoffs)){
  message(cutoffs[i])
  run_analysis_args$cutoff <- cutoffs[i]
  this.output <- run_analysis(run_analysis_args)
  
  this.result <- this.output$stat.results
  
  this.pairs <- this.output$unique.phon.pairs
  this.pairs$cutoff <- cutoffs[i]
  
  this.TPseg <- this.output$TP85.seg.results
  this.TPseg$cutoff <- cutoffs[i]
  
  this.MIseg <- this.output$MI85.seg.results
  this.MIseg$cutoff <- cutoffs[i]
  
  global_cutoff_results <- rbind(global_cutoff_results, this.result)
  global_cutoff_pairs <- rbind(global_cutoff_pairs, this.pairs)
  global_cutoff_TPseg <- rbind(global_cutoff_TPseg, this.TPseg)
  global_cutoff_MIseg <- rbind(global_cutoff_MIseg, this.MIseg)
}
write.csv(global_cutoff_results, "global_cutoff_results.csv")
write.csv(global_cutoff_pairs, "global_cutoff_pairs.csv")
write.csv(global_cutoff_MIseg, "global_cutoff_MIseg.csv")

results_by_syl <- global_cutoff_MIseg %>% 
  filter(N.syl < 4) %>% 
  group_by(cutoff, N.syl) %>% 
  summarize(precision=mean(precision, na.rm=TRUE),
            recall=mean(recall, na.rm=TRUE)) %>% 
  gather(key="measure", value="value", precision,recall)
ggplot(results_by_syl, aes(x=cutoff, y=value, group=N.syl, color=as.factor(N.syl))) + 
  geom_line() +
  facet_wrap(~measure)
ggplot(global_cutoff_results, aes(x=cutoff, y=precision)) + 
  geom_line(size=2) + 
  geom_line(data=filter(results_by_syl, measure=="precision"), aes(x=cutoff, y=value, group=N.syl, color=as.factor(N.syl)))
ggsave("plots/swingley_fig1a_global.png", height=8, width=8, units="in")

fig1b.data <- global_cutoff_MIseg %>% 
  filter(seg.result=="hit") %>% 
  count(cutoff, N.syl)
ggplot(fig1b.data, aes(x=cutoff, y=n, group=N.syl, color=as.factor(N.syl))) +
  geom_line()
ggsave("plots/swingley_fig1b_global.png", height=8, width=8, units="in")

global_data_freq_results <- run_analysis(run_analysis_args)
global_results_freq  <- global_data_freq_results$stat.results
# NOTE: no continuity measures for global
global_detail_freq  <- global_data_freq_results$addl.info

```

```{r global_results_data_wrangling}
global_results <- global_results %>% 
  gather( key="measure", value="value", recall, precision) %>% 
  unite(col="criterion", stat, cutoff, sep="_", remove=FALSE) %>% 
  left_join(unique(select(context_all, measure, method)), by="measure")
```

## THE PLOT (all methods)
```{r the_plot}
# any additional values to be saved in the plot name can be included as extra arguments

plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global_results_freq, 
                        outcome="precision", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
# what predicts how well the seg works for different contexts? more multisyllabic words (because of the embedding constraint). in english, do we actually have a lot of words that are made up of other words (and the smaller ones would get eliminated woth the embedding constraint)?
# is it a super bathtime utt if it's in th emiddle of a bunch of bathtime utteracnes? calculate loading for each utt for each context based on the contexts of all of the utterances around it (maybe +-2 utts again). Can we run WL method with 30-utt window, and what would that do?
# what does the embeddedness constrain drop out? are the things that get eliminated actually words, or not?
# temporal
# are the words that repeat monosyllabic or multi? implications for connecting TTR effect to (lack of) segmentation effect? also, do repeating words occur at utt boundaries? 
# What does TTR difference look like for MI dists (plot MI dists for context vs. nontext)
# plots of the chunk lengths for all the contexts (histograms)
# some organzing ideas: 
# how are the methods similar and different? Then pick a thing they do the same (TTR) and something they do differently (prop monosyllabic utts)
# family method? define context by family, nontext are size-matched samples from the whole corpus. it's one end of the context spectrum --- it is unambiguously a defensible subcorpus. 
# come up with a couple 
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="N.syl.tokens", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="MIN.segd.units", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="N.chunks", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="mean.chunk.lengths", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="min.chunk.lengths", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="med.chunk.lengths", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="max.chunk.lengths", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="med.distance", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=global_results_freq, 
                        outcome="TTR", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=global_results_freq, 
                        outcome="prop.most.freq", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=global_results_freq, 
                        outcome="mean.words.per.utt", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=global_results_freq, 
                        outcome="mean.syls.per.utt", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=global_results_freq, 
                        outcome="prop.one.word.utt", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=global_results_freq, 
                        outcome="prop.one.syl.utt", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
context_all_freq$chunks.per.utts <- context_all_freq$N.chunks/context_all_freq$N.utts
nontext_results_freq$chunks.per.utts <- nontext_results_freq$N.chunks/nontext_results_freq$N.utts
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=global_results_freq, 
                        outcome="chunks.per.utts", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE)
# z-scored
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="TTR", 
                        Z.score=TRUE,
                        print.tests=FALSE,
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE,
                        Z=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="precision", 
                        Z.score=TRUE,
                        print.tests=FALSE,
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE,
                        Z=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="prop.one.word.utt", 
                        Z.score=TRUE,
                        print.tests=FALSE,
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE,
                        Z=TRUE)
plot_context_vs_nontext(context_all_freq, 
                        nontext_results_freq, 
                        global=NULL, 
                        outcome="prop.most.freq", 
                        Z.score=TRUE,
                        print.tests=FALSE,
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext",
                        consider.freq=TRUE,
                        Z=TRUE)


# without freq
plot_context_vs_nontext(context_all, 
                        nontext_results, 
                        global_results, 
                        outcome="measure", 
                        xlabs=TRUE,
                        save.to="plots/context_vs_nontext")
plot_context_vs_nontext(context_all, 
                        nontext_results, 
                        global_results, 
                        outcome="TTR", 
                        xlabs=TRUE, 
                        save.to="plots/context_vs_nontext")
plot_context_vs_nontext(context_all, 
                        nontext_results, 
                        global_results=NULL, 
                        outcome="highest.freq.syl", 
                        # annotate="most.freq.syl",
                        save.to="plots/context_vs_nontext")
plot_context_vs_nontext(context_all, 
                        nontext_results, 
                        global_results, 
                        outcome="prop.most.freq", 
                        # annotate="most.freq.syl",
                        save.to="plots/context_vs_nontext")
plot_context_vs_nontext(context_all, 
                        nontext_results, 
                        global_results, 
                        outcome="mean.syls.per.utt", 
                        save.to="plots/context_vs_nontext")
plot_context_vs_nontext(context_all, 
                        nontext_results, 
                        global_results, 
                        outcome="mean.words.per.utt", 
                        save.to="plots/context_vs_nontext")
plot_context_vs_nontext(context_all, 
                        nontext_results, 
                        global_results, 
                        outcome="prop.one.word.utt", 
                        save.to="plots/context_vs_nontext")
plot_context_vs_nontext(context_all, 
                        nontext_results, 
                        global_results, 
                        outcome="prop.one.syl.utt", 
                        save.to="plots/context_vs_nontext")

ggplot(full_results, aes(x=context, y=value)) +
  geom_boxplot() +
  # geom_boxplot(color=NA, fill=NA, outlier.colour= NA) +
  facet_wrap( ~ method + measure, scales="free", ncol=2) +
  geom_point(aes(x=context, y=context.est, color=method), size=4, show.legend=FALSE) + 
  geom_hline(data=global_results, aes(yintercept=value), linetype = 2, size=1.5) + 
  theme(text = element_text(size=30), axis.ticks = element_blank(), axis.text.x = element_blank()) +
  labs(x=NULL, y=NULL) 
ggsave(filename="plots/allmethods_results_boxplots.png", width=12, height=20, units="in")

# each method individually
for(m in levels(full_results$method)){
  p <- ggplot(filter(full_results, method==m & N.utts > 99), aes(x=context, y=value)) +
    geom_boxplot() +
    # geom_boxplot(color=NA, fill=NA, outlier.colour= NA) +
    facet_wrap( ~ measure, scales="free", ncol=2) +
    geom_point(aes(x=context, y=context.est, color=context), size=4, show.legend=FALSE) + 
    geom_hline(data=global_results, aes(yintercept=value), linetype = 2, size=1.5) + 
    theme(text = element_text(size=30), axis.ticks = element_blank(), axis.text.x = element_text(angle=330, vjust=1, hjust=0)) +
    labs(x=NULL, y=NULL) 
  ggsave(p, filename=paste0("plots/allmethods_results_boxplots_", m, ".png"), width=16, height=10, units="in")
}


for(this.method in unique(full_results$method)){
  # with histograms instead of boxplots, to examine the nontext distributions (for shape, skew, etc.)
p <- ggplot(filter(full_results, method==this.method), aes(x=value))+
  geom_histogram() +
  geom_vline(aes(color=method, xintercept=context.est), size=2, show.legend=FALSE) + 
  facet_wrap(~ measure + context, scales="free", nrow=2) +
  theme(text = element_text(size=15), axis.ticks = element_blank(), axis.text.y = element_blank()) +
  labs(x=NULL, y=NULL)
  ggsave(p, filename=paste0("plots/allmethods_results_hist_", this.method ,".png"), width=20, height=6, units="in")
}

```


## significance test
```{r bootstrap_pvals}

tests.contexts.measure <- test_context_vs_nontext(context_all, nontext_results, outcome="measure")

tests.contexts.freq.pre <- test_context_vs_nontext(context_all_freq, nontext_results_freq,
                                                   outcome="precision")
tests.contexts.freq.TTR <- test_context_vs_nontext(context_all_freq, nontext_results_freq, 
                                                   outcome="TTR")
tests.contexts.freq.pmf <- test_context_vs_nontext(context_all_freq, nontext_results_freq, 
                                                   outcome="prop.most.freq")
tests.contexts.freq.pos <- test_context_vs_nontext(context_all_freq, nontext_results_freq, 
                                                   outcome="prop.one.syl.utt")
tests.contexts.freq.pow <- test_context_vs_nontext(context_all_freq, nontext_results_freq, 
                                                   outcome="prop.one.word.utt")
tests.contexts.freq.chu <- test_context_vs_nontext(context_all_freq, nontext_results_freq, 
                                                   outcome="N.chunks")

kable(filter(tests.contexts, criterion=="MI_0.85")[,-3], digits=3)
```

```{r what_predicts_context_effect}
understand <- full_results %>% 
  group_by(method, context, criterion, measure) %>% 
  summarize(N.utts=mean(N.utts), MIN.segd.units=mean(MIN.segd.units), N.words=mean(N.words), TTR=mean(TTR)) %>% 
  left_join(tests.contexts)
u <- lm(p.val ~ N.utts + MIN.segd.units + N.words + TTR, data=filter(understand, context !="routines"))
summary(u)

plot(p.val ~ N.utts, data=understand)
plot(p.val ~ MIN.segd.units, data=understand)
plot(p.val ~ N.words, data=understand)
plot(p.val ~ TTR, data=understand)

summary(understand$N.utts)
View(filter(understand, N.utts < 500))

filter(understand, N.utts > 4000) # which context is so much larger than all of the others?
understand <- filter(understand, context !="routines")
u <- lm(p.val ~ N.utts + MIN.segd.units + N.words + TTR, data=understand)
summary(u)

# get the Z scores for each context estimate in its nontext distribution
test <- test_context_vs_nontext(context_all_freq, nontext_results_freq, "precision")

context_seg_summary_avail <- context_seg_summary %>% 
  filter(seg.result != "false alarm") %>% 
  ungroup() %>% 
  select(-seg.result) %>% 
  group_by(method, context) %>% 
  summarise_each(funs(sum=sum(., na.rm=TRUE))) %>% 
  transmute(context=context, 
            prop.1syl.av=freq_1/freq.tot,
            prop.2syl.av=freq_2/freq.tot,
            prop.3syl.av=freq_3/freq.tot)

context_seg_summary_segd <- context_seg_summary %>% 
  filter(seg.result != "miss") %>% 
  ungroup() %>% 
  select(-seg.result) %>% 
  group_by(method, context) %>% 
  summarise_each(funs(sum=sum(., na.rm=TRUE))) %>% 
  transmute(context=context,
            prop.1syl.sg=freq_1/freq.tot,
            prop.2syl.sg=freq_2/freq.tot,
            prop.3syl.sg=freq_3/freq.tot)

summary.data <- context_all_freq %>% 
  dplyr::select(context, MIN.segd.units, N.utts:prop.one.syl.utt, N.chunks:method) %>% 
  left_join(test, by=c("context", "method")) %>% 
  left_join(context_seg_summary_avail, by=c("context", "method")) %>% 
  left_join(context_seg_summary_segd, by=c("context", "method")) %>% 
  unite(col=methcon, method, context) %>% 
  dplyr::select(-nontext.mean, -context.est, -p.val, -stars, -mean.TP, -med.distance ) 

# what variables are related to those Z scores? 
cor <- cor(summary.data[,-1], use="pairwise.complete.obs")
round(sort(cor[, which(colnames(cor)=="Z")]), 2) # what is strongly associated with the Z scores?

lm <- lm(Z ~ ., data=summary.data[,-1]) # tons of multicolinear predictors...
# library(lars)
# lasso <- lars(x=as.matrix(select(summary.data, -methcon, -Z)), y=summary.data$Z, type="lasso")
library(MASS)
lm.r <- lm.ridge(Z ~ ., data=as.data.frame(scale(summary.data[,-1])), lambda=seq(0,15,0.001), model=TRUE)
lmb <- lm.r$lambda[which.min(lm.r$GCV)]
lmr.co <- lm.r$coef[ ,which.min(lm.r$GCV)]

t <- data.frame(pred=names(lmr.co), coef=lmr.co)
t$coef.mag <- abs(t$coef)

ggplot(filter(t, coef.mag > median(t$coef.mag)), aes(x=reorder(pred, coef.mag), y=coef)) +
  geom_bar(stat = "identity") +
  labs(x=NULL) +
  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1)) 

lm <- lm(Z ~ prop.2syl.av + prop.2syl.sg + N.chunks + prop.one.syl.utt, data=summary.data[,-1])
summary(lm)

# pabiku TTR
pabiku <- c("pabiku", "golabi", "tupiro")
loTTR <- paste(sample(rep(pabiku[1:2], 12), 24), collapse="")
hiTTR <- paste(sample(rep(pabiku[1:3], 8), 24), collapse="")
```

```{r explore_utts}
df <- df[,1:3] # just in case
# how long are the utterances?
# orth.stats <- gsub(x=df$orth, pattern="_", replacement=" ", fixed=TRUE) # removing underscores
words.in.utts <- sapply(df$orth, strsplit, split=" ")
df$words.per.utt <- sapply(words.in.utts, length)
orth <- unlist(words.in.utts)  
  
mono.utts <- unique(filter(df, words.per.utt==1)$orth)
mono.utts.freqs <- table(filter(df, words.per.utt==1)$orth)

```


```{r explore_MI_dists}
rs <- list(context_WL_results=context_WL_results, 
          context_STM_results=context_STM_results,
          context_LDA_results=context_LDA_results,
          context_HJ_results=context_HJ_results)
names(rs) <- gsub(x=names(rs), pattern="context_", replacement="")
names(rs) <- gsub(x=names(rs), pattern="_results", replacement="")
for(i in 1:length(rs)){
  method <- names(rs)[i]
  plot.data <- rs[[i]]$addl.info
  plot.data.PR85 <-  plot.data %>% 
    group_by(context) %>% 
    summarize(freq85PR = quantile(AB.freq, .85),
              mi85PR = quantile(MI, .85))
  plot.data <- plot.data %>% 
    left_join(plot.data.PR85, by="context")
  plot.data$high.freq <- ifelse(plot.data$AB.freq >= plot.data$freq85PR, "high.freq", "low.freq")
  plot.data$high.MI <- ifelse(plot.data$MI >= plot.data$mi85PR, "high.MI", "low.MI")
  plot.data <- unite(plot.data, col=type, high.freq, high.MI, sep="_", remove = FALSE)
  
  p <- ggplot(plot.data, aes(y=MI, x=AB.freq)) +
    geom_point(alpha=.2) +
    facet_wrap(~ context, scales = "free_y") +
    scale_x_log10() +
    geom_hline(aes(yintercept=mi85PR), color="blue", lty=2) + 
    ggtitle(method)
  print(p)
  message(method)
  print(xtabs(~AB.freq + high.MI, data=plot.data)[1:15,])
  message("...truncating freq otput...")
}

```

```{r syllable_dists}
syls_global <- syllable_dist(df, dict, save.to="syl_dists")
syls_WL <- syllable_dist(df_WL, dict, method="WordLists", save.to="syl_dists")
syls_HJ <- syllable_dist(df_HJ, dict, method="HumanCoders", save.to="syl_dists")
syls_STM <- syllable_dist(df_STM, dict, method="STM", save.to="syl_dists")
syls_LDA <- syllable_dist(df_LDA, dict, method="LDA", save.to="syl_dists")
```


## THE PLOT (old --- one method at a time)
```{r}
full.results$context <- factor(full.results$context, levels=c("mealtime", "bedtime", "diaper.change", "bathtime", "body.touch", "play", "routines"))
ggplot(filter(full.results, criterion=="MI85"), aes(x=context, y=value))+
  # geom_boxplot() +
  geom_boxplot(color=NA, fill=NA, outlier.colour =NA) +
  facet_wrap(~measure) +
  geom_point(aes(x=context, y=context.est, color=context), size=4, show_guide=F) + 
  geom_hline(aes(yintercept=global.est), linetype = 2, size=1.5) + 
  theme(text = element_text(size=30), axis.ticks = element_blank(), axis.text.x = element_blank()) +
  labs(x=NULL, y=NULL)
ggsave(filename="plots/wordlists/global_context.png", width=12, height=5, units="in")

# with histograms instead of boxplots, to examine the nontext distributions (for shape, skew, etc.)
ggplot(filter(full.results, criterion=="MI85"), aes(x=value))+
  geom_histogram() +
  geom_vline(aes(color=context, xintercept=context.est), size=2) + 
  facet_wrap(~ measure + context, scales="free", ncol=7) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y=NULL)
```

## significance test
```{r}
# bootstrap p-values
full.results$above.est <- ifelse(full.results$value > full.results$context.est, 1, 0)
tests <- full.results %>%
  group_by(context, criterion, measure) %>%
  summarize(p.val = mean(above.est))
tests$stars <- ifelse(tests$p.val < .001, "***",
                      ifelse(tests$p.val < .01, "**",
                             ifelse(tests$p.val < .05, "*", 
                                    ifelse(tests$p.val < .1, "+", ""))))

kable(filter(tests, criterion=="MI85")[,-2], digits=3)
```

## sequence plots
```{r seqplot}
#########################################################################
# sequence plots
#########################################################################

# read in the contexts as they're used in the analysis (for both contexts and nontexts)
context_seq <- read.table("contexts_files/contexts_WL.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") 
context_seq <- read.table("contexts_files/contexts_HJ_voting.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") # this file is generated in context_coding_cleaning.R
context_seq <- read.table("contexts_files/contexts_HJ_prop.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="") # this file is generated in context_coding_cleaning.R
context_seq <- read.table("contexts_files/contexts_STM.txt", header=1, sep="\t", stringsAsFactors=F, quote="", comment.char ="")

master_doc_seq <- gather(context_seq, key=context, value=value, -orth, -phon, -utt) %>%
  separate(col=utt, into=c("file", "UttNum"), sep="_", remove=FALSE) %>%
  # for word list analysis only, make all contexts either 1 or 0 (smoothing over 1.5's from expand_windows)
  mutate(value=ifelse(value>0, 1, 0)) 

master_doc_seq$context <- as.factor(master_doc_seq$context)
master_doc_seq$utt <- as.factor(master_doc_seq$utt)
master_doc_seq$file <- as.factor(master_doc_seq$file)
master_doc_seq$UttNum <- as.numeric(master_doc_seq$UttNum)


# sequence plots for all of the files
library(ggplot2)
ggplot(filter(na.omit(master_doc_seq), context != "misc", context !="none"), aes(x=UttNum, y=context, color=context, alpha=value) ) +
  geom_point()+
  facet_wrap(~file, scales="free_x") +
  scale_alpha(guide = 'none')

# sequence plots for each file separately
context_codes <- "WL"
context_codes <- "HJ_voting"
context_codes <- "HJ_prop"
context_codes <- "STM"

files <- unique(master_doc_seq$file)
for (f in 1:length(files)){
  data <- filter(master_doc_seq, file==files[f], context != "misc", context !="none")
  # data <- filter(master_doc_seq, file==files[f])
  
  ggplot(data ) +
    geom_point(aes(x=UttNum, y=context, color=context, alpha=value, size=4, shape="|" ), na.rm = TRUE, show_guide=F) + 
    scale_shape_identity() +
    scale_alpha(guide = 'none') + 
    scale_size(guide = 'none')
  ggsave( paste0("plots/seqplot-",context_codes, files[f], ".png"), width=9, height=3, units="in" )
}

```

## descriptives
```{r}
descriptives <- results_descriptives(data=global.data, 
                                     context="global",
                                     criterion="MI85")
for(k in 1:length(colnames(contexts))){
  descriptives <- rbind(descriptives, results_descriptives(data=context.data[[k]], 
                                                           context=colnames(contexts[k]), 
                                                           criterion="MI85") )
}

# histograms of MI for each context
for(k in 0:length(names(contexts))){
  
  if(k==0) data <- global.data else data <- context.data[[k]]
  title <- ifelse(k==0, "global", names(contexts)[k])
  
  p <- ggplot(data$unique.phon.pairs, aes(x=MI)) +
    geom_histogram(fill="grey") +
    geom_vline(aes(xintercept=quantile(data$unique.phon.pairs$MI, .85)[[1]]), size=2, lty=2) + 
    theme_bw() +
    theme(text = element_text(size=20), axis.ticks = element_blank()) +
    ggtitle(title) + 
    labs(x="mutual information (MI)", y=NULL) +
    xlim(c(-5,15))
  
  ggsave(filename=paste0("plots/descriptives/MIhist_", title, ".png"),
         plot=p, 
         units="in", 
         width=4, 
         height=4)
}

# corpus descriptives
corpus_info_global <- corpus_decriptives(corpus=df[ , 1:3], 
                                         data=global.data, 
                                         contexts, 
                                         dict)


corpus.sum <- data.frame(Mean.length.utt=corpus_info_global$syls.per.utt$Mean.length.utt,
                         SD.length.utt=corpus_info_global$syls.per.utt$SD.length.utt,
                         Perc.monosyl.utts=corpus_info_global$syls.per.utt$Perc.syls.per.utt[[1]],
                         N.utts=corpus_info_global$syls.per.utt$N.utts,
                         N.words=length(global.data$streams$orth.stream),
                         context="global")
corpus.sum <- cbind(corpus.sum, corpus_info_global$syl.summary)

# for contexts where 1 (or greater) = yes and 0 = no (not prop=TRUE)
corpus_info_contexts <- list()
for(k in 1:length(names(contexts))){
  # only keep utterances that have > 0 for this context
  corpus <- df %>%
    gather(key=context, value=value, -utt, -orth, -phon) %>%
    filter(context == names(contexts)[k] & value > 0) %>%
    spread(key=context, value=value, fill=0) %>%
    select(utt, orth, phon)
  
  corpus_info_contexts[[ names(contexts)[k] ]] <- corpus_decriptives(corpus=corpus, 
                                                                     data=context.data[[k]], 
                                                                     contexts, 
                                                                     dict) 
  message(names(contexts)[k])
  # print(corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt)
  
  syl.freqs <- corpus_info_contexts[[ names(contexts)[k] ]]$word.freq %>%
    group_by(N.syl) %>%
    summarize(freq.mean=mean(freq), freq.sd=sd(freq), N.types=n(), freq.se = freq.sd/sqrt(N.types))
  
  this.corpus.sum <- data.frame(Mean.length.utt=corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt$Mean.length.utt,
                                SD.length.utt=corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt$SD.length.utt,
                                Perc.monosyl.utts=corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt$Perc.syls.per.utt[[1]],
                                N.utts=corpus_info_contexts[[ names(contexts)[k] ]]$syls.per.utt$N.utts,
                                N.words=length(context.data[[k]]$streams$orth.stream),
                                context=names(contexts)[k])
  this.corpus.sum <- cbind(this.corpus.sum, corpus_info_contexts[[ names(contexts)[k] ]]$syl.summary)
  corpus.sum <- rbind(corpus.sum, this.corpus.sum)
}
corpus.sum$length.utt.se <- corpus.sum$SD.length.utt/sqrt(corpus.sum$N.utts)

ggplot(corpus.sum, aes(x=context, y=Mean.length.utt)) +
  geom_bar(fill=NA, stat="identity", show_guide=F) +
  geom_bar(data=filter(corpus.sum, context == "global"), fill="grey", stat="identity", show_guide=F) +
  geom_bar(data=filter(corpus.sum, context != "global"), aes(fill=context), stat="identity", show_guide=F) +
  geom_errorbar(aes(ymax = Mean.length.utt + length.utt.se, ymin=Mean.length.utt - length.utt.se), position=position_dodge(width=0.9), width=.25) +
  # geom_errorbar(aes(ymax = Mean.length.utt + SD.length.utt, ymin=Mean.length.utt - SD.length.utt), position=position_dodge(width=0.9), width=0, alpha=.5, linetype=2) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y="Syllables per utterance")
ggsave(filename ="plots/syls_per_utt.png", width=12, height=6, units="in")

# percent monosyllable utterances?
ggplot(corpus.sum, aes(x=context, y=Perc.monosyl.utts)) +
  geom_bar(fill=NA, stat="identity", show_guide=F) +
  geom_bar(data=filter(corpus.sum, context == "global"), fill="grey", stat="identity", show_guide=F) +
  geom_bar(data=filter(corpus.sum, context != "global"), aes(fill=context), stat="identity", show_guide=F) +
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y="Proportion monosyllabic utterances")
ggsave(filename ="plots/perc_monosyl_utts.png", width=12, height=6, units="in")

# freq of 1- 2- and 3-syl words?
freq.plot <- corpus.sum %>%
  gather(key=key, value=value, freq1syl.tot:freq4syl.se ) %>%
  separate(col=key, into=c("N.syl", "stat"), sep="[.]") %>%
  separate(col=N.syl, into=c("xx", "N.syl"), sep="q") %>%
  select(-xx) %>%
  extract(col=N.syl, into=c("N.syl"), regex="([[:digit:]]+)") %>%
  spread(key=stat, value=value) %>%
  gather(key=key, value=value, freq1st.word:freq3rd.freq) %>%
  extract(col=key, into=c("rank", "stat"), regex="([[:digit:]]+)[[:alpha:]]{2}[.]([[:alpha:]]+)") %>%
  spread(key=stat, value=value) %>%
  select(context, N.words, N.syl, tot, mean, sd, se) %>%
  mutate(prop=tot/N.words) %>%
  unique()
  
ggplot(freq.plot, aes(x=context, y=prop)) +
  geom_bar(fill=NA, stat="identity", show_guide=F) +
  geom_bar(data=filter(freq.plot, context == "global"), fill="grey", stat="identity", show_guide=F) +
  geom_bar(data=filter(freq.plot, context != "global"), aes(fill=context), stat="identity", show_guide=F) +
  facet_wrap(~ N.syl) + 
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x=NULL, y="Proportion of total words")

ggplot(freq.plot, aes(x=N.syl, y=prop)) +
  geom_bar(data=filter(freq.plot, context == "global"), fill="grey", stat="identity", show_guide=F) +
  geom_bar(data=filter(freq.plot, context != "global"), aes(fill=context), stat="identity", show_guide=F) +
  facet_wrap(~ context, ncol=4) + 
  theme(text = element_text(size=20), axis.ticks = element_blank()) +
  labs(x="Number of syllables", y="Proportion of total words")
ggsave(filename ="plots/syls_per_word.png", width=12, height=6, units="in")

# chi-square tests
global.props <- filter(freq.plot, context=="global")$prop + (1-sum(filter(freq.plot, context=="global")$prop))/4 # need to make sure it sums to 1
context.props <- list()
for(k in 1:length(names(contexts)) ){
  context.props[[names(contexts)[k]]] <- filter(freq.plot, context==names(contexts)[k])$prop + (1-sum(filter(freq.plot, context==names(contexts)[k])$prop))/4 # need to make sure it sums to 1
  message(names(contexts)[k])
  print(chisq.test( context.props[[names(contexts)[k]]] , p=global.props ))
}

##################################################
# NOTE: Swingley 2005 only considers a "word" segmented if it has BOTH high within-unit MI and also high frequency as a unit.

plot.data.con <- data.frame(context=names(context.data), TP85recall.mean=NA, TP85precision.mean=NA, MI85recall.mean=NA, MI85precision.mean=NA, TP85recall.sd=NA, TP85precision.sd=NA, MI85recall.sd=NA, MI85precision.sd=NA, N.utt=NA, group="context")
for(k in 1:length(names(contexts))){
  plot.data.con[k, 2:3] <- colMeans(context.data[[k]]$TP85$seg.results[,4:5], na.rm=T)
  plot.data.con[k, 4:5] <- colMeans(context.data[[k]]$MI85$seg.results[,4:5], na.rm=T)
  plot.data.con$N.utt[k] <- context.data[[k]]$N.utterances
}


plot.data.non <- bootstrap.summary
plot.data.non$N.utt <- plot.data.con$N.utt
plot.data.non$group <- "nontext"

plot.data <- rbind(plot.data.con, plot.data.non)
plot.data$group <- as.factor(plot.data$group)

plot.data <- plot.data %>%
  tidyr::gather(variable, value, TP85recall.mean:MI85precision.sd) %>%
  tidyr::extract(col=variable, into=c("criterion", "variable", "stat"), regex="([A-Z]{2}[0-9]{2})([a-z]+)[.]([a-z]+)") %>%
  tidyr::spread(stat, value)
plot.data$criterion <- as.factor(plot.data$criterion)
plot.data$variable <- as.factor(plot.data$variable)

#####################################################
# Is accuracy just driven by number of utterances?

ggplot(plot.data, aes(x=N.utt, y=mean, group=group)) +
  geom_point(aes(color=context, shape=group), size=4 )+
  geom_line(stat="smooth", method="lm", aes(linetype=group)) +
  facet_wrap(~variable + criterion)

# exclude routines
ggplot(subset(plot.data, context != "routines"), aes(x=N.utt, y=mean, group=group)) +
  geom_point(aes(color=context, shape=group), size=4 )+
  geom_line(stat="smooth", method="lm", aes(linetype=group)) +
  facet_wrap(~variable + criterion) +
  ggtitle("Excluding routines context and corresponding nontext")

summary(lm(mean ~ group*N.utt*criterion, data=plot.data))

#####################################################
# Do context vs. nontexts differ in accuracy?

ggplot(plot.data, aes(x=group, y=mean, fill=context)) +
  geom_bar(aes(), stat="identity", position="dodge")+
  geom_errorbar(aes(ymax = mean + sd, ymin=mean - sd), width=.3, position=position_dodge(.9)) +
  facet_wrap(~ variable + criterion)

ggplot(plot.data, aes(x=context, y=mean, fill=group)) +
  geom_bar(aes(), stat="identity", position="dodge")+
  geom_errorbar(aes(ymax = mean + sd, ymin=mean - sd), width=.3, position=position_dodge(.9)) +
  facet_wrap(~ variable + criterion) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="Error bars are +-SD", x=NULL, y=NULL)

ggplot(plot.data, aes(x=criterion:variable, y=mean, fill=group)) +
  geom_bar(aes(), stat="identity", position="dodge")+
  geom_errorbar(aes(ymax = mean + sd, ymin=mean - sd), width=.3, position=position_dodge(.9)) +
  facet_wrap(~ context, ncol=4) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="Error bars are +-SD", x=NULL, y=NULL)


#####################################################
# Segmentation sucess by frequency
plot_seg_results(global.data$TP85$seg.results, "global TP85")
plot_seg_results(global.data$MI85$seg.results, "global MI85")

for(k in 1:length(colnames(contexts))){
  TPplot <- plot_seg_results(context.data[[k]]$TP85$seg.results, title=paste(colnames(contexts)[k], "TP85"))
  MIplot <- plot_seg_results(context.data[[k]]$MI85$seg.results, title=paste(colnames(contexts)[k], "MI85"))
  #print(TPplot)
  print(MIplot)
}



#####################################################
# is it segmenting seed words correctly?
global.data$TP85$check_seed_words <- check_seed_words(global.data$TP85$seg.results)
summary(global.data$TP85$check_seed_words$seg.result)
plot_seg_results(seg.results=global.data$TP85$check_seed_words, title="Segmentation accuracy for seed words\nTP85", boxplot=TRUE, scatterplot=TRUE, by="contexts")
plot_seg_results(seg.results=global.data$TP85$check_seed_words, title="Segmentation accuracy for seed words\nTP85", boxplot=TRUE, scatterplot=TRUE, by="syl")

global.data$MI85$check_seed_words <- check_seed_words(global.data$MI85$seg.results)
summary(global.data$MI85$check_seed_words$seg.result)
plot_seg_results(seg.results=global.data$MI85$check_seed_words, title="Segmentation accuracy for seed words\nMI85", boxplot=TRUE, scatterplot=TRUE, by="contexts")
plot_seg_results(seg.results=global.data$MI85$check_seed_words, title="Segmentation accuracy for seed words\nMI85", boxplot=TRUE, scatterplot=TRUE, by="syl")

for(k in 1:length(colnames(contexts))){
  context.data[[k]]$TP85$check_seed_words <- check_seed_words(context.data[[k]]$TP85$seg.results)
  summary(context.data[[k]]$TP85$check_seed_words$seg.result)
  TPplot<- plot_seg_results(seg.results=context.data[[k]]$TP85$check_seed_words, title=paste("Segmentation accuracy for seed words\nTP85", colnames(contexts)[k]), boxplot=TRUE, scatterplot=TRUE, by="contexts")
  
  context.data[[k]]$MI85$check_seed_words <- check_seed_words(context.data[[k]]$MI85$seg.results)
  summary(context.data[[k]]$MI85$check_seed_words$seg.result)
  MIplot <- plot_seg_results(seg.results=context.data[[k]]$MI85$check_seed_words, title=paste("Segmentation accuracy for seed words\n", colnames(contexts)[k], "context"), boxplot=TRUE, scatterplot=TRUE, by="contexts")
  #print(TPplot)
  print(MIplot)
}


gmodels::CrossTable(global.data$MI85$seg.results[,6], global.data$MI85$seg.results$syl.bins, prop.r=T, prop.c=F, prop.chisq=F)
seg.by.syl <- table(global.data$MI85$seg.results[,c(6, 10)])
syl.by.seg <- table(global.data$MI85$seg.results[,c(10, 6)])
plot(table(global.data$MI85$seg.results[,c(6, 10)]))
barplot(seg.by.syl, beside=T, legend=rownames(seg.by.syl), xlab="syllables")
barplot(seg.by.syl, beside=F, legend=rownames(seg.by.syl), xlab="syllables")
barplot(syl.by.seg, beside=T, legend=rownames(syl.by.seg), xlab=NULL)
barplot(syl.by.seg, beside=F, legend=rownames(syl.by.seg), xlab=NULL)

combined.seg.results <- context.data[[1]]$MI85$seg.results
combined.seg.results$context.corpus <- colnames(contexts)[1]
for(k in 2:length(colnames(contexts))){
  seg.results <- context.data[[k]]$MI85$seg.results
  seg.results$context.corpus <- colnames(contexts)[k]
  combined.seg.results <- rbind(combined.seg.results,seg.results)
}

combined.seg.results$context.corpus <- as.factor(combined.seg.results$context.corpus)

bottoms <- grepl("bottom", df$orth)
bottom.corpus <- df[bottoms,]
bottom.corpus[,c(1,6,7)]

bellys <- grepl("belly", df$orth)
belly.corpus <- df[bellys,]
belly.corpus[, c(1, 3:9)]
belly.seg <- grepl("'bE-lI", combined.seg.results$phon, fixed=TRUE)
belly.seg <- combined.seg.results[belly.seg,]

big.kiss <- df[grepl("'bIg 'kIs", df$phon, fixed=T),]; nrow(big.kiss)
big <- df[grepl("'bIg", df$phon, fixed=T),]; nrow(big)
kiss <- df[grepl("'kIs", df$phon, fixed=T),]; nrow(kiss)

arrange(filter(context.data$body.touch$unique.phon.pairs, syl1=="'bIg"), -MI)
arrange(filter(global.data$unique.phon.pairs, syl1=="'bIg"), -MI)
arrange(filter(context.data$body.touch$unique.phon.pairs, syl2=="'f{t"), -MI)
arrange(filter(global.data$unique.phon.pairs, syl2=="'f{t"), -MI)
arrange(filter(context.data$body.touch$unique.phon.pairs, syl1=="'tV"), -MI)
arrange(filter(context.data$body.touch$unique.phon.pairs, syl2=="mI"), -MI)
arrange(filter(global.data$unique.phon.pairs, syl1=="'tV"), -MI)
arrange(filter(global.data$unique.phon.pairs, syl2=="mI"), -MI)
head(arrange(context.data$body.touch$unique.phon.pairs, -freq), 40)

length(which(global.data$streams$phon.pairs$syl1=="'tV"))
length(which(context.data$body.touch$streams$phon.pairs$syl1=="'tV"))

filter(context.data$body.touch$unique.phon.pairs, syl2=="'kIs")
filter(global.data$unique.phon.pairs, syl2=="'kIs")

combined.seg.results[grepl("'kIs", combined.seg.results$phon),] # kiss

combined.seg.results[grepl("'tI-kP", combined.seg.results$phon),c(1,2,7,13)] # tickle
combined.seg.results[grepl("kP", combined.seg.results$phon),c(1,2,7,9,13)] # 'ckle (end of "tickle", "chuckle", "typical")
global.data$MI85$seg.results[grepl("'tI-kP", global.data$MI85$seg.results$phon, fixed=T),] # tickle

combined.seg.results[grepl("'f{t", combined.seg.results$phon, fixed=T),] # fat
combined.seg.results[grepl("'bIg", combined.seg.results$phon, fixed=T),] # big
combined.seg.results[grepl("'bIg-'f{t", combined.seg.results$phon, fixed=T),] # big fat
global.data$MI85$seg.results[grepl("'f{t", global.data$MI85$seg.results$phon, fixed=T),] # fat
global.data$MI85$seg.results[grepl("'bIg", global.data$MI85$seg.results$phon, fixed=T),] # big
global.data$MI85$seg.results[grepl("'tV", global.data$MI85$seg.results$phon, fixed=T),] # tu- (mmy)
df[grepl("'f{t", df$phon, fixed=T),c(1,7,9)] # fat
df[grepl("'bIg 'f{t 'tV mI", df$phon, fixed=T),c(1,7,9)] # big fat tummy
df[grepl("'bIg 'f{t", df$phon, fixed=T),c(1,7,9)] # big fat
df[grepl("'bIg 'kIs", df$phon, fixed=T),c(1,7,9)]# big kiss
df[grepl("'bIg", df$phon, fixed=T),c(1,7,9)] # big
df[grepl("'kIs", df$phon, fixed=T),c(1,7,9)]# kiss
global.data$MI85$seg.results[grepl("^@$", global.data$MI85$seg.results$phon),] # a
combined.seg.results[grepl("^@$", combined.seg.results$phon),] # a

# global.data$MI85$seg.results[grepl("'fV", global.data$MI85$seg.results$phon, fixed=T),] # fun (-nny)
# combined.seg.results[grepl("'h{-pI", combined.seg.results$phon, fixed=T),] # happy
# combined.seg.results[grepl("'kr2-IN", combined.seg.results$phon, fixed=T),] # crying
# combined.seg.results[grepl("'bQ-tP", combined.seg.results$phon, fixed=T),] # bottle
# combined.seg.results[grepl("'sI-lI", combined.seg.results$phon, fixed=T),]# 'sI-lI
# combined.seg.results[grepl("'bE-lI", combined.seg.results$phon, fixed=T),] # belly
# combined.seg.results[grepl("'bju-t@-fUl", combined.seg.results$phon, fixed=T),] # beautiful
# combined.seg.results[grepl("'wIn-dI", combined.seg.results$phon, fixed=T),] # windy
# combined.seg.results[grepl("'mV-mI", combined.seg.results$phon, fixed=T),] # mummy
# combined.seg.results[grepl("'s5-pI", combined.seg.results$phon, fixed=T),] # splashing
combined.seg.results[grepl("'spl{-SIN", combined.seg.results$phon, fixed=T),] # splashing
combined.seg.results[grepl("'spl{", combined.seg.results$phon, fixed=T),] # spla-
global.data$MI85$seg.results[grepl("'spl{-SIN", global.data$MI85$seg.results$phon, fixed=T),] # splashing
global.data$MI85$seg.results[grepl("'spl{", global.data$MI85$seg.results$phon, fixed=T),] # spla-
global.data$MI85$seg.results[grepl("SIN", global.data$MI85$seg.results$phon, fixed=T),] # -shing
global.data$unique.phon.pairs[grepl("'spl{", global.data$unique.phon.pairs$syl1, fixed=T) ,] # spla-
global.data$unique.phon.pairs[grepl("SIN", global.data$unique.phon.pairs$syl2, fixed=T) ,] # -shing

head(arrange(filter(global.data$MI85$seg.results, seg.result=="miss"), -freq), 100)
head(arrange(filter(global.data$MI85$seg.results, seg.result=="miss" & freq>2), freq), 100)

high.freq.fails <- arrange(filter(combined.seg.results, seg.result!="hit"), -freq) # high freq fails
df[grepl("'mV mI", df$phon, fixed=T),1:2] # mummy
df[grepl("'lI", df$phon, fixed=T),1:2] # li' (e.g., li-cking, little, li-sten)
df[grepl("'kV ", df$phon, fixed=T),1:2] # kuh (cu-ddle, co-ming, cou-ple, )
df[grepl("'I ", df$phon, fixed=T),1:2] # i (i-sn't, i-t'll, )
df[grepl("'d7 ", df$phon, fixed=T),1:2] # dear
filter(global.data$unique.phon.pairs, syl1=="'lI") # global data
filter(context.data$mealtime$unique.phon.pairs, syl1=="'lI") # global data

unique(global.data$streams$orth.stream[-which(global.data$streams$orth.stream %in% dict$word)])

library(igraph)
df.big <- df[grepl("'bIg", df$phon, fixed=T),]
streams.big <- make_streams(df.big)
plot(graph.edgelist(as.matrix(streams.big$phon.pairs), directed=F))

df.big.sample <- df.big[sample(1:nrow(df.big), 15),]
streams.big.sample <- make_streams(df.big.sample)
plot(graph.edgelist(as.matrix(streams.big.sample$phon.pairs), directed=F), main="overall\nsample with big")

df.big.body.touch <- df[grepl("'bIg", df$phon, fixed=T) & df$body.touch>0,]
df.big.body.touch.sample <- df.big.body.touch[sample(1:nrow(df.big.body.touch), 15),]
streams.big.body.touch.sample <- make_streams(df.big.body.touch.sample)
plot(graph.edgelist(as.matrix(streams.big.body.touch.sample$phon.pairs), directed=F), main="body.touch\nsample with big")

df.body.touch <- df[df$body.touch>0,]
df.body.touch.sample <- df.body.touch[sample(1:nrow(df.body.touch), 22),]
streams.df.body.touch.sample <- make_streams(df.body.touch.sample)
plot(graph.edgelist(as.matrix(streams.df.body.touch.sample$phon.pairs), directed=F), main="body.touch\nrandom sample")

df.sample <- df[sample(1:nrow(df), 22),]
streams.sample <- make_streams(df.sample)
plot(graph.edgelist(as.matrix(streams.sample$phon.pairs), directed=F), main="overall\nrandom sample")

body.touch.edge.list <- context.data$body.touch$streams$phon.pairs[grepl("'bIg", context.data$body.touch$streams$phon.pairs[,1], fixed=T),]
plot(graph.edgelist(as.matrix(body.touch.edge.list), directed=F))

overall.edge.list <- global.data$streams$phon.pairs[grepl("'bIg", global.data$streams$phon.pairs[,1], fixed=T),]
plot(graph.edgelist(as.matrix(overall.edge.list)))

routines.edge.list <- context.data$routines$streams$phon.pairs[grepl("'bIg", context.data$routines$streams$phon.pairs[,1], fixed=T),]
plot(graph.edgelist(as.matrix(routines.edge.list)))

network_plot(global.data, "global.data")
network_plot(context.data$body.touch, "body.touch")

library(GGally)
global.network <- network::network(as.matrix(global.data$streams$phon.pairs))
ggnet(global.network,  title="overall\nwhole corpus", weight.method="degree")

body.touch.network <- network::network(as.matrix(context.data$body.touch$streams$phon.pairs))
ggnet(body.touch.network, main="body.touch\nwhole corpus", weight.method="degree")


el <- graph.data.frame(global.data$unique.phon.pairs)
plot(el,layout=layout.fruchterman.reingold, edge.width=E(el)$width/2)
# line node size should be syllable freq, weights of edges should be 

ggplot(combined.seg.results, aes(x=seg.result, y=freq.segd)) +
  facet_wrap(~context.corpus, ncol=7) +
  geom_boxplot() +
  scale_y_log10() + labs(y="Log10(frequency)", x=NULL, title=NULL) + 
  scale_x_discrete(limits=c("false alarm", "hit")) +
  coord_flip() +
  theme(text = element_text(size=30))
ggplot(combined.seg.results, aes(x=seg.result, y=freq.orth)) +
  facet_wrap(~context.corpus, ncol=7) +
  geom_boxplot() +
  scale_y_log10() + labs(y="Log10(frequency)", x=NULL, title=NULL) + 
  scale_x_discrete(limits=c("miss", "hit")) +
  coord_flip() +
  theme(text = element_text(size=30))

combined.check_seed_words <- context.data[[1]]$MI85$check_seed_words
combined.check_seed_words$context.corpus <- colnames(contexts)[1]
for(k in 2:length(colnames(contexts))){
  check_seed_words <- context.data[[k]]$MI85$check_seed_words
  check_seed_words$context.corpus <- colnames(contexts)[k]
  combined.check_seed_words <- rbind(combined.check_seed_words,check_seed_words)
}

combined.check_seed_words$context.corpus <- as.factor(combined.check_seed_words$context.corpus)

ggplot(combined.check_seed_words, aes(x=seg.result, y=freq)) +
  facet_wrap(~context.corpus, ncol=4) +
  geom_boxplot() +
  geom_point(aes(color=context), alpha=.7, size=4, position = position_jitter(w = .3, h = 0)) +
  scale_y_log10() + labs(y="Log10(frequency)", x=NULL, title=title) + 
  coord_flip()

library(knitr)
N.utt
tests <- vector("list", length(names(contexts))) # storage variable
names(tests) <- colnames(contexts)

for(k in 1:length(names(contexts))){
  message(paste("processing ", names(contexts)[k], "...", sep=""))
  
  tests[[k]]$context.pairs.N <- nrow(context.data[[k]]$unique.phon.pairs)
  tests[[k]]$nontext.pairs.N <- nrow(nontext.data[[k]]$unique.phon.pairs)
  
  tests[[k]]$test.MI <- ks.test(context.data[[k]]$unique.phon.pairs$MI, nontext.data[[k]]$unique.phon.pairs$MI)
  tests[[k]]$test.TP <-ks.test(context.data[[k]]$unique.phon.pairs$TP, nontext.data[[k]]$unique.phon.pairs$TP)
}

tests

# # reformat for printing as table
tests.table <- data.frame(N.utt=N.utt , TP.KSstat = NA, TP.KSpval=NA, MI.KSstat=NA, MI.KSpval=NA)
  for(k in 1:length(names(context.data))){
    tests.table[k,2:5] <- round(c(tests[[k]]$test.TP$statistic, tests[[k]]$test.TP$p.value, tests[[k]]$test.MI$statistic, tests[[k]]$test.MI$p.value), 3)
  }
tests.table$TP.KSpval <- ifelse(tests.table$TP.KSpval==0, ">.001", tests.table$TP.KSpval)
tests.table$MI.KSpval <- ifelse(tests.table$MI.KSpval==0, ">.001", tests.table$MI.KSpval)
knitr::kable(tests.table)
x


# reformat data for plotting
plot.data.wide <- data.frame(context=NULL, sample=NULL, MI=NULL, TP=NULL)
for(k in 1:length(colnames(contexts))){
  con <- data.frame(context=colnames(contexts)[k], sample="context", MI=context.data[[k]]$unique.phon.pairs$MI, TP=context.data[[k]]$unique.phon.pairs$TP)
  non <- data.frame(context=colnames(contexts)[k], sample="nontext", MI=nontext.data[[k]]$unique.phon.pairs$MI, TP=nontext.data[[k]]$unique.phon.pairs$TP)
  
 plot.data.wide <- rbind(plot.data.wide, con, non)
 
}
plot.data <- melt(plot.data.wide)

ggplot(subset(plot.data, variable=="MI"), aes(x=value, fill=sample)) + 
  geom_histogram(aes(y = ..density..), position="identity", alpha=.5) +
  facet_wrap( ~ context , scales="free") +
  theme_bw() +
  labs(title="Mutual Information")

ggplot(subset(plot.data, variable=="TP"), aes(x=value, fill=sample)) + 
  geom_histogram(aes(y = ..density..), position="identity", alpha=.5) +
  facet_wrap( ~ context , scales="free") +
  theme_bw() +
  labs(title="Transitional Probabilities")

# dist of syllables
par(mfrow=c(1,1))
for(k in 1:length(colnames(contexts))){
  syl.freq <- table(context.data[[k]]$streams$phon.stream)
  hist(syl.freq, main=paste(colnames(contexts)[k], "syllable freq"))
}

for(k in 1:length(names(nontext.data))){
  syl.freq <- table(nontext.data[[k]]$streams$phon.stream)
  hist(syl.freq, main=paste(names(nontext.data)[k], " syllable freq (", colnames(contexts)[k], ")", sep=""))
}



freq.bigrams <- summarise(group_by(global.data$streams$phon.pairs, syl1, syl2), count=n()) # frequency of bigrams
freq.words <- table(global.data$streams$orth.stream) # frequency of words
freq.syl <- table(global.data$streams$phon.stream) # frequency of syllables

# freq plots
barplot(sort(freq.words, decreasing=TRUE)) 
barplot(sort(freq.words, decreasing=TRUE)[1:100]) # top 100 words only 
barplot(sort(freq.syl, decreasing=TRUE))
barplot(sort(freq.syl, decreasing=TRUE)[1:100]) # top 100 syllables only 

freq.words.df <- as.data.frame(freq.words)

# merge frequency with dictionary information (number of syllables, etc.)
colnames(freq.words.df)[1] <- "word"
freq.words.df <- merge(x=freq.words.df, y=dict, all.x=TRUE)

# present results

plot(Freq ~ jitter(N.syl), data = freq.words.df, main="Frequency of words \nby number of syllables")
plot(Freq ~ jitter(N.syl), data = filter(freq.words.df, Freq<2000), main="Frequency of words \nby number of syllables \n(outlier removed)") # remove outlier

ggplot(freq.words.df, aes(y=Freq, x=jitter(N.syl))) +
  geom_point(aes(alpha=.5)) +
  facet_wrap(~context)

freq.summary <- summarise(group_by(freq.words.df, Syllables=as.factor(N.syl)), Mean.Freq=mean(Freq), sd=sd(Freq), n=n(), se=sd/sqrt(n))
ggplot(freq.summary, aes(x=Syllables, y=Mean.Freq))+
  geom_bar(stat="identity", position=position_dodge(.9)) +
  geom_errorbar(aes(ymax = Mean.Freq + se, ymin=Mean.Freq - se), width=.3, position=position_dodge(.9)) +
  labs(title="Frequency by number of syllables\n(Error bars +-SE)")


# contexts and ambiguity?
context.syl.tokens <- list(NULL)
# number of syllables per utterance in contexts vs. nontexts?
for(k in 1:length(names(context.data))){
  context.syl.tokens[k] <- length(context.data[[k]]$streams$phon.stream)
}
nontext.syl.tokens <- list(NULL)
# number of syllables per utterance in contexts vs. nontexts?
for(k in 1:length(names(nontext.data))){
  nontext.syl.tokens[k] <- length(nontext.data[[k]]$streams$phon.stream)
}

names(context.syl.tokens) <- names(context.data)
names(nontext.syl.tokens) <- names(context.data)

N.utt

df$utterance <- row.names(df)
df$y <- 1

par(mfrow=c(1,1))

fifth <- floor(nrow(df)/5)
plot.data <- df[1:100, ]
ggplot(plot.data, aes(x=utterance, y=y, color=context)) + 
  theme_bw() +
  geom_point()


filter(context.data$mealtime$unique.phon.pairs, syl1=="'mIlk") 
filter(nontext.data[[1]]$unique.phon.pairs, syl1=="'mIlk") 
